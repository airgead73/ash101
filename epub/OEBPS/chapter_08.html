<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" xmlns:epub="http://www.idpf.org/2007/ops"
  xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" epub:prefix="index: http://www.index.com/">
<head>
<title>Chapter 08</title>
<meta content="width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1" name="viewport" />
<link rel="stylesheet" href="Styles/style.css" type="text/css"/>
<link rel="stylesheet" href="Styles/mobile.css" type="text/css"/> 
 <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script>
 
</head>

<body id="ch08" xml:lang="en-US">
<a id="p313"></a>
<section class="co" id="sec8.0">
  <div class="co_header" id="co_header_ch8">
    <h1><span class="ct_number">CHAPTER EIGHT</span></h1>
    <div class="seperator">&nbsp;</div>
    <h2><span class="ct_title">Teacher&ndash;Made Assessments</span></h2>
  </div>
  <div class="co_quote">
    <p class="quote_text">A fool must now and then be right by chance.</p>
    <cite class="quote_cite">&mdash;William Cowper</cite> </div>

<a id="p314"></a>
  <div class="co_intro">
    <div class="co_text">
        <p class="txt">Even a blind squirrel sometimes finds a nut. And a fool is sometimes right by chance. But more often, the fool is wrong and the blind squirrel goes hungry&mdash;and ends up feeding a red-tailed hawk that is far from blind.</p>
        <p class="txt">No one would ever have accused Jordyn of being a fool. She was clearly smarter than most of her classmates. But she didn&#39;t always have time to study for the many multiple-choice pop quizzes that Mr. Moskal liked to give. Yet she almost always did well.</p>
        <p class="txt">&quot;How d&#39;ya do it?&quot; asked Louis, who was trying hard to hang out with her.</p>
        <p class="txt">&quot;I guess,&quot; said Jordyn. &quot;I do well just by chance.&quot; </p>
        <p class="txt">&quot;That&#39;s a lie,&quot; said Louis, who was academically gifted but not especially socially intelligent. He went on to explain that by chance, Jordyn might do well some of the time&mdash;as might any other student in the class. But if chance were the only factor determining her results, she should do very poorly most of the time. &quot;If a multiple-choice item has four options,&quot; he expounded like a little professor, &quot;and each of them is equally probable, if you have absolutely no idea which is correct, on average you should answer correctly 25% of the time. And you should be dead wrong three quarters of the time.&quot;</p>
        <p class="txt">&quot;I&#39;m dead right three quarters of the time,&quot; Jordyn smirked, &quot;and I&#39;m not going to any movie with you.&quot;</p>
        <p class="txt">It turned out, as Louis eventually discovered, that Jordyn had quickly noticed that Mr. Moskal&#39;s test items were so poorly constructed that the clever application of a handful of guidelines almost always assured a high degree of success, even if she only knew a smattering of correct answers to begin with. For example, Mr. Moskal made extensive use of terms like <em>always</em>, <em>never</em>, <em>everywhere</em>, and entirely in his multiple-choice options; Jordyn knew that these are almost always false. She also knew that the longest, most inclusive options are more likely to be correct than shorter, very specific options. And she was clever enough to realize that options that don&#39;t match the question for grammatical or logical reasons are likely incorrect&mdash;as are silly or humorous options. And options like &quot;all of the above&quot; are always correct if two of the above are correct, and &quot;none of the above&quot; is more often incorrect than not.</p>
        <p class="txt">Mr. Moskal should have read this chapter!</p>
    </div>

    <div class="co_aside" id="co_aside_ch8" alt="Teacher sits at her desk and does paperwork." title="Teacher sits at her desk and does paperwork.">
	   <div class="co_aside_cr">
	     <div class="co_src"><em>Monkeybusinessimages/iStock/Getty Images Plus</em></div>
	   </div>
    </div>
   
  </div>
 <script type="text/javascript" src="Scripts/app-min.js"></script>
</section>


<!--
/////////////////////////////////////////////////////////////
///////////////////////// 8.1 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->


<section class="page" id="sec8.1">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">8.1</span> <span class="sec_title">Planning for Teacher-Made Tests</span></h1>

    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What are some important steps in planning for assessment?</span></h2>
  </div>
      <div class="section-lead">
         <p class="intro">Reading the chapter might have improved Mr. Moskal&#39;s construction of teacher-made tests (as opposed to commercially prepared standardized tests, discussed in Chapter 10).</p>
     </div>

  <div class="section-body">
      
     
	  <div class="single-column">    
	         <p class="txt">He would know that he should not rely solely on his memory and intuition when constructing a test but should begin with a clear notion of his educational goals. He then needs to decide on the best ways of determining whether his learners have reached these goals. If his assessments are to be useful for determining how well his students have learned (<em>summative</em> function of tests) and for improving their learning (<em>formative</em> function of tests), he will need some detailed test blueprints, and perhaps some rubrics and checklists, to help him evaluate student performances. </p>
      </div>

<a id="p315"></a>
   <div class="inner-section">
	  <h2 class="h2">Identify Goals and Learning Objectives</h2>
	  <div class="single-column">
                     <p class="txt">Educational goals are the nation&#39;s, state&#39;s, school district&#39;s, or teacher&#39;s general statements of the broad intended outcomes of the educational process. Learning, or <em>instructional</em>, objectives are more specific statements of intended learning outcomes relative to a lesson, unit, or even course. Whereas educational goals are often somewhat vague and idealistic, the most useful learning objectives for the classroom tend to be very explicit. Most are phrased in terms of behaviors that can be taught and learned and that can be assessed. </p>

             <h3 class="h3">National Educational Standards (Goals)</h3>
                    <p class="txt">The nation&#39;s educational goals (or <em>standards</em>), for example, are often detailed in legislation and regulations. As we saw in Chapter 1, in the United States the legislation governing K&ndash;12 education, ESSA, requires all states to develop standards and to devise or select appropriate assessments to determine the extent to which these standards are being met.   </p>

                    <p class="txt">Virtually all states have now published descriptions of educational standards and criteria that can be used to assess the extent to which educational goals are being met. As we saw, following a nationwide education initiative involving a consortium of educators, Common Core State Standards (Council of Chief State School Officers, 2018) were developed. These standards describe what students should know at each grade level for each subject. One intended result of adopting common core standards is to bring about a realignment of curricula across different states.</p>

	    </div>

<div class="text_container">
        <h4><button class="ec_expand" aria-expanded="false" aria-controls="ecbox8-1">LEARN MORE</button></h4>
       <div id="ecbox8-1">
		 <div class="single-column">
                    <p class="txt">State standards serve as a guide for the specific instructional objectives developed by local school jurisdictions and ultimately by classroom teachers. For example, Figure 8.1 shows the nine California core reading standards for Literature at the grade 1 level (Sacramento County Office of Education, 2018b). These are derived directly from the CCSS. </p>

                    <p class="txt">Note that each of the standards shown in Figure 8.1 suggests certain assessment and instructional activities. For example, the fourth standard&mdash;<em>identify words and phrases in stories or </em><em>poems that suggest feelings or appeal to the senses</em>&mdash;leads to a wide range of instructional possibilities. Teachers might take steps to ensure that students understand what emotions are and that they recognize words relating to them. Perhaps direct teaching methods might be used to inform learners about the human senses. Group activities might encourage learners to generate affect-related words. And learners might be asked to search stories and poems for words and phrases associated with feelings. </p>
             </div>
  <a id="p316"></a>

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.1</span><br />
          <span class="figure_title">California standards: Grade 1 reading and literature</span> </h4>
          <p class="caption">California State Standards, based on the CCSS, serve as guidelines for schools and teachers.</p>
		  <p class="src">Based on &quot;eStandards,&quot; by Sacramento County Office of Education, 2018b (http&#8202;://estandards.scoecurriculum.net/index.html).</p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-1" data-type="tipTrigger">
	        <img src="figures/Figure_8.1.jpg" alt="Chart shows nine California reading and literature standards for first grade, labeled RL 1.1 through RL 1.9. In order, beginning with RL 1.1, these standards are: Ask and answer questions about key details in a text; Retell stories, including key details, and demonstrate understanding of their central message or lesson; Describe characters, settings, and major events in a story using key details; Identify words and phrases in stories or poems that suggest feelings or appeal to the senses; Explain major differences between books that tell stories and books that give information, drawing on a wide reading of a range of text types; Identify who is telling the story at various points in the text; Use illustrations and details in a story to describe its characters, setting, or events; (RL 1.8 is described as not applicable); and Compare and contrast the adventures and experiences of characters in stories." title="Chart shows nine California reading and literature standards for first grade, labeled RL 1.1 through RL 1.9. In order, beginning with RL 1.1, these standards are: Ask and answer questions about key details in a text; Retell stories, including key details, and demonstrate understanding of their central message or lesson; Describe characters, settings, and major events in a story using key details; Identify words and phrases in stories or poems that suggest feelings or appeal to the senses; Explain major differences between books that tell stories and books that give information, drawing on a wide reading of a range of text types; Identify who is telling the story at various points in the text; Use illustrations and details in a story to describe its characters, setting, or events; (RL 1.8 is described as not applicable); and Compare and contrast the adventures and experiences of characters in stories." id="fig_8.1"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-1">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.1.jpg" alt="Chart shows nine California reading and literature standards for first grade, labeled RL 1.1 through RL 1.9. In order, beginning with RL 1.1, these standards are: Ask and answer questions about key details in a text; Retell stories, including key details, and demonstrate understanding of their central message or lesson; Describe characters, settings, and major events in a story using key details; Identify words and phrases in stories or poems that suggest feelings or appeal to the senses; Explain major differences between books that tell stories and books that give information, drawing on a wide reading of a range of text types; Identify who is telling the story at various points in the text; Use illustrations and details in a story to describe its characters, setting, or events; (RL 1.8 is described as not applicable); and Compare and contrast the adventures and experiences of characters in stories." title="Chart shows nine California reading and literature standards for first grade, labeled RL 1.1 through RL 1.9. In order, beginning with RL 1.1, these standards are: Ask and answer questions about key details in a text; Retell stories, including key details, and demonstrate understanding of their central message or lesson; Describe characters, settings, and major events in a story using key details; Identify words and phrases in stories or poems that suggest feelings or appeal to the senses; Explain major differences between books that tell stories and books that give information, drawing on a wide reading of a range of text types; Identify who is telling the story at various points in the text; Use illustrations and details in a story to describe its characters, setting, or events; (RL 1.8 is described as not applicable); and Compare and contrast the adventures and experiences of characters in stories." id="fig_8.1"/>
 	  </div>
   </section>

        <div class="single-column">
	                <p class="txt">A standard such as this even suggests instructional activities related to other subject areas. For example, in art classes, students might be asked to draw facial expressions corresponding to emotional states described in the stories they are reading in language arts. And in mathematics, they might be asked to count the number of affect-linked words or phrases in different paragraphs or on different pages. And, depending on relevant mathematics objectives, they might be encouraged to add these or to subtract the smaller number from the larger.</p>

                    <p class="txt">Not only do state standards suggest a variety of instructional activities, but by the same token, they also serve as indispensable guidelines for the school&#39;s and the teacher&#39;s development of instructional objectives&mdash;and these are basic to sound educational assessment. Just as the main purpose of all forms of instruction is to improve learning, an overriding objective of assessment is to help learners reach instructional objectives. </p> 
			</div>		
           <button class="ec_collapse" type="button" name="ecbox8-1"><span>X</span> close</button>  
       </div> 
	     
	  </div> 

	  <div class="single-column">
         <h3 class="h3">National Science Standards</h3>
                    <p class="txt">Another example of common core standards are the <strong>Next Generation Science Standards (NGSS)</strong> (Next Generation Science Standards, 2018). These standards are the product of a collaboration among 26 states and various national groups, including the National Research Council, the American Association for the Advancement of Science, and the National Science Teachers Association. They are designed to improve the teaching of science in U.S. schools and are built around the notion that there are three distinct dimensions involved in learning science: </p>
<a id="p317"></a>
						<ol class="nl">
	                    <li class="li_text"><em>core ideas</em> in each of the main domains of science (physical science, earth and space science, life science, and engineering design); </li>
	                    <li class="li_text"><em>practices</em>, meaning what it is that scientists do and the range of skills and knowledge required to engage in these practices; and</li>
	                    <li class="li_text"><em>crosscutting</em>, which refers to making connections across the four domains of science.</li>
	                    </ol>

                    <p class="txt">Accordingly, each of the many standards is described in terms of all three of these dimensions. An illustration of a standard is shown in Table 8.1. More than two thirds of American states have now adopted the NGSS standards or have used them as a basis for developing their own science standards (National Science Teachers Association, 2018). </p>
    </div>

	<div class="tbl_scroll_on_mobile">				
     <table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 8.1:</span> <br />
          <span class="tbl_title">NGSS sample standard</span>
          </caption>
          <thead class="tbl_header">
            <tr>
              <th colspan="3">Middle school chemical reactions</th>       
            </tr>
          </thead>
          <tbody>
            <tr>
              <td colspan="3">Students who demonstrate understanding can:
			  	<ol class="table_nl"><li class="li_table">Analyze and interpret data on the properties of substances before and after the substances interact to determine if a chemical reaction has occurred</li>
			  		<li class="li_table">Develop and use a model to describe how the total number of atoms does not change in a chemical reaction and thus mass is conserved</li>
			  		<li class="li_table">Undertake a design project to construct, test, and modify a device that either releases or absorbs thermal energy by chemical processes</li>
                </ol>
			  
			  </td>
           
            </tr>
            <tr>
              <td width="30%">Science and engineering practices <br/><br/>e.g., developing and using models </td>
              <td width="30%">Disciplinary core ideas <br/><br/>e.g., each pure substance has characteristic and physical and chemical properties that can be used to identify it</td>
              <td width="30%">Crosscutting concepts <br/><br/>e.g., matter is conserved because atoms are conserved in physical and chemical processes</td>
            </tr>
          </tbody>
          <tfoot class="no-border">
            <tr>
              <td colspan="6"><em>Source: Based on &quot;Middle School Chemical Reactions,&quot; by National Science Teachers Association, 2018 (http<strong>&#8202;</strong>://ngss.nsta.org/DisplayStandard.aspx?view=topic&amp;id=25).</em></td>
            </tr>
          </tfoot>
     </table>
     </div>

</div>

      <h2 class="h2">Create Test Blueprints</h2>
      <div class="single-column">         
                    <p class="txt">The best way of ensuring that assessments are directed toward instructional objectives is to use <em>test blueprints</em>. These are basically tables of specifications for developing assessment instruments. They are typically based closely on the instructional objectives for a course or a unit. They may also reflect a list or a hierarchical arrangement of relevant intellectual or motor activities such as those provided by revisions of Bloom&#39;s taxonomy (described in Chapter&nbsp;4). </p>

                    <p class="txt">Many states provide blueprints for large-scale testing. For example, the Ohio Department of Education provides detailed, multigrade test blueprints for mathematics, English language arts, science, and social studies (Ohio Department of Education, 2018; Johnstone &amp; Thurlow,&nbsp;2012). </p>
<a id="p318"></a>
          <h3 class="h3">Examples of Test Blueprints</h3>
                    <p class="txt">Suppose you are teaching sixth-grade mathematics in California. California core standards list detailed objectives at that grade level for five different areas: ratios and proportional relationships, the number system, expressions and equations, geometry, and statistics and probability (Sacramento County Office of Education, 2018a). The first of six core standards for geometry reads as follows:</p>

                    <p class="ext"><em>Find the area of right triangles, other triangles, special quadrilaterals, and </em><em>polygons by composing into rectangles or decomposing into triangles and other </em><em>shapes; apply these techniques in the context of solving real-world and math</em><em></em><em>ematical problems.</em> (Sacramento County Office of Education, 2018a, p. 27)</p>

                    <p class="txt">Part of a test blueprint reflecting related learning objectives, based on Bloom&#39;s revised taxonomy, might look something like that in Table 8.2. Numbers in the grid indicate the number of test items for each category. Questions in parentheses are examples of the sorts of items that might be used to assess a specific cognitive process with respect to a given topic. Test blueprints of this kind might also include the value assigned to each type of test item.</p>
     </div>

<div class="tbl_scroll_on_mobile">
<table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 8.2</span> <br />
          <span class="tbl_title">Part of a sample test blueprint for a single geometry objective reflecting Bloom&#39;s revised taxonomy, cognitive domain</span>
          </caption>
          <thead class="tbl_header">
            <tr>
              <th>Topic</th>
              <th>Remembering</th>
              <th>Understanding</th>
              <th>Higher processes (applying, analyzing, evaluating, creating)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Right triangles</strong></td>
              <td>4 items (e.g., What is the formula for finding the area of a right triangle?)</td>
              <td> </td>
              <td>1 item (e.g., If you were building a house and could have a total of only 80 feet of perimeter wall, which of the following shapes would give you the largest area? Quadrilateral; polygon; square; right-angle triangle; other shape. Prove that your answer is correct.)</td>
              
            </tr>
            <tr >
              <td><strong>Quadrilaterals</strong></td>
              <td>3 items</td>
              <td> </td>
              <td> </td>          
            </tr>
            <tr>
              <td><strong>Other triangles</strong></td>
              <td>3 items</td>
              <td>2 items (e.g., Illustrate how you would find the area of an isosceles triangle by sketching a solution.) </td>
              <td>1 item</td>   
            </tr>
          </tbody>
     </table>
	 </div>

   <div class="single-column"> 
      <p class="txt">There are several other approaches to devising test blueprints. For example, the blueprint might list what learners are expected to understand, remember, or be able to do. In addition, the most useful blueprints will include an indication of how many items or questions there might be for each entry in the list and the test value for each. Figure 8.2 gives an example of a checklist that can be used as a blueprint for a unit covering part of the content of Chapter 2 in this text. (For other examples of test blueprints, see Tables 4.4 and 4.5 in Chapter 4.)</p>
   </div>
    
	<a id="p319"></a>

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.2</span><br />
          <span class="figure_title">Checklist test blueprint</span> </h4>
          <p class="caption">A teacher might use a checklist test blueprint such as this as an assessment rubric; a student might use it as a study guide and self-assessment guide.</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-2" data-type="tipTrigger">
	        <img src="figures/Figure_8.2.jpg" alt="Example of checklist test blueprint for a unit on &quot;Characteristics of Good Testing Instruments.&quot; The box is divided into three areas: fairness (for example, know what test fairness means), validity (for example, understand how test validity can be improved), and reliability (for example, know how reliability is calculated)." title="Example of checklist test blueprint for a unit on &quot;Characteristics of Good Testing Instruments.&quot; The box is divided into three areas: fairness (for example, know what test fairness means), validity (for example, understand how test validity can be improved), and reliability (for example, know how reliability is calculated)." id="fig_8.2"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-2">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.2.jpg" alt="Example of checklist test blueprint for a unit on &quot;Characteristics of Good Testing Instruments.&quot; The box is divided into three areas: fairness (for example, know what test fairness means), validity (for example, understand how test validity can be improved), and reliability (for example, know how reliability is calculated)." title="Example of checklist test blueprint for a unit on &quot;Characteristics of Good Testing Instruments.&quot; The box is divided into three areas: fairness (for example, know what test fairness means), validity (for example, understand how test validity can be improved), and reliability (for example, know how reliability is calculated)." id="fig_8.2"/>
 	  </div>
   </section>



   <div class="single-column"> 
      <h3 class="h3">Uses and Limitations of Test Blueprints</h3>
                    <p class="txt">A blueprint such as that shown in Figure 8.2 is useful for more than simply organizing and writing items for a test. It can also be adapted as a <em>rubric</em> that might be used for assessing learner progress, as well as for guiding learner efforts. Perhaps most important, it directs the attention of both teachers and learners toward the <em>higher</em> levels of mental activity. </p>

                    <p class="txt">In this connection, it is worth noting that despite teachers&#39; best intentions and their most carefully prepared test blueprints, assessments do not always reflect instructional objectives. For a variety of reasons, including that they are much easier to assess, the lowest levels of cognitive activity in Bloom&#39;s taxonomy (knowledge and comprehension) are often far more likely to be tapped by school assessments than are the higher levels. As Adams (2015) notes with respect to health education, in spite of the importance of higher level cognitive skills that would foster critical thinking and evaluative judgment, educators &quot;focus overwhelmingly on the lower levels of the taxonomy, knowledge and comprehension&quot; (p. 153). When Broman, Bernholt, and Parchmann (2015) looked at problems used to determine student interest and knowledge in science, they found that almost all test items were assessing lower rather than higher order thinking. Similarly, Jideani and Jideani (2012) report that knowledge- and comprehension-based assessments predominated in their investigation of assessment in food sciences classes. And this was true even though instructors <em>intended</em> that their students go beyond remembering and understanding&mdash;that they also learn to apply, analyze, evaluate, and create. </p>
   </div>


	<a id="p320"></a>
      <div class="inner-section">
        <h2 class="h2">Create Rubrics</h2>
          <div class="single-column">
                    <p class="txt">As we saw in Chapter 7, another important tool for assessment is the rubric. A <em>rubric</em> is a written guide for assessment. Rubrics are used extensively in performance assessments&mdash;in which, without such guides, evaluations are often highly subjective, unpredictable, and unfair. Inconsistent assessments are the hallmark of a lack of test reliability. And measures that are unreliable are also invalid.</p>
                    <p class="txt">Rubrics, like test blueprints, are a guide not only for assessment but also for instruction. Also like blueprints, they are typically given to the learner before instruction begins. They often tell the student what is important and expected far more clearly than might be expressed verbally.</p>
		  </div>
           

	 <div class="text_container">
        <h4><button class="ec_expand" aria-expanded="false" aria-controls="ecbox8-2">LEARN MORE</button></h4>
       <div id="ecbox8-2">
		          <div class="single-column">
                     <p class="txt">Table 8.3 is an example of a rubric that might be used for evaluating an <em>analysis paper</em> at the sixth-grade level. Developing detailed rubrics of this kind for every instructional unit simplifies the teacher&#39;s instruction and assessment tasks enormously. It makes lesson planning straightforward and clear, dramatically shortens the amount of time that might otherwise be spent in planning and developing assessment instruments. and is one of the surest ways of increasing test reliability, validity, and fairness.</p>

                     <p class="txt">The rubric shown in Table 8.3 is basically a holistic rubric: It is designed to provide a single, summative score. Holistic rubrics are most appropriate when assessing work for which there is no single correct response and the focus is on overall proficiency. Analytic rubrics, which provide ratings for different criteria, are most appropriate when there is a need for consistent scoring or when the objective is to provide learners and teachers with more detailed feedback related to strengths and weaknesses. Analytic rubrics often take the form of a grid that serves as a guide for providing assessments relating to multiple criteria. For example, Table 8.4 shows how the holistic rubric in Table 8.3 can be made into an analytic rubric.</p>
                 </div>
  	<a id="p321"></a>  
	
	<div class="tbl_scroll_on_mobile">
       <table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 8.3</span> <br />
          <span class="tbl_title">Holistic rubric for evaluation of an analysis paper</span>
          </caption>
          <thead class="tbl_header">
            <tr>
              <th>Your analysis paper will be evaluated for each of the following:</th>
              <th>Points</th>            
            </tr>
          </thead>
          <tbody>
            <tr class="no_border_td">
              <td >1.&ensp; Purpose clearly stated in two or three sentences</td>
			  <td >10</td>
			 </tr>
            <tr class="no_border_td">
              <td >2.&ensp; Information provided to support and justify the purpose</td>
			  <td >10</td>
			 </tr>
            <tr class="no_border_td">
              <td>3.&ensp; Relevant information by way of facts, examples, and research included</td>
			  <td>20</td>
			 </tr>
            <tr class="no_border_td">
              <td>4.&ensp; Absence of irrelevant information</td>
			  <td>5</td>
			 </tr>
            <tr class="no_border_td">
              <td>5.&ensp; Analysis presented in coherent, logical fashion evident in paragraphing and sequencing</td>
			  <td>20</td>
			 </tr>
            <tr class="no_border_td">
              <td>6.&ensp; Few grammatical and spelling errors (up to 10 points may be deducted)</td>
			  <td>0</td>
			 </tr>
            <tr class="no_border_td">
              <td>7.&ensp; Clear, well-supported conclusions</td>
			  <td>15</td>
			 </tr>
            <tr class="no_border_td">
              <td>8.&ensp; High interest level</td>
			  <td>20</td>
             
			 </tr>
            <tr class="background_tr">
              <td class="background_td">TOTAL </td>
              <td class="background_td">100 </td>
            </tr>
 
          </tbody>  
     </table>
     </div>
      
    <div class="tbl_scroll_on_mobile">
    <table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 8.4</span> <br />
          <span class="tbl_title">Analytic rubric for evaluation of an analysis paper</span>
          </caption>
		  <tbody>
		        <tr><th class="white" colspan="5">Not all cells in the grid have been filled out. You might try completing them or using the grid as a template for an analytic rubric in one of your areas of expertise.</th></tr>
		        <tr><th class="white" colspan="5">Your analysis paper will be evaluated for each of the following:</th></tr>
	    
            <tr>
              <th class="heading">Criteria</th>
              <th class="heading">Adequate<br/>(50&ndash;59%)</th>
              <th class="heading">Competent<br/>(60&ndash;69%)</th>
              <th class="heading">Good<br/>(70&ndash;79%)</th>
              <th class="heading">Excellent<br/>(80&ndash;89%)</th>
            </tr>
          
           
            <tr class="no_background_tr">
              <td class="no_background_td"><ol class="tbl_nl2"><li class="li_table">Clear statement of purpose</li></ol> </td>
              <td class="no_background_td">Purpose implied rather than stated. </td>
              <td class="no_background_td">Statement of purpose brief and not entirely clear. </td>
              <td class="no_background_td">Purpose made clear but not early enough. </td>
              <td class="no_background_td">One or two sentences focus directly on the purpose early in the presentation. </td>
            </tr>
            <tr class="background_tr">
              <td class="background_td"><ol class="tbl_nl2" start="2"><li class="li_table">Information provided to support statement of purpose</li></ol> </td>
              <td class="background_td">Lack of convincing supporting information. </td>
              <td class="background_td">Some minimal support provided for the analysis. </td>
              <td class="background_td">Purpose supported with convincing information. </td>
              <td class="background_td">Purpose exceptionally clearly justified and explained. </td>
            </tr>
              <tr class="no_background_tr">
              <td class="no_background_td"><ol class="tbl_nl2" start="3"><li class="li_table">Relevant research in way of facts, examples, and research included </li></ol></td>
              <td class="no_background_td">A preponderance of opinion; lack of relevant research. </td>
              <td class="no_background_td">Some documented research included. </td>
              <td class="no_background_td">Most facts, examples, and research documented. </td>
              <td class="no_background_td">All facts, examples, and research well documented. </td>
            </tr>
            <tr class="background_tr">
              <td class="background_td"><ol class="tbl_nl2" start="4"><li class="li_table">Absence of irrelevant information</li></ol> </td>
              <td class="background_td">Main thrust of paper built on irrelevant information and opinion. </td>
              <td class="background_td">As much as 10%. </td>
              <td class="background_td"> </td>
              <td class="background_td">  </td>
            </tr>
            <tr class="no_background_tr">
              <td class="no_background_td"><ol class="tbl_nl2" start="5"><li class="li_table">Analysis presented in coherent, logical fashion</li></ol> </td>
              <td class="no_background_td"> </td>
              <td class="no_background_td">  </td>
              <td class="no_background_td">  </td>
              <td class="no_background_td"> </td>
            </tr>
			<tr class="background_tr">
			  <td class="background_td"><ol class="tbl_nl2" start="6"><li class="li_table">Few grammatical and spelling errors</li></ol> </td>
              <td class="background_td"> </td>
              <td class="background_td">  </td>
              <td class="background_td">  </td>
              <td class="background_td"> </td>
            </tr>
			<tr class="no_background_tr">
			  <td class="no_background_td"><ol class="tbl_nl2" start="7"><li class="li_table">Clear, well-supported conclusions</li></ol> </td>
              <td class="no_background_td"> </td>
              <td class="no_background_td">  </td>
              <td class="no_background_td">  </td>
              <td class="no_background_td"> </td>
            </tr>
			<tr class="background_tr">
			  <td class="background_td"><ol class="tbl_nl2" start="8"><li class="li_table">High interest level</li></ol> </td>
              <td class="background_td"> </td>
              <td class="background_td">  </td>
              <td class="background_td">  </td>
              <td class="background_td"> </td>
            </tr>
          </tbody>
         
     </table>
	 </div>
			
           <button class="ec_collapse" type="button" name="ecbox8-2"><span>X</span> close</button>  
       </div> 
	    
	  </div> 

	  </div>


   <h2 class="h2">Approaches to Classroom Assessment</h2>
     <div class="single-column"> 

	  <p class="txt">As we saw earlier, assessment can serve at least three different functions in schools. </p>

      	<ol class="nl">
	          <li class="li_text">Assessment might be used for placement purposes before instruction (placement assessment). <em>Diagnostic assessment</em>, which is a form of preassessment used to identify strengths and weakness and to detect learning difficulties, is generally considered a form of placement assessment.</li>
	          <li class="li_text">Assessment might assume a helping role when feedback from ongoing assessments is given to learners to help them improve their learning and when ongoing assessments suggest to the teacher how instructional strategies might be modified (formative assessment).</li>
	          <li class="li_text">School assessments often serve to provide a summary of the learner&#39;s performance and achievements. These unit- or year-end assessments are usually the basis for grades (summative assessment).</li>
	   </ol>

	 </div>

  	<a id="p322"></a> 
     	 <figure class="photo-right"><img src="Images/8.1.jpg" alt="Two students perform a chemistry experiment as their teacher observes." title="Two students perform a chemistry experiment as their teacher observes." id="img8.1"/>
        <figcaption>
          <p class="cr">Monkeybusinessimages/iStock/Getty Images Plus</p>
          <p class="caption">Because they are closer to real-life situations, performance-based assessments are often described as more <em>authentic</em> assessments. Some of the most important learning targets associated with the chemistry class to which these students belong cannot easily be assessed with a selected-response test. The test is in the performance.</p>
        </figcaption>
      </figure>

      <div class="single-column"> 
          <p class="txt">Teacher-made assessments, no matter to which of these uses they are put, can take any one of several forms. Among them are performance-based assessments, <strong>selected-response assessments</strong>, and <strong>constructed-response assessments</strong>.</p>
      </div>



</div>
<script type="text/javascript" src="Scripts/app-min.js"></script>
</section>







<!--
/////////////////////////////////////////////////////////////
///////////////////////// 8.2 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->

<section class="page" id="sec8.2">
  <div class="section-header">

    <h1 class="h1"><span class="sec_number">8.2</span> <span class="sec_title">Constructed- and Selected-Response Assessments</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What kinds of teacher-made assessment options are available?</span></h2>
  </div>
   <div class="section-lead">
       <p class="intro">Test items are the basic units that make up an assessment. These are often referred to as <em>test </em><em>questions</em>, although many assessment items are not questions at all; instead, they are directions, instructions, or requests.</p>     
  </div>

<div class="section-body">
       <div class="single-column">        
          <p class="txt">Some teacher-made assessments include several different kinds of items. Often, however, they are made up of a single sort of item. Test items can generally be divided into two broad categories: those that ask students to select a correct answer, termed <em>selected-response assessments</em>, and those that require examinees to produce (construct) the correct response, usually in writing but also sometimes orally. These are referred to as <em>constructed-response assessments</em>. </p>
       </div>
  

      <div class="inner-section">
        <h2 class="h2">Selected-Response Assessments</h2>
          <div class="single-column">
     
                    <p class="key-point">Selected-response items are generally considered to be more objective than constructed-response items, simply because each item usually has a single clearly correct answer. </p>
                    <p class="txt">In most cases, if more than one response is correct, that is taken into account in scoring. As a result, answer keys for assessments made up of selected-response items tend to be simple and exact. No matter which examiner scores a selected-response assessment, results should be identical. </p>
                    <p class="txt">There are four principal kinds of selected-response items.</p>
	<a id="p323"></a> 
						<ol class="nl">
							<li class="li_text"><strong>Multiple-choice items</strong> ask students to select which of several alternatives is the correct response to a statement or question.</li>
							<li class="li_text"><strong>True&ndash;false items</strong>, also called <em>binary-choice items</em>, ask the responder to make a choice between two alternatives, such as true or false.</li>
							<li class="li_text"><strong>Matching-test items</strong> present two or more corresponding lists, from which the examinee must select those that match.</li>
							<li class="li_text"><strong>Interpretive items</strong> are often similar to multiple-choice items, except that they provide information that examinees need to interpret in order to select the correct alternative. Information may be in the form of a chart, graph, paragraph, video, or audio recording.</li>
	                   </ol>
		  </div>
     </div>



  <h2 class="h2">Constructed-Response Assessments</h2>
  <div class="single-column">
                    <p class="key-point">Constructed-response items are more subjective than selected-response items because they ask learners to generate their own responses. As a result, they often have more than one correct answer. </p>

                    <p class="txt">Test makers distinguish between two broad forms of constructed-response items based largely on the length of the answer that is required. Thus, there are <strong>short-answer items</strong> requiring brief responses&mdash;often no longer than a single paragraph&mdash;and <strong>essay items</strong> that ask the student to write a longer, essay-form response for the item. Figure 8.3 summarizes these distinctions.</p>
  </div>

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.3</span><br />
          <span class="figure_title">Types of assessment items</span> </h4>
          <p class="caption">There are four common types of selected-response items and two kinds of constructed-response items. Some tests include more than one type of assessment item.</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-3" data-type="tipTrigger">
	        <img src="figures/Figure_8.3.jpg" alt="Branching diagram that begins with a box labeled &quot;types of assessment items.&quot; From this box are two lines: one to a box labeled &quot;selected-response (more objective), and one to a box labeled &quot;constructed-response (less objective).&quot; There are four subcategories of selected-response items. These boxes are labeled &quot;multiple choice,&quot; &quot;true&ndash;false,&quot; &quot;matching,&quot; and &quot;interpretive.&quot; There are two subcategories of constructed-response items. These boxes are labeled &quot;short-answer&quot; and &quot;essay.&quot;" title="Branching diagram that begins with a box labeled &quot;types of assessment items.&quot; From this box are two lines: one to a box labeled &quot;selected-response (more objective), and one to a box labeled &quot;constructed-response (less objective).&quot; There are four subcategories of selected-response items. These boxes are labeled &quot;multiple choice,&quot; &quot;true&ndash;false,&quot; &quot;matching,&quot; and &quot;interpretive.&quot; There are two subcategories of constructed-response items. These boxes are labeled &quot;short-answer&quot; and &quot;essay.&quot;" id="fig_8.3" />
	     </a>
   </figure>

   <section class="tipBox" id="fig8-3">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.3.jpg" alt="Branching diagram that begins with a box labeled &quot;types of assessment items.&quot; From this box are two lines: one to a box labeled &quot;selected-response (more objective), and one to a box labeled &quot;constructed-response (less objective).&quot; There are four subcategories of selected-response items. These boxes are labeled &quot;multiple choice,&quot; &quot;true&ndash;false,&quot; &quot;matching,&quot; and &quot;interpretive.&quot; There are two subcategories of constructed-response items. These boxes are labeled &quot;short-answer&quot; and &quot;essay.&quot;" title="Branching diagram that begins with a box labeled &quot;types of assessment items.&quot; From this box are two lines: one to a box labeled &quot;selected-response (more objective), and one to a box labeled &quot;constructed-response (less objective).&quot; There are four subcategories of selected-response items. These boxes are labeled &quot;multiple choice,&quot; &quot;true&ndash;false,&quot; &quot;matching,&quot; and &quot;interpretive.&quot; There are two subcategories of constructed-response items. These boxes are labeled &quot;short-answer&quot; and &quot;essay.&quot;" id="fig_8.3"/>
 	  </div>
   </section>


</div>
  <script type="text/javascript" src="Scripts/app-min.js"></script>


</section>



<!--
/////////////////////////////////////////////////////////////
///////////////////////// 8.3 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->


<section class="page" id="sec8.3">
	<a id="p324"></a>
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">8.3</span> <span class="sec_title">Comparing Objective Versus Essay and Short-Answer Tests</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What are the benefits and drawbacks of objective 
and subjective tests?</span></h2>
  </div>

  <div class="section-lead">
  <p class="intro">The selected-response (objective) items and the more subjective essay and short-answer items shown in Figure 8.3 can both be used to measure almost any significant aspect of students&#39; behavior. It is true, however, that some instructional objectives are more easily assessed with one type of item than with the other.</p>  
  </div>

<div class="section-body">

<h2 class="h2">Uses, Strengths, Limitations</h2>
   <div class="single-column">
          <p class="txt">The most important uses, strengths, and limitations of these approaches are described in the following list. While the descriptions can serve as a guide in deciding which to use in a given situation, most good assessment programs use a variety of approaches.</p>

            <h3 class="h3">1. Higher Level Skills</h3>
                    <p class="key-point">It is easier to tap higher level processes (analysis, synthesis, and evaluation) with an essay examination. </p>
                    <p class="txt">These can more easily be constructed to allow students to organize knowledge, make inferences from it, illustrate it, apply it, and extrapolate from it. </p>
                    <p class="txt">Still, good multiple-choice items can be designed to measure much the same things as constructed-response items. Consider, for example, the following multiple-choice item.</p>
                    <p class="ext"><em>Harvey is going on a solo fishing and camping trip in the far north. What equipment and supplies should he bring?</em><br/>
                    a.&ensp;&ensp;rainproof tent; rainproof gear; fishing equipment; food<br/>
                    b.&ensp;&ensp;an electric outboard motor; a dinner suit; a hunting rifle<br/>
                    c.&ensp;&ensp;some books; a smartphone; fishing equipment; money<br/>
                    *d.&ensp;&ensp;an ax; camping supplies; fishing equipment; warm, waterproof clothing
					</p>
                    <p class="txt">Answering this item requires that the student analyze the situation, imagine different scenarios, and apply previously acquired knowledge to a new situation. In much the same way, it is possible to design multiple-choice items that require that students synthesize ideas and perhaps even that they create new ones.</p>
                    <p class="txt">As we saw, however, the evidence indicates that most selected-response assessments tend to tap remembering&mdash;the lowest level in Bloom&#39;s taxonomy. Most items simply ask the student to name, recognize, relate, or recall. Few classroom teachers can easily create items that assess higher cognitive processes.</p>
	<a id="p325"></a>
            <h3 class="h3">2. Content Coverage </h3>
                    <p class="txt">Because essay and short-answer exams usually consist of only a few items, the range of skills and of information sampled is often less than what can be sampled with more objective tests. </p>
                    <p class="key-point">Selected-response assessments permit coverage of more content per unit of testing time.</p>

            <h3 class="h3">3. Variation in Responses</h3>
                    <p class="key-point">Essay examinations allow for more divergence. </p>
                    <p class="txt">They make it possible for students to produce unexpected and unscripted responses. Those who do not like to be limited in their answers often prefer essays over more objective assessments. Conversely, those who have difficulty expressing themselves when writing often prefer selected-response assessments. </p>
                    <p class="txt">Interestingly, when Mingo, Chang, and Williams (2018) asked 161 educational psychology undergraduates whether they preferred essay or multiple-choice exams, the students preferred the constructed-response (essay) exams. However, their preferences did not correlate with their performance. That is, those who preferred one form of testing over another did not consistently do better on the form they preferred. </p>

            <h3 class="h3">4. Time to Construct</h3>
                    <p class="key-point">Constructing an essay examination is considerably easier and less time consuming than making up an objective examination. </p>
                    <p class="txt">In fact, an entire test with an essay format can often be written in the same time it would take to write no more than two or three good multiple-choice items.</p>

            <h3 class="h3">5. Difficulty in Scoring</h3>
                    <p class="key-point">Scoring essay examinations usually requires much more time than scoring objective tests, especially when classes are large. </p>
                    <p class="txt">This is especially true when tests are scored electronically. When classes are very small, however, the time required for making and scoring an essay test might be less than that required for making and scoring a selected-response test. The hypothetical relationship between class size and total time for constructing and scoring constructed-response and selected-response tests is shown in Figure 8.4.</p>
     </div>

	<a id="p326"></a>

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.4</span><br />
          <span class="figure_title">Construction and scoring time: Essays versus objective assessments</span> </h4>
          <p class="caption">A graph showing the hypothetical relationship between class size and total time required for constructing and scoring selected response tests (multiple-choice for example) and constructed-response tests (essay tests). Preparation and scoring time for essay tests increases dramatically with larger class size but does not change appreciably for machine-scored objective tests.</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-4" data-type="tipTrigger">
	        <img src="figures/Figure_8.4.jpg" alt="Line graph in which the x-axis is labeled &quot;number of students from low to high&quot; and the y-axis is labeled &quot;total time for construction and scoring, from low to high.&quot; Two lines are plotted on the graph: one for essay assessment and one for objective assessment. The essay line starts near the bottom left and runs through the top right of the graph because scoring time increases with increasing numbers of students. The objective assessment line starts in the middle of the left side of the graph and moves almost horizontally (slightly upward) to the right side of the graph, indicating that the size of the class has little effect on construction and scoring time." title="Line graph in which the x-axis is labeled &quot;number of students from low to high&quot; and the y-axis is labeled &quot;total time for construction and scoring, from low to high.&quot; Two lines are plotted on the graph: one for essay assessment and one for objective assessment. The essay line starts near the bottom left and runs through the top right of the graph because scoring time increases with increasing numbers of students. The objective assessment line starts in the middle of the left side of the graph and moves almost horizontally (slightly upward) to the right side of the graph, indicating that the size of the class has little effect on construction and scoring time." id="fig_8.4"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-4">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.4.jpg" alt="Line graph in which the x-axis is labeled &quot;number of students from low to high&quot; and the y-axis is labeled &quot;total time for construction and scoring, from low to high.&quot; Two lines are plotted on the graph: one for essay assessment and one for objective assessment. The essay line starts near the bottom left and runs through the top right of the graph because scoring time increases with increasing numbers of students. The objective assessment line starts in the middle of the left side of the graph and moves almost horizontally (slightly upward) to the right side of the graph, indicating that the size of the class has little effect on construction and scoring time." title="Line graph in which the x-axis is labeled &quot;number of students from low to high&quot; and the y-axis is labeled &quot;total time for construction and scoring, from low to high.&quot; Two lines are plotted on the graph: one for essay assessment and one for objective assessment. The essay line starts near the bottom left and runs through the top right of the graph because scoring time increases with increasing numbers of students. The objective assessment line starts in the middle of the left side of the graph and moves almost horizontally (slightly upward) to the right side of the graph, indicating that the size of the class has little effect on construction and scoring time." id="fig_8.4"/>
 	  </div>
   </section>
	
	 
	  <div class="single-column">
	            <h3 class="h3">6. Reliability</h3>
                    <p class="key-point">The reliability of essay examinations is much lower than that of objective tests, primarily because of the subjectivity involved in scoring them. </p>
                    <p class="txt">In addition, suggests Brown (2010), examiners often overemphasize the language aspects of the essays they are scoring. As a result, they pay less attention to the content, and the validity of the grades suffers.</p>
                    <p class="txt">Some researchers have begun to develop computer programs designed to score constructed-response test items (Gerard &amp; Linn, 2016). Typically, however, use of these is limited to questions for which acceptable responses are highly constrained and easily recognizable. Ha and Nehm (2016) report that these scoring programs are often inaccurate and are especially sensitive to misspelled words. In their investigation, they found that ELL students produced approximately twice as many misspelled words as non-ELL students. As a result, electronic assessments were especially inaccurate and unfair for them. However, recent advances in computer-scoring of constructed-response assessments suggests that reliable, consistent, and fair measurements are not entirely out of reach (Gierl, Latifi, Lai, Boulais, &amp; De Champlain, 2014).    </p>
	  </div>


 <div class="inner-section">
	  <h2 class="h2">Which Approach Is Best?</h2>
	  <div class="single-column">
            <p class="txt">The simple answer is, it depends. Few teachers will ever find themselves in situations in which they must always use either one form of assessment or the other. Some class situations, particularly those in which size is a factor, may lend themselves more readily to objective formats; in other situations, essay formats may be better; sometimes a combination of both may be desirable. The important point is that each form of assessment has advantages and disadvantages. A good teacher should endeavor to develop the skills necessary for constructing the best items possible in a variety of formats without becoming a passionate advocate of one over the other.</p>
<a id="p327"></a>
           <p class="txt">The good teacher also needs to keep in mind that there are many alternatives to assessment other than the usual teacher-made or commercially prepared tests. In the final analysis, the assessment procedure chosen should be determined by the goals of the instructional process and the purposes for which the assessment will be used. Also, classroom teachers need not make all decisions relating to assessment instruments and approaches or to important matters such as curriculum details, learning objectives, and instructional approaches. Help, support, and advice are available from many sources, including other teachers, administrators, parents, and sometimes even students. In many schools, formal associations, termed professional learning communities, are an extremely valuable resource (see <em>In the Classroom: </em><em>Professional Learning Communities</em>).</p>

					 <p class="txt"><a class="trigger-readmore" href="#tip8-1" data-type="tipTrigger">In the Classroom: Professional Learning Communities</a></p>

      </div>

      <section class="tipBox" id="tip8-1">
      <div class="box-6">
		  <h3 class="h3">In the Classroom: Professional Learning Communities</h3>
          <div class="single-column">
            <p class="txt">A PLC is a grouping of educators, both new and experienced&mdash;and sometimes of parents as well&mdash;who come together to talk about, reflect on, and share ideas and resources in an effort to improve curriculum, learning, instruction, and assessment. Typically, PLCs are formal organizations within schools or school systems. Principals or other school leaders such as department heads or superintendents are usually centrally involved in setting up a PLC (Vanblaere &amp; Devos, 2018). These are geared toward establishing collaboration as a basis for promoting student learning. PLCs are characterized by the following elements.</p>
          </div>
			  <figure class="photo-right"><img src="Images/8.2.jpg" alt="Group of teachers sit around a table having a discussion." title="Group of teachers sit around a table having a discussion." id="img8.2"/>
           <figcaption>
          <p class="cr">Bowdenimages/iStock/Getty Images Plus</p>
          <p class="caption">PLCs are organized groups of educators and other stakeholders who meet regularly to reflect and collaborate on improving curriculum, learning, instruction, and assessment. They are a powerful strategy for educational improvement.</p>
           </figcaption>
          </figure>
          <div class="single-column">
        	<ul class="bl">
			<li class="li_text">supportive and collaborative educational leadership</li>
			<li class="li_text">sharing of goals and values</li>
			<li class="li_text">collaborative creativity and innovation</li>
			<li class="li_text">sharing of personal experiences</li>
			<li class="li_text">sharing of instructional approaches and resources</li>
			<li class="li_text">sharing of assessment strategies and applications</li>
			<li class="li_text">a high degree of mutual support</li>
           </ul>
<a id="p328"></a>
            <p class="txt">On occasion, PLCs might even consist of preservice teachers. Kuehl (2018) describes a study in which graduate students who were beginning a 12-month teacher education program were paired with gifted fifth- and sixth-grade students. Both the teachers and the students had been asked to read the novel <em>Bud, Not Buddy</em> by Christopher Paul Curtis. Teachers interacted with their student partners electronically, responding to student questions and comments. All teachers were also part of a PLC that met regularly so the teachers could exchange information and ideas and collaborate in their interactions with their student partners. At the end of the course, teachers were asked to write reflective essays describing their reaction to the PLC. Interestingly, teacher reflections focused almost exclusively on the positive aspects of working within the PLC. Most felt that discussions in the PLC had been highly beneficial and that they had learned more from PLC interactions than from their one-on-one work with the students. </p>

            <p class="txt">Evidence suggests that PLCs are a powerful means of professional development and support and a compelling strategy for educational change and improvement. For example, Shanks (2016) describes how teacher candidates can use PLCs during their training to improve their lesson planning, assessment, and classroom instruction. Teachers in all schools, suggests Bradea (2016), should view themselves as part of a PLC. And all should collaborate with colleagues, administrators, and parents, in the interests of better schools. </p>
 	      </div>
 	</div>

   </section> 
   
 	  <div class="single-column">
              <p class="txt">Table 8.5 shows how different types of assessment might be used to tap learning objectives relating to Bloom&#39;s revised taxonomy (discussed in Chapter 4). Note that the most common assessments for higher mental processes such as analyzing, evaluating, and creating are either constructed-response or performance-based assessments. However, as we will see in the next section, selected-response assessments such as multiple-choice tests can also be designed to tap these processes.</p>
	   </div>
<a id="p329"></a>

<div class="tbl_scroll_on_mobile">
	   <table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 8.5</span> <br />
          <span class="tbl_title">What assessment approach to use</span>
          </caption>
          <thead class="tbl_header">
            <tr>
              <th>Bloom&#39;s revised taxonomy of educational objectives</th>
              <th>Verbs related to each objective<br/><br/>Students are asked to:</th>
              <th>Some useful approaches to assessment</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Remembering </td>
              <td>copy, duplicate, list, learn, replicate, imitate, memorize, name, order, relate, reproduce, repeat, recognize, . . . </td>
              <td>Selected-response assessments, including multiple-choice, true&ndash;false, matching, and interpretive </td>
            </tr>
            <tr>
              <td>Understanding </td>
              <td>indicate, know, identify, locate, recognize, report, explain, restate, review, describe, distinguish, . . . </td>
              <td>Selected-response assessments that require learner to locate, identify, recognize, . . . Constructed-response assessments, including short-answer and longer essay items, in which students are asked to explain, describe, compare, . . . </td>
            </tr>
            <tr>
              <td>Applying </td>
              <td>demonstrate, plan, draw, outline, dramatize, choose, sketch, solve, interpret, operate, do, . . . </td>
              <td>Written constructed-response assessments, in which students are required to describe prototypes or simulations showing applications Performance assessments, in which learners demonstrate an application, perhaps by sketching or dramatizing it </td>
            </tr>
			<tr>
              <td>Analyzing </td>
              <td>calculate, check, categorize, balance, compare, contrast, test, differentiate, examine, try, . . .  </td>
              <td>Written assessments requiring comparisons, detailed analyses, advanced calculations Performance assessments involving activities such as debating or designing concept maps </td>
            </tr>
			<tr>
              <td>Evaluating </td>
              <td>assess, choose, appraise, price, defend, judge, rate, calculate, support, criticize, predict, . . . </td>
              <td>Written assessments requiring judging, evaluating, critiquing Performance assessments using portfolio entries reflecting opinions, reflections, appraisals, reviews, etc. </td>
            </tr>
			<tr>
              <td>Creating </td>
              <td>arrange, write, produce, make, design, formulate, compose, construct, build, generate, craft, . . . </td>
              <td>Written assignments perhaps summarizing original research projects Performance assessments involving original output such as musical compositions, written material, designs, computer programs, etc. </td>
            </tr>
          </tbody>
     </table>
   </div>

    </div>

</div>
<script type="text/javascript" src="Scripts/app-min.js"></script>
</section>


<!--
/////////////////////////////////////////////////////////////
///////////////////////// 8.4 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->


<section class="page" id="sec8.4">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">8.4</span> <span class="sec_title">Developing Selected-Response Assessments: Multiple-Choice Items</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What are some guidelines for constructing multiple-choice items?</span></h2>
  </div>
  <div class="section-lead">
       <p class="intro">As we noted, selected-response assessments tend to be more objective than constructed-response assessments. After all, most of them have only one correct answer. Among the most common of the highly objective selected-response assessments is that consisting of multiple-choice items.  </p>

   </div>

<a id="p330"></a>
<div class="section-body">
        <div class="single-column">
                   <p class="txt">These are items that have a <strong>stem</strong>&mdash;often a question or an incomplete statement&mdash;followed by a series of possible responses referred to as <strong>alternatives</strong>. There are usually four or five alternatives, only one of which is normally correct; the others are termed <strong>distracters</strong>.</p>

                    <p class="txt">On occasion, some multiple-choice tests may contain more than one correct alternative. These, as Pae (2014) found, are usually more difficult than items with a single correct answer, providing responders are required to select all correct alternatives for the item to be marked correct. Pae tested the English speaking and writing skills of 206 participants who spoke English but whose first language was Chinese, French, Hebrew, or Korean. Of the different multiple-choice item formats used, those that allowed multiple answers were the most difficult. </p>

                    <p class="txt">Multiple-choice stems and alternatives can take a variety of forms. Stems might consist of questions, statements requiring completion, or negative statements. Alternatives might be best answer, combined answers, or single answers. Examples of each of these items are shown in Figure 8.5.</p>
        </div>   

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.5</span><br />
          <span class="figure_title">Examples of multiple-choice items</span> </h4>
          <p class="caption">Stems and alternatives in multiple-choice items can take a variety of forms. In these examples, the alternatives are always ordered alphabetically or numerically. This is a precaution against establishing a pattern that might provide a clue for guessing the correct response. (Correct responses are checked.)</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-5" data-type="tipTrigger">
	        <img src="figures/Figure_8.5.jpg" alt="Six boxes showing examples of multiple-choice items. The boxes are labeled &quot;incomplete statement stem,&quot; &quot;question stem,&quot; &quot;negative statement stem,&quot; &quot;best answer alternative,&quot; &quot;combined answer alternative,&quot; and &quot;single answer alternative.&quot; In each box is an example of the item." title="Six boxes showing examples of multiple-choice items. The boxes are labeled &quot;incomplete statement stem,&quot; &quot;question stem,&quot; &quot;negative statement stem,&quot; &quot;best answer alternative,&quot; &quot;combined answer alternative,&quot; and &quot;single answer alternative.&quot; In each box is an example of the item." id="fig_8.5"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-5">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.5.jpg" alt="Six boxes showing examples of multiple-choice items. The boxes are labeled &quot;incomplete statement stem,&quot; &quot;question stem,&quot; &quot;negative statement stem,&quot; &quot;best answer alternative,&quot; &quot;combined answer alternative,&quot; and &quot;single answer alternative.&quot; In each box is an example of the item." title="Six boxes showing examples of multiple-choice items. The boxes are labeled &quot;incomplete statement stem,&quot; &quot;question stem,&quot; &quot;negative statement stem,&quot; &quot;best answer alternative,&quot; &quot;combined answer alternative,&quot; and &quot;single answer alternative.&quot; In each box is an example of the item." id="fig_8.5"/>
 	  </div>
   </section>
	

 
  <a id="p331"></a>
    <div class="inner-section">
      <h2 class="h2">Guidelines for Constructing Multiple-Choice Items</h2>
      <div class="single-column">
           <p class="txt">Writing good multiple-choice items requires attention to a number of important guidelines. Many of them involve common sense (which makes them no less valid).</p>

           <h3 class="h3">1. Provide Clear Stems and Answer Choices</h3>
                    <p class="txt">Both stems and alternatives should be clearly worded, unambiguous, grammatically correct, specific, and at the appropriate level of difficulty. Also, they should carefully avoid the use of <em>double-barreled</em> stems and alternatives. A double-barreled statement or question is one that involves more than one issue but allows for only one interpretation and, accordingly, a single answer. For example, the question <em>Do you agree that gasoline should be less expensive and bet</em><em>ter?</em> is doubled barreled. It asks whether you agree that gasoline should be less expensive. But it also asks a second question: Do you agree that it should be better? In a multiple-choice item, an alternative such as <em>formative assessment occurs during instruction and is used for providing a grade</em> is double barreled.</p>

                    <p class="txt">Stems should not only be clear and <em>single barreled</em>, they should also be meaningful by themselves. Compare, for example, the following two items.</p>


                   	<ol>
                    	       <li class="li_text"><em>In the story</em> The Red Rat<em>, how did Sally feel toward Angela after her accident?</em>
							   	<ul class="minus_top_margin">
							   		<li class="li_text">a. sad</li>
							   		<li class="li_text">b. angry</li>
							   		<li class="li_text">c. jealous</li>
									<li class="li_text">d. confused</li>
							   	</ul></li>
							   

                    	       <li class="li_text"><em>In the story</em> The Red Rat<em>, how did Sally feel toward Angela after Angela&#39;s accident?</em>
							   	<ul class="minus_top_margin">
							   		<li class="li_text">a. sad</li>
							   		<li class="li_text">b. angry</li>
							   		<li class="li_text">c. jealous</li>
									<li class="li_text">d. confused</li>
							   	</ul>							   
							   
							   </li>
                    </ol>


                    <p class="txt">The problem with the first stem is that the pronoun <em>her</em> has an ambiguous referent. Does the question refer to Sally&#39;s accident or to Angela&#39;s? The second stem corrects that error. </p>

                    <p class="txt">Similarly, stems that use the word <em>they</em> without a specific context or reference are sometimes vague and misleading. For example, the true&ndash;false question <em>Is it true that they say you should avoid double negatives?</em> might be true or false, depending on who <em>they</em> is. If <em>they</em> refers to most authors of assessment textbooks, the correct answer is <em>true</em>. But if <em>they</em> refers to the Mowrat family, the correct answer would be <em>false</em>: They don&#39;t never say don&#39;t use no double negatives!</p>

           <h3 class="h3">2. Avoid Double Negatives</h3>
                    <p class="txt">But seriously, don&#39;t use double negatives when writing multiple-choice items. They are highly confusing and should be avoided at all costs. Common, easily found examples of double and even triple negatives include combinations like these.</p>
  <a id="p332"></a>
                    <p class="reference"><em>It is not unnecessary to pay attention</em>&mdash;meaning, simply, &quot;It is necessary to pay attention.&quot;</p>
					<p class="reference"><em>It is not impossible to pay attention</em>&mdash;meaning, &quot;It is possible to pay attention.&quot;</p>
					<p class="reference"><em>The switch is not disabled</em>&mdash;meaning, &quot;The switch is functioning.&quot;</p>
					<p class="reference"><em>It is impossible to not do something illegal</em>&mdash;meaning, strangely, &quot;It is not possible to do something legal.&quot;</p>
					<p class="reference"><em>For lack of no other option</em>&mdash;meaning very little. If we lack <em>no</em> other option, there must be another option. No?</p><br/>
					<p class="txt">Test makers need to be especially wary of negative prefixes such as <em>un&ndash;</em>, <em>im&ndash;</em>, <em>dis&ndash;</em>, <em>in&ndash;</em>, and so&nbsp;on.</p>
			</div>

 
       <figure class="photo-right"><img src="Images/8.3.jpg" alt="Teenage girl has her laptop open in front of her as she uses her smartphone." title="Teenage girl has her laptop open in front of her as she uses her smartphone." id="img8.3"/>
        <figcaption>
          <p class="cr">Bojan89/iStock/Getty Images Plus</p>
          <p class="caption">This young woman is completing an online, selected-response test. Perhaps using two devices allows her to search for answers on her smartphone while she completes the timed test on her computer. That is one of the factors that needs to be considered in online courses.</p>
        </figcaption>
      </figure>

      <div class="single-column">
           <h3 class="h3">3. Avoid Simple Recall</h3>
                    <p class="txt">Unless the intention is clearly to test memorization, test items should not be taken word for word from the text or other study materials. This is especially the case when instructional objectives involve application, analysis, or other higher mental processes.</p>

           <h3 class="h3">4. Make All Distracters Plausible</h3>
                    <p class="txt">Create distracters that seem equally plausible to students who don&#39;t know the correct answer. Otherwise, answering correctly might simply be a matter of eliminating highly implausible distracters. Consider the following example of a poor item.</p>


 	               <ol>
                    	       <li class="li_text"><em>10 + 12 + 18 =</em>
							   	<ul class="minus_top_margin">
							   		<li class="li_text">a. 2</li>
							   		<li class="li_text">b. 2,146</li>
							   		<li class="li_text">c. 40</li>
									<li class="li_text">d. 1</li>
							   	</ul>
							   </li>				   							   							 
                    </ol>


                    <p class="txt">For students who don&#39;t know how to calculate the correct answer, highly implausible distracters that can easily be eliminated may dramatically increase the score-inflating effects of guessing.</p>

           <h3 class="h3">5. Avoid Unintentional Cues</h3>
                    <p class="txt">Ending the stem with <em>a</em> or <em>an</em> often provides a cue, such as in this example.</p>

 <a id="p333"></a>
 	               <ol>
                    	       <li class="li_text"><em>A pachyderm is an</em>
							   	<ul class="minus_top_margin">
							   		<li class="li_text">a. cougar</li>
							   		<li class="li_text">b. dog</li>
							   		<li class="li_text">c. elephant</li>
									<li class="li_text">d. water buffalo</li>
							   	</ul>
							   </li>				   							   							 
                    </ol>

           <h3 class="h3">6. Avoid Strong Qualifiers</h3>
                    <p class="txt">Avoid the use of strong qualifying words such as <em>never</em>, <em>always</em>, <em>none</em>, <em>impossible</em>, and <em>absolutely</em> in distracters. Distracters that contain them are most often incorrect. On the other hand, distracters that contain weaker qualifiers such as <em>sometimes</em>, <em>frequently</em>, and <em>usually</em> are often associated with correct alternatives. At other times, they are simply vague and confusing. Consider the following example.</p>


	 	            <ol>
                    	       <li class="li_text"><em>Multiple-choice alternatives that contain strong qualifiers are</em>
							   	<ul class="minus_top_margin">
							   		<li class="li_text">a. always incorrect</li>
							   		<li class="li_text">b. never incorrect</li>
							   		<li class="li_text">c. usually incorrect</li>
									<li class="li_text">d. always difficult</li>
							   	</ul>
							   </li>				   							   							 
                    </ol>
					
                    <p class="txt">As expected, alternatives with strong qualifiers (<em>always</em> and <em>never</em>) are incorrect, and the alternative with a weak qualifier (<em>usually</em>) is correct.</p>

                    <p class="txt">Weak qualifiers often present an additional problem: They can be highly ambiguous. For example, interpreting the alternative <em>usually incorrect</em> is difficult because the term is imprecise. Does <em>usually</em> in this context mean most of the time? Does it mean more than half the time? More than three quarters of the time?</p>

                    <p class="txt">In stems, both kinds of qualifiers also tend to be ambiguous, and the weaker ones are more ambiguous than those that are strong. <em>Never</em> usually means &quot;not ever&quot;&mdash;although it can sometimes be interpreted to mean &quot;hardly ever.&quot; But <em>frequently</em> is one of those alarmingly vague terms for whose meaning we have no absolutes&mdash;only relatives. Just how often is frequently? We don&#39;t really know&mdash;which is why the word fits so well in many of our lies and exaggerations.</p>

           <h3 class="h3">7. Assess Instructional Objectives</h3>
                    <p class="txt">Multiple-choice assessments, like all forms of educational assessment, also need to be relevant to instructional objectives. That is, they need to include items that sample course objectives in proportion to their importance. This is one of the reasons teachers should use test blueprints and should make sure they are closely aligned with instructional objectives. </p>

           <h3 class="h3">8. Create Fair, Reliable, and Valid Tests</h3>
                    <p class="txt">Finally, as we saw in Chapter 2, assessments need to be as fair, valid, and reliable as possible. Recall that fair tests are those that do the following:</p>
 <a id="p334"></a>
						<ul class="min_top_margin, bl">
						        <li class="li_text">assess material that all learners have had an opportunity to learn</li>
						        <li class="li_text">allow sufficient time for all students to complete the test</li>
						        <li class="li_text">guard against cheating</li>
						        <li class="li_text">assess material that has been covered</li>
						        <li class="li_text">make accommodation for learner&#39;s special needs</li>
								<li class="li_text">are free of the influence of biases and stereotypes</li>
						        <li class="li_text">avoid misleading, trick questions</li>
						        <li class="li_text">grade assessments consistently</li>
 						     </ul>


                   <p class="txt">Recall, too, that the most reliable and valid assessments tend to be those based on longer tests or on a variety of shorter tests for which scoring criteria are clear and consistent. These guidelines are summarized in Table 8.6.</p>

             </div>
         
		 <div class="tbl_scroll_on_mobile">
		 <table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 8.6</span> <br />
          <span class="tbl_title">Checklist for constructing good multiple-choice items</span>
          </caption>
          <tbody>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are stems and alternatives clear and unambiguous? </td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided negatives as much as possible? </td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I included items that measure more than simple recall?</td>
            </tr>
			<tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are all distracters equally plausible?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided unintentional cues that suggest correct answers?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided qualifiers such as <em>never</em>, <em>always</em>, and <em>usually</em>?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Do my items assess my instructional objectives? </td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are my assessments as fair, reliable, and valid as possible? </td>
            </tr>
          </tbody>
     </table>
	 </div>
   </div>

<h2 class="h2">Multiple-Choice Items to Tap Higher Mental Processes</h2>
      <div class="single-column">
             <p class="txt">Multiple-choice items can easily be constructed to tap lower level cognitive skills such as remembering. They are not so easily built to assess higher level cognitive skills, but doing so is entirely possible, argues Scully (2017). She provides a number of strategies teachers might use for this purpose, several of which are listed here.</p>
 <a id="p335"></a>			 
			 	<ol class="nl">
			 		<li class="li_text">Use a guide such as Bloom&#39;s revised taxonomy or Darwazeh&#39;s (2017) revision of Bloom. Not only do these taxonomies provide detailed information about each of the higher cognitive processes, they also suggest a large number of verbs linked with each. Verbs such as <em>list</em> or <em>describe</em> ask the learner to remember&mdash;a lower level mental process. In contrast, verbs such as <em>relate</em>, <em>apply</em>, <em>compare</em>, <em>assess</em>, and <em>criticize</em> imply the use of higher level cognitive processes. (See Figure 4.5, Chapter 4, for a list of verbs related to educational objectives in the cognitive domain.) For example, the stem for a multiple-choice item designed to tap evaluation might be worded as follows: <em>Select the best (or the worst) experimental design for a study that . . .</em></li>
			 		<li class="li_text">Consider flipping items so that what might otherwise be an alternative becomes a stem. For example, the stem &quot;Which of the following scenarios best describes formative assessment?&quot; followed by four descriptions will probably tap lower level cognitive skills such as remembering. But if the stem is flipped and now becomes one of four alternatives, a new stem might be, &quot;A teacher uses a strategy called Thumbs Up, Thumbs Down with her students. This illustrates the use of:&quot; (Scully, 2017, p. 6). Answering correctly now requires the student to analyze the scenario and to apply certain principles to determine which alternative is correct.</li>
			 		<li class="li_text">Create some items that do not have a single correct answer but that have a number of plausible answers, one of which is best. This sort of item requires that learners make fine discriminations among alternatives, thus bringing higher level cognitive processes into play. </li>
			 		<li class="li_text">Describe a scenario rather than simply asking a question. Consider, for example, these two stems:
					 	<ol class="nl">
					 		<li class="li_text">What are some of the common early symptoms of autism?</li>
					 		<li class="li_text">Two-year old Willard avoids eye contact, does not like to be held, appears to have delayed speech development, does not respond to his name, and does not point or wave goodbye. Which of the following diagnoses is most likely to be correct?</li>
					 	</ol>
					</li>
	           </ol>

                    <p class="txt">Scenarios such as that presented in the second stem may require the learner to analyze, interpret, and apply rather than simply remember.</p>

                    <p class="txt">Although multiple-choice items can be constructed to tap higher mental processes, whether they do so depends on the strategies required of each learner. For example, one learner, who happens to know very little about childhood disorders but who has experience with autistic children, might simply recall a scenario identical to the one presented in stem 2 above and quickly select the correct answer. A second learner, who is familiar with the detailed symptoms of a wide range of common and uncommon childhood disorders, might need to carefully analyze the various behaviors described, evaluate them in light of a range of possibilities, and struggle to make a quick selection.</p>

	 </div>

</div>

<script type="text/javascript" src="Scripts/app-min.js"></script>   
</section>




<!--
/////////////////////////////////////////////////////////////
///////////////////////// 8.5 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->
 

<section class="page" id="sec8.5">


  <div class="section-header">
    <h1 class="h1"><span class="sec_number">8.5</span> <span class="sec_title">Developing Selected-Response Assessments: Matching Items</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What are some guidelines for constructing matching items?</span></h2>
  </div>
  <div class="section-lead">
       <p class="intro">The simplest and most common matching-test item is one that presents two columns of information, arranged so that each item in one column matches a single item in the other. Columns are also organized so that matching terms are randomly juxtaposed, as shown in Figure 8.6.</p>

   </div>


<div class="section-body">
 <a id="p336"></a>	 

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.6</span><br />
          <span class="figure_title">Example of a matching-test item</span> </h4>
          <p class="caption">Matching items should have very clear instructions. More complex matching items sometimes allow some responses to be used more than once or not at all.</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-6" data-type="tipTrigger">
	        <img src="figures/Figure_8.6.jpg" alt="Illustration includes student instructions at the top with two boxes below labeled &quot;A. Type of assessment&quot; and &quot;B. Description.&quot; The student would match the description to the type of assessment by placing the corresponding number next to the assessment type." title="Illustration includes student instructions at the top with two boxes below labeled &quot;A. Type of assessment&quot; and &quot;B. Description.&quot; The student would match the description to the type of assessment by placing the corresponding number next to the assessment type." id="fig_8.6"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-6">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.6.jpg" alt="Illustration includes student instructions at the top with two boxes below labeled &quot;A. Type of assessment&quot; and &quot;B. Description.&quot; The student would match the description to the type of assessment by placing the corresponding number next to the assessment type." title="Illustration includes student instructions at the top with two boxes below labeled &quot;A. Type of assessment&quot; and &quot;B. Description.&quot; The student would match the description to the type of assessment by placing the corresponding number next to the assessment type." id="fig_8.6"/>
 	  </div>
   </section>
	


         
    <div class="single-column">        
          <p class="key-point">Matching items can be especially useful for assessing understanding, in addition to remembering. In particular, they assess the student&#39;s knowledge of associations and relationships.</p>
          <p class="txt">They can easily be constructed by generating corresponding matching lists for a wide variety of items. For example, Figure 8.6 matches terms with descriptions. Other possible matches include historical events with dates; words with definitions; words in one language with translations in another; geometric shapes with their names; literary works titles with names of authors; historical figures with historical positions or historical events; names of different kinds of implements with their uses; theorists with concepts; and so on.</p>
          <p class="txt">The most common matching items present what is called the <strong>premise column</strong> (or sometimes the <em>stem</em> column) on the left and possible matches in what is called the <strong>response column</strong> on the right. A matching item might have more entries in the response column or an equal number in each. </p>
    </div>


   <div class="inner-section">
   <h2 class="h2">Advantages of Matching Items</h2>
     <div class="single-column">        
                   <p class="txt">From a measurement point of view, one advantage of having different numbers of entries in each column is that this reduces the possibility of answering correctly when the student does not know the answer. In Figure 8.6, for example, students who know three of four correct responses will also get the fourth correct. By the same token, those who know two of the four will have a 50&ndash;50 chance of getting the next two correct. Even if a student knows only one response, there would still be a pretty good chance of guessing one or all of the others correctly. But the more items there are in the response column, the lower the odds of selecting the correct unknown response by chance. Figure 8.7 presents an example of a matching item with more items in the response than the premise column.</p>
     </div>

 <a id="p337"></a>

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.7</span><br />
          <span class="figure_title">Example of a matching-test item with uneven columns</span> </h4>
          <p class="caption">When matching-test items contain more items in the response list than in the premise list, the reliability of the measure increases because the probability of correctly guessing unknown responses decreases.</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-7" data-type="tipTrigger">
	        <img src="figures/Figure_8.7.jpg" alt="Illustration includes student instructions at the top with two boxes below labeled &quot;A. World leader&quot; and &quot;B. Country led.&quot; There are more countries listed in box B than there are world leaders listed in box A." title="Illustration includes student instructions at the top with two boxes below labeled &quot;A. World leader&quot; and &quot;B. Country led.&quot; There are more countries listed in box B than there are world leaders listed in box A." id="fig_8.7"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-7">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.7.jpg" alt="Illustration includes student instructions at the top with two boxes below labeled &quot;A. World leader&quot; and &quot;B. Country led.&quot; There are more countries listed in box B than there are world leaders listed in box A." title="Illustration includes student instructions at the top with two boxes below labeled &quot;A. World leader&quot; and &quot;B. Country led.&quot; There are more countries listed in box B than there are world leaders listed in box A." id="fig_8.7"/>
 	  </div>
   </section>
	


	      <div class="single-column">        
                   <p class="txt">Similarly, some matching tests are constructed in such a way that each item in the response list might be used once, more than once, or not at all. Not only does this approach effectively eliminate the possibility of narrowing down options for guessing, it might be constructed to require that the student engage in behaviors that require calculating, comparing, differentiating, predicting, appraising, and so on. All of these activities tap higher level cognitive skills. </p>
     </div>
   </div>

 <a id="p338"></a>
  <h2 class="h2">Guidelines for Constructing Matching Items</h2>
    <div class="single-column">    
	       <p class="txt">Not all matching-test items are equally good. Consider, for example, the item shown in Figure 8.8. Note how the instructions are clear and precise: They state exactly what the test taker must do and how often each response can be used. Yet it really is a very bad item. Entries in each column are structured so differently that for those with adequate reading skills, grammatical cues make the answers totally obvious. The person who built this item should have paid attention to the following guidelines.</p>

	</div>

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.8</span><br />
          <span class="figure_title">Example of a poorly constructed matching-test item</span> </h4>
          <p class="caption">To avoid many of the problems that are obvious in this example, simply use complete sentences or parallel structures in the premise column.</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-8" data-type="tipTrigger">
	        <img src="figures/Figure_8.8.jpg" alt="Illustration includes student instructions at the top with two boxes below labeled &quot;A. In the Story, &lsquo;Pablo&#39;s Chicken&#39;&quot; and &quot;B. Answers based on &lsquo;Pablo&#39;s Chicken.&#39;&quot; The student would complete the sentences in box A with words or phrases from box B." title="Illustration includes student instructions at the top with two boxes below labeled &quot;A. In the Story, &lsquo;Pablo&#39;s Chicken&#39;&quot; and &quot;B. Answers based on &lsquo;Pablo&#39;s Chicken.&#39;&quot; The student would complete the sentences in box A with words or phrases from box B." id="fig_8.8"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-8">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.8.jpg" alt="Illustration includes student instructions at the top with two boxes below labeled &quot;A. In the Story, &lsquo;Pablo&#39;s Chicken&#39;&quot; and &quot;B. Answers based on &lsquo;Pablo&#39;s Chicken.&#39;&quot; The student would complete the sentences in box A with words or phrases from box B." title="Illustration includes student instructions at the top with two boxes below labeled &quot;A. In the Story, &lsquo;Pablo&#39;s Chicken&#39;&quot; and &quot;B. Answers based on &lsquo;Pablo&#39;s Chicken.&#39;&quot; The student would complete the sentences in box A with words or phrases from box B." id="fig_8.8"/>
 	  </div>
   </section>
	



    <div class="single-column">   
	           	<ol class="nl">
	           		<li class="li_text">Items in each column should be parallel. For example, in Figure 8.6, all items in the premise are terms relating to types of educational assessment, and all items in the response column are descriptions of kinds of assessment. Similarly, in Figure 8.7, premise entries are all names of world leaders, and response entries are all countries. The following is an example of nonparallel premise items that are to be matched to a response list of different formulas for calculating the surface area of different geometric figures:<br/> 
					&#x5f;&#x5f;&#x5f;<em>triangle</em><br/>
					&#x5f;&#x5f;&#x5f;<em>square</em><br/>
					&#x5f;&#x5f;&#x5f;<em>rectangle</em><br/>
					&#x5f;&#x5f;&#x5f;<em>cardboard boxes</em><br/>
					&#x5f;&#x5f;&#x5f;<em>circle</em><br/><br/>
					The inclusion of <em>cardboard boxes</em> among these geometric shapes is confusing and unnecessary. Test makers must also guard against items that are not grammatically parallel, as is shown in Figure 8.8.
					</li>
	           		<li class="li_text"> <a id="p339"></a>All items in the response list should be plausible. This is especially true if the response list contains more entries than the premise column. The test is less reliable when it contains items that allow students to quickly discard implausible responses.</li>
	           		<li class="li_text">To increase the reliability of the test, the response column should contain more items than the premise column.</li>
	           		<li class="li_text">The number of items in each column should be limited to 6 to 10. Longer columns place too much strain on memory. Recall from Chapter 3 that our adult short-term memory is thought to be limited to seven plus or minus two items. It is difficult to keep more items than this in our conscious awareness at any one time.</li>
	           		<li class="li_text">The grammatical structure should provide no unwanted clues, as is the case, for example, for the items in Figure 8.8: The grammatical structure reveals almost all the correct responses. Moreover, the fourth item in the response column is an implausible response.</li>
	           		<li class="li_text">Directions should be clear and specific. They should stipulate how the match is to be made and on what basis. For example, directions for online matching tests might read: &quot;Drag each item in column B to the appropriate box in front of the matching item in column A.&quot; Similar instructions for a written matching-item test might specify: &quot;Write the number in front of each answer in column B in the appropriate space after each statement in column A.&quot;</li>
	           		<li class="li_text">Response items should be listed in logical order. Note, for example, that response columns in Figures 8.7 and 8.8 are alphabetical. When response items are numerical, they should be listed in ascending or descending order. Doing so eliminates the possibility of developing some detectable pattern. It also discourages students from wasting their time looking for a pattern.</li>
	           		<li class="li_text">For paper-and-pencil matching items, lists should be entirely on one page. Having to flip from one page to another can be time consuming and confusing.</li>
	            </ol>
              <p class="text">Table 8.7 summarizes these guidelines in the form of a checklist.</p>
	</div>
         
		 <div class="tbl_scroll_on_mobile">
		 <table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 8.7</span> <br />
          <span class="tbl_title">Checklist for constructing good matching items</span>
          </caption>
          <tbody>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are items in the premise column parallel?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are items in the response column parallel?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are all response-column items plausible?</td>
            </tr>
			<tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I included more response- than premise-column items?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are my lists limited to no more than seven or so items?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided unintentional cues that suggest correct matches?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are my directions clear, specific, and complete?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I listed response-column items in logical order?</td>
            </tr>
			<tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are my columns entirely on one page?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Do my items assess my instructional objectives?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are my assessments as fair, reliable, and valid as possible?</td>
            </tr>
          </tbody>
     </table>
     </div>

  </div> 
   <script type="text/javascript" src="Scripts/app-min.js"></script>
</section>



<!--
/////////////////////////////////////////////////////////////
///////////////////////// 8.6 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->


<section class="page" id="sec8.6">
 <a id="p340"></a>
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">8.6</span> <span class="sec_title">Developing Selected-Response Assessments: True&ndash;False Items</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What are some guidelines for constructing true&ndash;false items? </span></h2>
  </div>
  <div class="section-lead">
       <p class="intro">A relatively common form of assessment, often used in early grades, is the true&ndash;false item. </p>
   </div>


<div class="section-body">

        <div class="single-column">
                    <p class="txt">True&ndash;false items typically take the form of a statement that the examinee must judge as <em>true</em> or <em>false</em>. However, they can also consist of statements or questions for which the correct answer might involve choosing between responses such as <em>yes</em> and <em>no</em> or <em>right</em> and <em>wrong</em>. As a result, they are sometimes called <strong>binary-choice items</strong> rather than true&ndash;false items.</p>
                    <p class="txt">True&ndash;false test items tend to be popular in early grades because they are easy to construct, can be used to sample a wide range of knowledge, and provide a quick and easy way to look at the extent to which instructional objectives are being met.</p>
                    <p class="txt">Most true&ndash;false items are simple propositions that can be answered <em>true</em> or <em>false</em> (or <em>yes</em> or <em>no</em>). For example:</p>
					<p class="txt"><em>Reliability is a measure of the consistency of an assessment</em>. &#x5f;<u>&#10003;</u>&#x5f;T&#x5f;&#x5f;&#x5f;F</p>
                    <p class="txt"><em>Face validity reflects how closely scores from repeated administrations of the same test </em><em>resemble each other</em>. &#x5f;&#x5f;&#x5f;T &#x5f;<u>&#10003;</u>&#x5f;F</p>
                    <p class="txt"><em>Predictive validity is a kind of criterion-related validity</em>. &#x5f;<u>&#10003;</u>&#x5f;T&#x5f;&#x5f;&#x5f;F</p>
        </div>



    <div class="inner-section">
      <h2 class="h2">True&ndash;False Assessments to Tap Higher Mental Processes</h2>
      <div class="single-column">
                    <p class="key-point">Answering these true&ndash;false questions requires little more than simple recall. However, it is possible to construct true&ndash;false items that assess other cognitive skills. </p>
                    <p class="txt">Consider the following example.</p>
                    <p class="ext"><em>Is the following statement true or false?</em><br/>
                    (9 &divide; 3) &times; 12 + 20 = 56  &#x5f;<u>&#10003;</u>&#x5f;T&#x5f;&#x5f;&#x5f;F</p>
                    <p class="txt">Responding correctly to this item requires recalling and applying mathematical operations rather than simply remembering a correct answer. </p>
                    <p class="key-point">Binary-choice items can also be constructed so that the responder is required to engage in a variety of higher level cognitive activities such as comparing, predicting, evaluating, and generalizing. </p>
					<p class="txt">Figure 8.9 shows examples of how this might be accomplished in relation to the Bloom&#39;s revised taxonomy.</p>

     </div>
      

 <a id="p341"></a>

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.9</span><br />
          <span class="figure_title">True&ndash;false items tapping higher cognitive skills</span> </h4>
          <p class="caption">Although it is possible to design true&ndash;false items that do more than measure simple recall, other forms of assessment are often more appropriate for objectives relating to activities such as analyzing, evaluating, and creating.</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-9" data-type="tipTrigger">
	        <img src="figures/Figure_8.9.jpg" alt="Chart showing Bloom&#39;s revised taxonomy of educational objectives, a possible activity representing each objective, and examples of a true&ndash;false item that reflects the boldface verb in the &quot;possible activity&quot; column. In the column for Bloom&#39;s revised taxonomy of educational objectives are remembering, understanding, applying, analyzing, evaluating, and creating. A possible activity for remembering could be copy, order, repeat, or recognize. &quot;Recognize&quot; is in boldface, and the example of true&ndash;false item that reflects &quot;recognize&quot; is asking the student if the image provided is a spider or not." title="Chart showing Bloom&#39;s revised taxonomy of educational objectives, a possible activity representing each objective, and examples of a true&ndash;false item that reflects the boldface verb in the &quot;possible activity&quot; column. In the column for Bloom&#39;s revised taxonomy of educational objectives are remembering, understanding, applying, analyzing, evaluating, and creating. A possible activity for remembering could be copy, order, repeat, or recognize. &quot;Recognize&quot; is in boldface, and the example of true&ndash;false item that reflects &quot;recognize&quot; is asking the student if the image provided is a spider or not." id="fig_8.9"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-9">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.9.jpg" alt="Chart showing Bloom&#39;s revised taxonomy of educational objectives, a possible activity representing each objective, and examples of a true&ndash;false item that reflects the boldface verb in the &quot;possible activity&quot; column. In the column for Bloom&#39;s revised taxonomy of educational objectives are remembering, understanding, applying, analyzing, evaluating, and creating. A possible activity for remembering could be copy, order, repeat, or recognize. &quot;Recognize&quot; is in boldface, and the example of true&ndash;false item that reflects &quot;recognize&quot; is asking the student if the image provided is a spider or not." title="Chart showing Bloom&#39;s revised taxonomy of educational objectives, a possible activity representing each objective, and examples of a true&ndash;false item that reflects the boldface verb in the &quot;possible activity&quot; column. In the column for Bloom&#39;s revised taxonomy of educational objectives are remembering, understanding, applying, analyzing, evaluating, and creating. A possible activity for remembering could be copy, order, repeat, or recognize. &quot;Recognize&quot; is in boldface, and the example of true&ndash;false item that reflects &quot;recognize&quot; is asking the student if the image provided is a spider or not." id="fig_8.9"/>
 	  </div>
   </section>
	



      <div class="single-column">

                    <p class="txt">Note that Figure 8.9 does not include any examples that relate to <em>creating</em>. Objectives that have to do with designing, writing, producing, and related activities are far better assessed by means of performance-based assessments or constructed-response items than with the more objective, selected-response assessments. </p>
 <a id="p342"></a>
					<p class="txt">Note, too, that although it is possible to design true&ndash;false items that seem to tap higher cognitive processes, whether they do so depends on what the learner already knows. As we saw earlier, what a test measures is not defined entirely by the test items themselves. Rather, it depends on the relationship between the test item and the individual learner. Consider, for example, the Figure 8.9 item that illustrates judging&mdash;an activity described as having to do with evaluating:</p>

						   <ul class="bl">
						        <li class="li_text"><em>It is usually better to use a constructed-response test rather than a true&ndash;false test for </em><em>objectives having to do with evaluating</em>. &#x5f;&#x5f;&#x5f;T&#x5f;&#x5f;&#x5f;F</li>			
 						   </ul>
   					
					 <p class="txt">After considering the characteristics of constructed-response and true&ndash;false tests, one learner might respond by analyzing the requirements of evaluative cognitive activities and judging which characteristics of these assessments would be best suited for the objective. That learner&#39;s cognitive activity would illustrate evaluating. Another learner, however, might simply remember having read or heard that constructed-response assessments are better suited for objectives relating to evaluating and would quickly select the correct response. That learner&#39;s cognitive activity would represent the lowest level in Bloom&#39;s taxonomy: remembering.</p>
 	 </div> 
	</div>


   <h2 class="h2">Limitations of True&ndash;False Assessments </h2>
    <div class="single-column">
                    <p class="txt">True&ndash;false assessments are open to a number of serious criticisms. First, unless they are carefully and deliberately constructed to go beyond black-and-white facts, they tend to measure little more than simple recall. Second, because there is a 50% chance of answering any one question correctly&mdash;all other things being equal&mdash;they tend to provide unreliable assessments. If everyone in a class knew absolutely nothing about an area being tested with true&ndash;false items, and they all simply guessed each answer randomly, the average performance of the class would be around 50%.</p>
                    <p class="txt">Nevertheless, the chance of receiving a high mark as a result of <strong>blind guessing</strong>&mdash;the process of selecting any response at random&mdash;is very low. And the chance of receiving a very poor mark is equal to that of receiving a very high one. </p>
                    <p class="txt">Most guessing tends to be relatively educated rather than completely random. Even if they are uncertain about the correct answer, many students know something about the item and guess on the basis of the information they have and the logic and good sense at their disposal. Or, as Bar-Hillel, Peer, and Acquisti (2014) found, they may simply respond on the basis of a preexisting bias. When these researchers asked respondents to mentally simulate a coin toss and reveal what the outcome of the first toss would be, some 80% of the participants chose &quot;heads.&quot; The authors suggest that the reason for this bias is linguistic: It has to do with the fact that we typically say, &quot;Heads or tails,&quot; and almost never, &quot;Tails or heads.&quot; Similarly, we always say, &quot;True or false,&quot; never, &quot;False or true.&quot;</p>

	</div>

   <div class="inner-section">
      <h2 class="h2">Variations on Binary-Choice Items</h2>
      <div class="single-column">
                    <p class="txt">In one study, Wakabayashi and Guskin (2010) used an intriguing approach to reduce the effect of guessing. Instead of simply giving respondents the traditional choice of true or false, they added a third option: unsure. When students were retested on the same material later, items initially marked <em>unsure</em> were more likely to have been learned in the interim and to be answered correctly on the second test than were incorrect responses of which the responders had been more certain.</p>
 <a id="p343"></a>
					<p class="txt">Another interesting approach that uses true&ndash;false items to understand more clearly the learner&#39;s thinking processes asks responders to explain their choices. For example, Stein, Larrabee, and Barman (2008) developed an online test designed to uncover false beliefs that people have about science. The test consists of 47 true&ndash;false items, each of which asks responders to explain the reasons for their choices. As an example, one of the items reads as follows:</p>
                    <p class="ext"><em>An astronaut is standing on the moon with a baseball in her/his hand. When the </em><em>baseball is released, it will fall to the moon&#39;s surface</em>. (p. 5)</p>
                    <p class="txt">The correct answer, <em>true</em>, was selected by only 32.8% of 305 respondents, all of whom were students enrolled in teacher education programs at two different universities. More reassuringly, 94.4% chose the correct answer (<em>true</em>) for this next item:</p>
                    <p class="ext"><em>A force is needed to change the motion of an object</em>. (Stein et al., 2008, p. 5)</p>
                    <p class="txt">The usefulness of this approach lies in the explanations, which often reveal serious misconceptions. Strikingly, this is frequently the case even when answers are correct. For example, more than 40% of respondents who answered this last item correctly did so for the wrong reasons. They failed to identify the normal forces (called <em>reaction forces</em>) that counter the effects of gravity.</p>
                    <p class="txt">Asking students to explain their choices on multiple-choice tests might reveal significant gaps in knowledge or logic. This approach could contribute in important ways to the use of these tests for formative purposes. Table 8.8 presents guidelines for writing true&ndash;false items.</p>  
	  </div>

         <div class="tbl_scroll_on_mobile">
		 <table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 8.8</span> <br />
          <span class="tbl_title">Checklist for constructing good true&ndash;false items</span>
          </caption>
          <tbody>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Is the item useful for assessing important learning objectives?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided negatives as much as possible?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I included items that measure more than simple recall?</td>
            </tr>
			<tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Is the item absolutely clear and unambiguous?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided qualifiers such as <em>never</em>, <em>always</em>, and <em>usually</em>?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided a pattern of correct responses?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I balanced correct response choices?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I made my statements as brief as possible?</td>
            </tr>
			<tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided trick questions?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are my true and false statements of approximately equal length?</td>
            </tr>
          </tbody>
     </table>
	 </div>

   </div>
</div>
<script type="text/javascript" src="Scripts/app-min.js"></script>
</section>


<!--
/////////////////////////////////////////////////////////////
///////////////////////// 8.7 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->



<section class="page" id="sec8.7">
 <a id="p344"></a>
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">8.7</span> <span class="sec_title">Developing Selected-Response Assessments: Interpretive Items</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What are some guidelines for constructing interpretive items?</span></h2>
  </div>
  <div class="section-lead">     
	   <p class="intro">Interpretive items present information that the responder needs to interpret when answering test items. Although the test items themselves might take the form of any of the objective test formats&mdash;matching, multiple-choice, or true&ndash;false&mdash;in most cases the material to be interpreted is followed by multiple-choice questions. </p>
   </div>


<div class="section-body">
    <div class="single-column">
                <p class="txt">Interpretive material most often takes the form of one or two written paragraphs. It may also involve graphs, charts, maps, tables, and video or audio recordings. Figure 8.10 is an example of a true&ndash;false interpretive item based on a graph. Answering the items correctly might require analysis and inference in addition to basic skills in reading graphs. </p>
    </div>


   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.10</span><br />
          <span class="figure_title">Interpretive true&ndash;false item based on a graph</span> </h4>
          <p class="caption">Interpretive test items are most often based on written material but can also be based on a variety of visual or auditory material, as shown here.</p>
		  <p class="src"><em>U.S. Census Bureau, 2011.</em></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-10" data-type="tipTrigger">
	        <img src="figures/Figure_8.10.jpg" alt="Line graph with the x-axis labeled &quot;year&quot; with years 2000, 2006, 2007, 2008, and 2009. The y-axis is labeled &quot;percentage of adults aged 25 to 34&quot; with a scale of 25 to 60 rising in increments of 5. Two lines are plotted on the graph: one labeled &quot;currently married&quot; and the other labeled &quot;never married&quot;. The &quot;currently married&quot; line runs through the following data points for each of the 5 years: 55 (2000), about 49 (2006), about 48 (2007), about 47 (2008), and about 45 (2009). This line slopes downward most drastically from 2000 to 2006 and then gradually decreases through 2009. The &quot;never married&quot; line runs through the following data points for each of the 5 years: &quot;35 (2000), about 42 (2006), about 43 (2007), about 44 (2008), and about 47 (2009). This line slopes upward most drastically from 2000 to 2006 and then gradually increases through 2009. Beneath the graph are five true&ndash;false questions based on information in the line graph." title="Line graph with the x-axis labeled &quot;year&quot; with years 2000, 2006, 2007, 2008, and 2009. The y-axis is labeled &quot;percentage of adults aged 25 to 34&quot; with a scale of 25 to 60 rising in increments of 5. Two lines are plotted on the graph: one labeled &quot;currently married&quot; and the other labeled &quot;never married&quot;. The &quot;currently married&quot; line runs through the following data points for each of the 5 years: 55 (2000), about 49 (2006), about 48 (2007), about 47 (2008), and about 45 (2009). This line slopes downward most drastically from 2000 to 2006 and then gradually decreases through 2009. The &quot;never married&quot; line runs through the following data points for each of the 5 years: &quot;35 (2000), about 42 (2006), about 43 (2007), about 44 (2008), and about 47 (2009). This line slopes upward most drastically from 2000 to 2006 and then gradually increases through 2009. Beneath the graph are five true&ndash;false questions based on information in the line graph." id="fig_8.10"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-10">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.10.jpg" alt="Line graph with the x-axis labeled &quot;year&quot; with years 2000, 2006, 2007, 2008, and 2009. The y-axis is labeled &quot;percentage of adults aged 25 to 34&quot; with a scale of 25 to 60 rising in increments of 5. Two lines are plotted on the graph: one labeled &quot;currently married&quot; and the other labeled &quot;never married&quot;. The &quot;currently married&quot; line runs through the following data points for each of the 5 years: 55 (2000), about 49 (2006), about 48 (2007), about 47 (2008), and about 45 (2009). This line slopes downward most drastically from 2000 to 2006 and then gradually decreases through 2009. The &quot;never married&quot; line runs through the following data points for each of the 5 years: &quot;35 (2000), about 42 (2006), about 43 (2007), about 44 (2008), and about 47 (2009). This line slopes upward most drastically from 2000 to 2006 and then gradually increases through 2009. Beneath the graph are five true&ndash;false questions based on information in the line graph." title="Line graph with the x-axis labeled &quot;year&quot; with years 2000, 2006, 2007, 2008, and 2009. The y-axis is labeled &quot;percentage of adults aged 25 to 34&quot; with a scale of 25 to 60 rising in increments of 5. Two lines are plotted on the graph: one labeled &quot;currently married&quot; and the other labeled &quot;never married&quot;. The &quot;currently married&quot; line runs through the following data points for each of the 5 years: 55 (2000), about 49 (2006), about 48 (2007), about 47 (2008), and about 45 (2009). This line slopes downward most drastically from 2000 to 2006 and then gradually decreases through 2009. The &quot;never married&quot; line runs through the following data points for each of the 5 years: &quot;35 (2000), about 42 (2006), about 43 (2007), about 44 (2008), and about 47 (2009). This line slopes upward most drastically from 2000 to 2006 and then gradually increases through 2009. Beneath the graph are five true&ndash;false questions based on information in the line graph." id="fig_8.10"/>
 	  </div>
   </section>


 <a id="p345"></a>
    <div class="single-column">
                    <p class="txt">Figure 8.11 illustrates a more common form of interpretive test item. It is based on written material that is novel for the student. Responding correctly requires a high level of reading skill and might also reflect a number of higher mental processes such as those involved in analyzing, generalizing, applying, and evaluating.</p>
    </div>

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.11</span><br />
          <span class="figure_title">Interpretive multiple-choice item based on written material</span> </h4>
          <p class="caption">The most common interpretive items are based on written material. One of their disadvantages is that they are highly dependent on reading skills.</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-11" data-type="tipTrigger">
	        <img src="figures/Figure_8.11.jpg" alt="Box at the top of the figure includes a description of beavers. There are two boxes below this description, each containing a multiple-choice question based on the description." title="Box at the top of the figure includes a description of beavers. There are two boxes below this description, each containing a multiple-choice question based on the description." id="fig_8.11"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-11">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.11.jpg" alt="Box at the top of the figure includes a description of beavers. There are two boxes below this description, each containing a multiple-choice question based on the description." title="Box at the top of the figure includes a description of beavers. There are two boxes below this description, each containing a multiple-choice question based on the description." id="fig_8.11"/>
 	  </div>
   </section>

    <div class="inner-section">
      <h2 class="h2">Advantages of Interpretive Items</h2>
      <div class="single-column">
                    <p class="txt">Interpretive items present several advantages over traditional multiple-choice items. Most important, they make it considerably easier to engage intellectual processes other than simple recall. Because the material to be interpreted is usually novel, the student cannot rely on recall to respond correctly.</p>
                    <p class="txt">A second advantage of interpretive items is that they can be used to assess understanding of material that is closer to real life. For example, they can easily be adapted to assess how clearly students understand the sorts of tables, charts, maps, and graphs that are found in newspapers, on television, and in online sources.</p>
                    <p class="txt">Finally, not only can interpretive test items be used to assess a large range of intellectual skills, they can also be scored completely objectively. This is not the case for performance-based assessments or for most constructed-response assessments.</p>
	  </div>
   </div>

 <a id="p346"></a>
   <h2 class="h2">Limitations of Interpretive Items</h2>
	<div class="single-column">
                    <p class="txt">Interpretive test items do have a number of limitations and disadvantages, however. Among them is that interpretive items often rely heavily on reading skills. As a result, incorrect responses may reflect reading comprehension problems rather than problems in the intellectual skills being assessed. They can be especially unfair for ELL students</p>

                    <p class="txt">Another disadvantage of interpretive items is the difficulty of constructing good interpretive material and related multiple-choice items. Developing interpretive text or visual representations is a time-consuming and demanding task. Poorly designed items tend to measure recognition and recall, both of which can be assessed by means of multiple-choice or true&ndash;false formats, which are easier to construct.</p>

                    <p class="txt">Finally, like other objective assessments, interpretive test items seldom engage the productive skills involved in creating. As we noted, performance-based and constructed-response tests are more appropriate for tapping the highest levels of cognitive skills.</p>
		</div>

    <div class="inner-section">
      <h2 class="h2">Constructing Good Interpretive Items</h2>
      <div class="single-column">
                      <p class="key-point">As is true for all forms of assessment, the best interpretive test items are those that are relevant to instructional objectives, sample widely, tap a range of mental processes, and are as fair, valid, and reliable as possible. </p>

                    <p class="txt">Table 8.9 provides a brief checklist of guidelines for constructing good interpretive items.</p>
	  </div>


         <div class="tbl_scroll_on_mobile">
		 <table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 8.9</span> <br />
          <span class="tbl_title">Checklist for constructing good interpretive items</span>
          </caption>
          <tbody>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Is the item useful for assessing important learning objectives?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Is the reading level and the difficulty appropriate for my students?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Does the item tap more than simple recall?</td>
            </tr>
			<tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided questions that are answered literally in the interpretive material?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided questions that can be answered without the interpretive material?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are multiple-choice items well constructed?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Is the item absolutely clear and unambiguous?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I included all the information required?</td>
            </tr>
			<tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Are instructions clear?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided unnecessary length?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Is the interpretive material novel for learners?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I avoided trick questions?</td>
            </tr>
          </tbody>
     </table>
	 </div>

   </div>

 </div> 
 <script type="text/javascript" src="Scripts/app-min.js"></script>
</section>



<!--
/////////////////////////////////////////////////////////////
///////////////////////// 8.8 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->



<section class="page" id="sec8.8">
 <a id="p347"></a>
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">8.8</span> <span class="sec_title">Developing Constructed-Response Assessments: Short-Answer (Restricted-Response) Items</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Questions</span> <span class="focus_q">What are some guidelines for constructing short-answer items? What are the advantages and limitations of constructed-response assessments?</span></h2>
  </div>
  <div class="section-lead">     
	   <p class="intro">As the name implies, constructed-response assessments require test takers to generate their own responses. Two main kinds of constructed-response assessments are used in educational measurement: short-answer items (also referred to as <em>restricted-response items</em>) and essay items (also called <em>extended-response items</em>).</p>
  </div>

<div class="section-body">
    <div class="single-column">
                   <p class="key-point">The main advantage of constructed-response assessments is that they lend themselves better to evaluating higher thought processes and cognitive skills. Also, they allow for more variation and more creativity.</p>
                   <p class="key-point">When compared with selected-response assessments, constructed-response assessments have two principal limitations. First, they usually consist of a small number of items and therefore sample course content less widely. Second, they tend to be less objective than selected-response assessments, simply because they can seldom be scored completely objectively.</p>
                   <p class="txt">Short-answer items sometimes require a response restricted to a single word or short phrase to fill in a blank left in a sentence. These are normally referred to as <strong>completion items</strong> (or <em>fill-in-the-blank items</em>). At other times, they require a brief written response&mdash;perhaps only one or two words&mdash;that does not fill in a blank space in a sentence. In contrast, essay items typically ask for a longer, more detailed written response, often consisting of a number of paragraphs or even pages.</p>
                   <p class="txt">Completion items can easily be generated simply by taking any clear, unambiguous sentence from a written source and reproducing it with a single word or phrase left out. One problem with this approach is that it encourages rote memorization rather than a more thoughtful approach to studying. In addition, out-of-context sentences are often somewhat ambiguous at best; at worst, they can be completely misleading.</p>

    </div>
	

    <div class="inner-section">
      <h2 class="h2">Advantages of Short-Answer Items</h2>
      <div class="single-column">
          <p class="txt">Short-answer items have several important advantages:</p>

         	<ol class="nl">
         	<li class="li_text">Because they require the test taker to produce a response, they effectively eliminate much of the influence of guessing. This presents a distinct advantage over selected-response approaches such as multiple-choice and true&ndash;false assessments. </li>
         	<li class="li_text">Because they ask for a single correct answer or a very brief response, they can be highly objective even though they ask for a constructed response. As a result, it is a simple matter to generate a <strong>marking key</strong> (list of possible correct responses) very much as is done for multiple-choice, true&ndash;false, or matching items.</li>
         	<li class="li_text">They are easy to construct and can quickly sample a wide range of knowledge.</li>
         	</ol>
     </div>
 </div>
 <a id="p348"></a>
    <h2 class="h2">Limitations of Short-Answer Items</h2>
       <div class="single-column">
                     <p class="txt">Among the limitations of short-answer items is the fact that because examiners need to read every response, such items can take longer to score than selected-response measures. Moreover, constructing short-answer items is not always easy: Sometimes it is difficult to phrase the item so that only one answer is correct.</p>

                    <p class="txt">Another limitation has to do with possible contamination of scores due to bad spelling. If marks are deducted for misspelled words, what is being measured becomes a mixture of spelling and content knowledge. Yet if spelling errors are ignored, the marker may occasionally have to guess at whether the misspelled word actually represents the correct answer.</p>

                    <p class="txt">Finally, because correct responses are usually limited to a single choice, they do not allow for creativity and are unlikely to engage processes such as synthesis and evaluation.</p>
       </div>

   <div class="inner-section">
     <h2 class="h2">Examples of Short-Answer Items</h2>
      <div class="single-column">
                    <p class="txt">General guidelines for the construction of short-answer items include many of those listed earlier in Tables 8.6 through 8.9. In addition, test makers need to ensure that only one answer is correct and that what is required as a response is clear and unambiguous. For this reason, when preparing completion items it is often advisable to place blanks at the end of the sentence rather than in the middle. For example, consider these two completion items:</p>

					<ol class="nl"><li class="li_table"><em>In 1972 </em>______________<em> produced a film entitled</em> Une Belle Fille Comme Moi.</li>
					<li class="li_table"><em>The name of the person who produced the 1972 film</em> Une Belle Fille Comme Moi <em>is </em>______________.</li></ol>

					<p class="txt">Although the correct answer in both cases is Fran&ccedil;ois Truffaut, the first item could also be answered correctly with the word <em>France</em>. But the structure of the second item makes the nature of the required response clear. It&#39;s even clearer to rephrase the sentence as a question so that it is no longer a fill-in-the blank short-answer item. For example:</p>

					<p class="ext"><em>What is the name of the director who produced the 1972 film</em> Une Belle Fille Comme Moi? ______________</p>

					<p class="txt">Although short-answer items generally require only one or two words as a correct response, some ask for slightly longer responses. For example:</p>

					<p class="ext"><em>Define face validity</em>. ______________<br/><em>What states are contiguous with Nevada?</em> ______________, ______________, ______________, and ______________.</p>

					<p class="txt">As shown in the second example, four blanks are provided as a clue to the number of responses required. Providing a single, longer blank would increase the item&#39;s level of difficulty.</p>

	  </div>
</div>



  </div> 
  <script type="text/javascript" src="Scripts/app-min.js"></script>
</section>


<!--
/////////////////////////////////////////////////////////////
///////////////////////// 8.9 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->



<section class="page" id="sec8.9">
 <a id="p349"></a>
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">8.9</span> <span class="sec_title">Developing Constructed-Response Assessments: Essay Constructed- (Extended-) Response Items</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Questions</span> <span class="focus_q">What are some guidelines for constructing extended-response items? What are the advantages and limitations of constructed-response assessments?</span></h2>
  </div>

  <div class="section-lead">     
	   <p class="intro">Essay items also require test takers to generate their own responses. However, instead of being asked to supply a single word or short phrase, they are asked to write longer responses.</p>
      </div>


<div class="section-body">
    <div class="single-column">
                    <p class="txt">The instructions given to the test taker&mdash;sometimes referred to as the <em>stimulus</em>&mdash;can vary in length and form but should always be worded so that the student clearly understands what is required. Whenever possible, the stimulus should also include an indication of scoring guidelines. Without scoring guidelines, responses can vary widely and increase the difficulty of scoring an item.</p>
    </div>
   
    <div class="inner-section">
      <h2 class="h2">Advantages of Essay Response</h2>
      <div class="single-column">
                    <p class="txt">Some essay assessments might ask a simple question that requires a one- or two-sentence response. For example:</p>
                    <p class="reference"><em>What is the main effect of additional government spending on unemployment?</em></p>
					<p class="reference"><em>Why is weather forecasting more accurate now than it was in the middle of the 20th </em><em>century?</em></p><br/>
                    <p class="txt">Both of these questions can be answered correctly with a few sentences. And both can be keyed so that different examiners scoring them would arrive at very similar results. Items of this kind can have a high degree of reliability.</p>
                    <p class="txt">Many essay items ask for lengthier expositions that typically require organizing information, marshaling arguments, defending opinions, appealing to authority, and so on. In responding to these, students typically have wide latitude both in terms of what they say and how they say it. As a result, longer essay responses are especially useful for tapping higher mental processes such as applying, analyzing, evaluating, and creating. This is their major advantage over the more objective approaches.</p>
     </div>          
    </div>

   
   <h2 class="h2">Limitations of Essay Response</h2>
   <div class="single-column">
                    <p class="txt">The main limitation of longer essay assessments has to do with their scoring. Not only is scoring essays highly time consuming, it tends to be decidedly subjective. As a result, both reliability and validity of such assessments are lower than for more objective measures. For example, some examiners consistently give higher marks than others (Brown, 2010). Also, because there is sometimes a sequential effect in scoring essays, essay items that follow each other are more likely to receive similar marks than those that are farther apart (Attali, 2011). </p>
 <a id="p350"></a>
                    <p class="txt">Ebuoh and Ezeudu (2015) report a study in which 42 biology teachers from 27 secondary schools were asked to score biology papers in one of several different ways: One third scored all items in sequence; one third scored the first item on each test before moving to the second; a third group was part of an independent scorers group, in which each paper was scored by two or more teachers working independently and scores were pooled. Highest reliability ratings were obtained with independent scorers. Second highest reliabilities were obtained when teachers scored the first item on all papers, then the second, and so on.</p>

                    <p class="txt">Another study looked at the relative reliabilities of two approaches to scoring essays: the <em>holistic</em> approach, in which the teacher selects student answers that are graded <em>high</em>, <em>average</em>, or <em>low</em> and uses these as models to which all answers will be compared; and the <em>analytical</em> approach, in which the teacher develops a rubric or checklist detailing all of the major points students are expected to include in their responses. In this study, Ebuoh (2018) had 212 biology teachers from 31 different schools use either holistic or analytic approaches to scoring. Following analysis of study results, Ebuoh found that scores based on these two approaches were in relatively high agreement. Both approaches are useful and effective. In general, however, the holistic approach is less time consuming and somewhat more reliable. The main general conclusion is that the holistic approach is more efficient, primarily because it is less time consuming than the analytic approach.</p>

                    <p class="txt">This research suggests that there are at least three ways of scoring essay tests to increase their reliability. </p>

						<ol class="nl">
							<li class="li_text">The reliability of essay tests can be increased significantly by using a holistic approach, in which outstanding, average, or subaverage responses are used as models.</li>
							<li class="li_text">Reliability can also be increased by using detailed scoring guides such as checklists and rubrics (analytical approach). These guides typically specify as precisely as possible details of the points, arguments, conclusions, and opinions that will be considered in the scoring, and the weightings assigned to each.</li>
							<li class="li_text">Scoring one item on all papers before moving to the next item results in higher reliability than scoring all items on each paper sequentially.</li>
							<li class="li_text">Using more than one rater for each paper and pooling the results, while time consuming (and sometimes expensive), can also increase reliability.</li>
	                   </ol>
                   
				   <p class="txt">Essay questions can be developed to assess knowledge of subject matter (remembering, in Bloom&#39;s revised taxonomy), or they can be designed to engage any of the higher level intellectual skills. Figure 8.12 gives examples of how this might be done.</p>

   </div>

 <a id="p351"></a>

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 8.12</span><br />
          <span class="figure_title">Essay items tapping higher intellectual skills</span> </h4>
          <p class="caption">What each item assesses depends on what test takers already know and the strategies they use to craft their responses. Even responding to the first <em>remembering</em> item might require a great deal of <em>creating</em>, <em>analyzing</em>, <em>evaluating</em>, and <em>understanding</em> if the learner has not already memorized a correct response.</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig8-12" data-type="tipTrigger">
	        <img src="figures/Figure_8.12.jpg" alt="Figure shows Bloom&#39;s revised taxonomy of educational objectives, a possible activity representing each objective, and examples of an essay item that reflects the boldface verb in the &quot;possible activity&quot; column. In the column for Bloom&#39;s Revised Taxonomy of Educational Objectives are remembering, understanding, applying, analyzing, evaluating, and creating. A possible activity for applying could be demonstrate, solve, or sketch. &quot;Solve&quot; is in boldface, and the example of true&ndash;false item that reflects &quot;recognize&quot; is asking the student to solve a math equation." title="Figure shows Bloom&#39;s revised taxonomy of educational objectives, a possible activity representing each objective, and examples of an essay item that reflects the boldface verb in the &quot;possible activity&quot; column. In the column for Bloom&#39;s Revised Taxonomy of Educational Objectives are remembering, understanding, applying, analyzing, evaluating, and creating. A possible activity for applying could be demonstrate, solve, or sketch. &quot;Solve&quot; is in boldface, and the example of true&ndash;false item that reflects &quot;recognize&quot; is asking the student to solve a math equation." id="fig_8.12"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig8-12">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_8.12.jpg" alt="Figure shows Bloom&#39;s revised taxonomy of educational objectives, a possible activity representing each objective, and examples of an essay item that reflects the boldface verb in the &quot;possible activity&quot; column. In the column for Bloom&#39;s Revised Taxonomy of Educational Objectives are remembering, understanding, applying, analyzing, evaluating, and creating. A possible activity for applying could be demonstrate, solve, or sketch. &quot;Solve&quot; is in boldface, and the example of true&ndash;false item that reflects &quot;recognize&quot; is asking the student to solve a math equation." title="Figure shows Bloom&#39;s revised taxonomy of educational objectives, a possible activity representing each objective, and examples of an essay item that reflects the boldface verb in the &quot;possible activity&quot; column. In the column for Bloom&#39;s Revised Taxonomy of Educational Objectives are remembering, understanding, applying, analyzing, evaluating, and creating. A possible activity for applying could be demonstrate, solve, or sketch. &quot;Solve&quot; is in boldface, and the example of true&ndash;false item that reflects &quot;recognize&quot; is asking the student to solve a math equation." id="fig_8.12"/>
 	  </div>
   </section>


	  <div class="single-column">
                   <p class="txt">Writing good essay questions is not as time consuming as writing objective items such as multiple-choice questions, but it does require attention to several guidelines. These are summarized in Table 8.10.</p>
	  </div>

<a id="p352"></a>

         <div class="tbl_scroll_on_mobile">
		 <table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 8.10</span> <br />
          <span class="tbl_title">Checklist for constructing good essay items</span>
          </caption>
          <tbody>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Do the essay questions assess intended instructional objectives?</td>
            </tr>
            <tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I worded the stimulus so that the requirements are clear?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Am I assessing more than simple recall?</td>
            </tr>
			<tr class="no_background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I indicated how much time should be spent on each item (usually by saying how many points each item is worth)?</td>
            </tr>
            <tr class="background_tr">
              <td>&#x5f;&#x5f;&#x5f;Yes&#x5f;&#x5f;&#x5f;No&ensp;Have I developed a scoring rubric or checklist?</td>
            </tr>
          </tbody>
     </table>
	 </div>



  </div> 
  <script type="text/javascript" src="Scripts/app-min.js"></script>
</section>


<!--
/////////////////////////////////////////////////////////////
///////////////////////// 8.10 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->



<section class="page" id="sec8.10">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">8.10</span> <span class="sec_title">Planning for Assessment</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What are some important steps in planning for assessment?</span></h2>
  </div>

    <div class="section-lead">     
	   <p class="intro">In Chapter 2&mdash;and again, earlier in this chapter&mdash;we described the various steps that make up an intelligent and effective assessment plan.</p>
    </div>


<div class="section-body">
    <div class="single-column">
       	<ol class="nl">
       		<li class="li_text"><em>Know clearly what your instructional objectives are, and communicate them to your </em><em>students.</em></li>
       		<li class="li_text"><em>Match objectives to larger educational standards; match assessment to objectives; </em><em>match instruction to objectives.</em></li>
       		<li class="li_text"><em>Use formative assessment as an integral part of instruction.</em></li>
       		<li class="li_text"><em>Use a variety of different assessments, especially when important decisions depend on </em><em>their outcomes.</em></li>
       		<li class="li_text"><em>Use blueprints to construct tests and develop keys, checklists, and rubrics to score them.</em></li>
       	</ol>

        <p class="txt">The importance of these five steps can hardly be overemphasized.</p>
    </div>
</div> 
<script type="text/javascript" src="Scripts/app-min.js"></script>
</section>



<!--
/////////////////////////////////////////////////////////////
///////////////////////// EOC //////////////////////////////
/////////////////////////////////////////////////////////////
-->


<section class="page" id="ch8_summary">
  <div class="chapter-review-header">
    <h1 class="eoc_h1"><strong>Chapter 8<br />
      </strong> Themes and Questions</h1>
    <div class="seperator">&nbsp;</div>
  </div>
  <div class="chapter-review">
    <h2 class="h2">Themes</h2>
    <div class="single-column">
      <h3 class="h3"> Planning for Teacher-Made Tests</h3>
                 <p class="txt">Important steps in planning for assessment include clarifying instructional objectives, often by reference to widely accepted common standards, devising test blueprints, matching instruction and assessment to goals, developing rubrics and other scoring guides, and using a variety of approaches to assessment. Assessment should be used for <em>placement</em> decisions (sometimes involving <em>diagnostic assessment</em> to detect problems and suggest interventions), for improving teaching and learning (<em>formative</em> function), and for evaluating and grading achievement and progress (<em>summative</em> function).</p>

<a id="p353"></a>
      <h3 class="h3">Constructed- and Selected-Response Assessments</h3>
                <p class="txt">In addition to performance assessments, there are <em>selected-response</em> and <em>constructed-response</em> approaches to assessment. Selected-response assessments provide the test taker with a choice among responses, one or more of which is correct (for example, <em>multiple-choice</em>, <em>matching</em>, <em>true&ndash;false</em>, and <em>interpretive</em> items). Constructed-response assessments require the responder to generate a response either in the form of a single word or short phrase (<em>completion</em> item) or in the form of shorter or longer essays. The more objective selected-response approaches have higher reliability but take longer to construct and are less suited to tapping higher intellectual processes. PLCs, formal collaborative groupings of educators across schools, school districts, and even states, can contribute dramatically to the educational enterprise.</p>

      <h3 class="h3">Developing Selected-Response Assessments</h3>
                <p class="txt">Multiple-choice items (stem and alternatives) can tap a wide sample of knowledge and skills and tend to have high reliability. They should be clear, unambiguous, free of double-barreled stems or alternatives, as fair as possible, and constructed without double negatives and implausible distracters. They can be written so as to tap higher order cognitive skills. Matching items present two columns with matching entries arranged so they are not juxtaposed. They are easily constructed and provide a quick way of assessing students&#39; content knowledge. Similarly, true&ndash;false items are easily constructed and can sample a wide range of knowledge. But they usually assess little more than recall and are susceptible to the effects of guessing. Interpretive items present information (in writing or in the form of graphs, figures, charts, or video and audio recordings) followed by a series of items (usually multiple-choice) whose answers require <em>interpretation</em> of the material. Interpretive items are useful for assessing skills that go beyond simple recall.</p>
 
     <h3 class="h3">Developing Constructed-Response Assessments </h3>
                <p class="txt">These assessments require learners to <em>generate</em> rather than <em>select</em> responses. They take the form of short-answer (restricted-response) items, which include completion items (fill-in-the-blank items or questions that require very brief, easily keyed answers), and of longer essay-type (extended-response) questions. A principal advantage of constructed-response measures is that they can be designed to tap higher mental processes. However, scoring them is time consuming and highly subjective. Approaches such as using student <em>model</em> responses as a guide (holistic approach), scoring according to predetermined criteria listing all the points and arguments that should be included (analytic approach), scoring one item at a time rather than all items on each paper sequentially, and pooling independent rater scores can each increase the reliability of essay tests.</p>
    </div>


    <h2 class="h2">Applied Questions</h2>
    <div class="single-column">
      <ol class="applied-questions">
        <li class="li_text"><em>What are some important steps in planning for assessment?</em> Develop a detailed, written assessment plan in your area of expertise.</li>
        <li class="li_text"><em>What kinds of teacher-made assessment options are available?</em> Write a brief essay outlining the principal relative advantages and disadvantages of selected- and constructed-response assessments.</li>
        <li class="li_text"><em>What are some guidelines for constructing good selected-response assessments?</em> Construct an interpretive test item related to your area of expertise that taps one or more of the higher level skills in Bloom&#39;s revised taxonomy (discussed in Chapter 4). </li>
        <li class="li_text"><em>What are the advantages and limitations of constructed-response assessments?</em> Generate essay questions that are likely to tap each of the objectives listed in Bloom&#39;s taxonomy.</li>
      </ol>
    </div>

 <a id="p354"></a>
    <h2 class="h2">Key Terms</h2>
	  
	  <script type="text/javascript"> (function(a){a.fn.answerToggle=function(b){var b=a.extend({revealClass:".reveal"},b),c=a(b.revealClass);this.length!=c.length?console.log(this.length,c.length):(a(b.revealClass).hide(),this.each(function(b){var d=a(c[b]);a(this).click(function(a){a.preventDefault();d.slideToggle("fast")})}))}})(jQuery);
$('document').ready(function(){
$('.answerToggle').answerToggle('.reveal');
}); </script>
    <div class="single-column">
                    <p class="tx"><a href="#" class="answerToggle"><strong>alternatives</strong></a>&ensp;</p>
<p class="reveal">A label used for the choices of response in a multiple-choice item.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>binary-choice item</strong></a>&ensp;</p>
<p class="reveal">A more general term for what is usually called a <em>true&ndash;false</em> item. A test item for which the responder must choose between two contrary terms, such as <em>yes</em> and <em>no</em> or <em>true</em> and <em>false</em>.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>blind guessing</strong></a>&ensp;</p>
<p class="reveal">On a test, totally random selection of responses.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>completion item</strong></a>&ensp;</p>
<p class="reveal">A constructed-response test item that requires a brief answer, often consisting of a single word or phrase. Also called a <em>fill-in-the-blank</em> item.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>constructed-response assessments</strong></a>&ensp;</p>
<p class="reveal">Tests that require the student to generate (supply or construct) the correct response, rather than simply selecting it from a choice of alternatives.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>distracters</strong></a>&ensp;</p>
<p class="reveal">In a multiple-choice item, all alternatives that are incorrect.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>essay items</strong></a>&ensp;</p>
<p class="reveal">Constructed-response test items that require the responder to write a relatively lengthy answer.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>interpretive item</strong></a>&ensp;</p>
<p class="reveal">A test item that presents information in the form of a chart, graph, map, drawing, or recording and poses objective questions based on that information.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>marking key</strong></a>&ensp;</p>
<p class="reveal">A guide to correct responses. The phrase is typically used in connection with objective tests such as multiple-choice, true&ndash;false, matching, and completion items.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>matching-test items</strong></a>&ensp;</p>
<p class="reveal">Test items, usually presented in two columns, with the examinee required to select correct matches, one from each column.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>multiple-choice items</strong></a>&ensp;</p>
<p class="reveal">Test items consisting of a <em>stem</em> (question or statement) followed by a number of alternative choices, one or more of which correctly complete the stem.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>Next Generation Science Standards (NGSS)</strong></a>&ensp;</p>
<p class="reveal">Widely used K&ndash;12 science standards developed through collaboration of various national science and education groups, built around the notion that science involves three dimensions: core ideas, practices, and crosscutting ideas.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>premise column</strong></a>&ensp;</p>
<p class="reveal">In logic, a premise is a proposition (statement) underlying an argument or a conclusion. In test construction, a <em>premise column</em> is a list of matching-test items that are to be matched to entries in a second <em>response column</em>. The premise column is usually on the left.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>response column</strong></a>&ensp;</p>
<p class="reveal">In a matching test item, the column that contains the list of items that are to be matched to items in the <em>prem</em><em></em><em>ise column</em>. The response column is usually on the right.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>selected-response assessments</strong></a>&ensp;</p>
<p class="reveal">Assessments such as true&ndash;false or multiple-choice tests, on which the examinee is asked to <em>select</em> the correct response.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>short-answer items</strong></a>&ensp;</p>
<p class="reveal">Test items that ask for a brief <em>constructed</em> response&mdash;sometimes no longer than a single word, as in some <em>fill-</em><em>in-the-blank</em> items.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>stem</strong></a>&ensp;</p>
<p class="reveal">The introductory statement or question in a multiple-choice item.</p>
                    <p class="tx"><a href="#" class="answerToggle"><strong>true&ndash;false item</strong></a>&ensp;</p>
<p class="reveal">A test item in which the responder is asked to indicate simply whether a statement is true or false. Also referred to as a <em>binary-choice</em> item. </p>

    </div>
  </div>

</section>
<script type="text/javascript" src="Scripts/app-min.js"></script>


</body>
</html>
