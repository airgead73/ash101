<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" xmlns:epub="http://www.idpf.org/2007/ops"
  xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" epub:prefix="index: http://www.index.com/">
<head>
<title>Chapter 02</title>
<meta content="width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1" name="viewport" />
<link rel="stylesheet" href="styles/style.css" type="text/css"/>
<link rel="stylesheet" href="styles/mobile.css" type="text/css"/> 
 <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script>
</head>

<body id="ch02" xml:lang="en-US">
<a id="p47"></a>
<section class="co" id="sec2.0">
  <div class="co_header" id="co_header_ch2">
    <h1><span class="ct_number">CHAPTER TWO</span></h1>
    <div class="seperator">&nbsp;</div>
    <h2><span class="ct_title">Purposes and Characteristics of Educational Assessment</span></h2>
  </div>
  <div class="co_quote">
    <p class="quote_text">He uses statistics as a drunken man uses lamp-posts&mdash;for support rather than illumination.</p>
    <cite class="quote_cite">&mdash;Andrew Lang</cite> </div>
  <div class="co_intro">
  <a id="p48"></a>
  <div class="co_text">
    <p class="txt">This is not to imply that teachers are like drunk people leaning on their lampposts, but it is true that some teachers use the results of their assessments like crutches to support their summaries of the characteristics, virtues, and vices of their students.</p>
    <p class="txt">For those teachers, test results and grades serve as handy statistics that summarize all the important things that can be communicated to school administrators and parents.</p>
    <p class="txt">Others use the results of their assessments in very different ways.</p>
    <p class="key-point">They see assessment results not as summarizing important accomplishments, but as tools that suggest to both the learner and the teacher how the teaching&#8211;learning process can be improved.</p>
    <p class="txt">For these teachers, assessments are less like lampposts against which to lean and rest, and more like the light on top that throws back the darkness and shows both teacher and student the path ahead.</p>
	</div>

	 <div class="co_aside" id="co_aside_ch2" alt="Teacher handing back exams results to young students in a classroom." title="Teacher handing back exams results to young students in a classroom.">
	   <div class="co_aside_cr">
	     <div class="co_src"><em>Shironosov/iStock/Getty Images Plus</em></div>
	   </div>
    </div>
  </div>
</section>

<section class="page" id="sec2.1">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.1</span> <span class="sec_title">Purposes of Educational Assessment</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What are the principal uses of educational assessment?</span></h2>
  </div>
  <div class="section-lead">
  <p class="intro">The metaphor of using the lamppost versus the light on top highlights the difference between summative and formative assessment.</p>
  </div>
  <div class="section-body">
	 <div class="single-column">
			
			<p class="txt">As we saw in Chapter 1, summative assessment occurs mainly at the end of an instructional period, and its chief purpose is to provide a grade. It is a summation of the learner&#39;s achievements.</p>
			<p class="txt">In contrast, formative assessment is an integral and ongoing part of instruction. Its central purpose is to provide guidance for both the teacher and the learner in an effort to improve learning. Its goal is <em>formative</em> rather than <em>summative</em>. As we will see in Chapter 5, assessment designed specifically to enhance learning&mdash;in other words, formative assessment&mdash;is the main emphasis of current approaches to educational assessment.</p>
		</div>	
	<div class="text_container">
      <h4><button class="ec_expand" aria-expanded="false" aria-controls="ecbox2-1">LEARN MORE</button></h4>
      <div id="ecbox2-1">
	  <div class="single-column">
        <p class="txt">Placement assessment, often called <em>preassessment</em>, occurs prior to instruction. Its main purpose is to provide educators with information about learner readiness. Placement assessment gauges the extent to which a learner possesses the skills, knowledge, and abilities required for admission into a unit of study, course, program, or institution. Placement assessment can influence the teacher&#39;s choice of content and instructional approaches, as well as the student&#39;s choice of, or eligibility for, a program.</p>
		
        <p class="txt">Placement assessment tries to answer questions such as the following.</p>

		<ul class="bl">
          <li class="li_text">Should Valeria be given the opportunity to join the advanced science club?</li>
          <li class="li_text">Should Isaac be moved into the accelerated reading group?</li>
          <li class="li_text">Is Ali a candidate for further screening?</li>
          <li class="li_text">Is Mia qualified for college admission?</li>
        </ul>
		
        <p class="txt">In some cases <em>diagnostic assessment</em> may also be a part of educational assessment, often as a form of preassessment. The main purpose of diagnostic assessment is to determine the strengths and weaknesses of individual students.</p>

        <p class="txt">Although diagnostic assessment is used mainly prior to instruction, it can also be used during instruction and serve as a guide for lesson planning (see Table 2.1).</p>
		</div>

<div class="tbl_scroll_on_mobile">
        <table class="tbl-xl">
          <caption class="tbl_name">
          <span class="tbl_number">Table 2.1</span> <br/>
          <span class="tbl_title">Kinds of educational assessment based on their main purposes</span>
          </caption>
          <thead class="tbl_header">
            <tr>
              <th>Placement assessment</th>
              <th>Formative assessment</th>
              <th>Summative assessment</th>
              <th>Diagnostic assessment</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Useful for assessing preexisting knowledge and skills</td>
              <td>Designed to improve teaching/learning process</td>
              <td>Summarizes extent to which instructional objectives have been met</td>
              <td>Identifies learner strengths and weaknesses</td>
            </tr>
            <tr >
              <td>Provides information for making decisions about learner&#39;s readiness</td>
              <td>Provides feedback for teachers and learners to enhance learning and motivation</td>
              <td>Provides basis for grading</td>
              <td>Provides information useful for knowing where to start</td>
            </tr>
            <tr>
              <td>Useful for placement and selection decisions</td>
              <td>Enhances learning; fosters self-regulated learning; increases motivation</td>
              <td>Useful for placement and selection decisions</td>
              <td>Suggests placement options; suggests instructional accommodations</td>
            </tr>
			</tbody>
        </table>
    </div>
      <button class="ec_collapse" type="button" name="ecbox2-1"><span>X</span> close</button>
	</div>
	
 </div>
 </div>
 <script type="text/javascript" src="Scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// 2.2 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->
<a id="p49"></a>
<section class="page" id="sec2.2">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.2</span> <span class="sec_title">Guidelines for Good Educational Assessment</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What are the main characteristics of good educational assessment?</span></h2>
  </div>
  <div class="section-lead">
   <p class="intro">While we can distinguish between assessment used for placement, formation, or summation, all forms of educational assessment share the same goal: to help learners learn. Accordingly, educational assessment and instruction are part of the same process: assessment <em>for</em> learning rather than simply <em>of</em> learning.</p>
   </div>
   <div class="section-body">
   <div class="single-column">
   

    <p class="key-point">In one sense the assessment of learners is an assessment of the teacher&#39;s effectiveness and of the soundness and appropriateness of instructional and assessment strategies.</p>

    <p class="txt">This is not to say that when students do well on a test, the credit should go entirely to their teacher. Some students do remarkably well with embarrassingly inadequate instruction, and others perform poorly even with the most gifted teachers. Still, when instructors use clearly stated and well-understood instructional objectives, which are then attained by their students, we can conclude that these instructors have contributed to their students&#39; success.</p>

    <p class="txt">The following guidelines can be highly useful for planning sound and appropriate instruction and assessment.</p>
     </div>
	 <a id="p50"></a>
	 <div class="inner-section">
	<h2 class="h2">Communicating Instructional Objectives</h2>
	<div class="single-column">
	<p class="txt">Quality instruction involves knowing and communicating clear and explicit instructional objectives. Not only do teachers need to understand what their learning objectives are, but students need to know what is expected of them.</p>
    <p class="key-point">If students understand what the most important learning goals are, they are far more likely to reach them than if they are just feeling their way around in the dark.</p>
	<p class="txt">For example, at the beginning of a unit on geography, Mr. Morales tells his seventh graders that at the end of the unit, they will be able to create a map of their town to scale. To do this, he explains, he will have to teach them some mapmaking skills, along with the math they will need to calculate how the map will be scaled. He then begins a lesson on ratios to get the process started.</p>
     </div>
	 </div>
    <h2 class="h2">Aligning Goals, Instruction, and Assessments</h2>
	<div class="single-column">
    <p class="key-point">It is important to match instruction, assessment, and grading to goals.</p>
	<p class="txt">In theory, this guideline might seem obvious; but in practice, it is not always apparent.</p>
	<p class="txt">Consider Mrs. Williams, a high school teacher who delights in teaching obscure details about the lives and societies of the authors whose essays are part of the curriculum. Naturally, her students assume that some of the class objectives have to do with learning these intriguing details. And to no one&#39;s surprise, her tests typically include questions like <em>Name three foods that would have been common in Frederick Douglass&#39;s time</em>. (Among the correct answers are responses like <em>shad and herring, sweet potatoes, bacon skin, ginger-cake, and Indian corn</em>. Students often leave her classes very hungry.)</p>

	<p class="txt">However, her grades are based less on what students learn in class and more on the quality of their English. She has developed an elaborate system for subtracting points based on the number and severity of grammatical and spelling errors. As a result, her students&#39; grades reflect the accuracy of their responses only when their grammar and spelling are impeccable. Thus, her grading does not match her instruction; it exemplifies poor educational alignment.</p>
	

	<h3 class="h3">Educational Alignment</h3>
	<p class="txt"><strong>Educational alignment</strong> is the expression used to describe an approach to education that carefully and deliberately matches learning objectives with curriculum, assessment, and instruction (Lamain, Luyten, &#38; Noort, 2017).</p>

    <p class="key-point">The backward design approach to curriculum and instruction, explained in Chapter 1, is especially useful for ensuring educational alignment.</p>

	<p class="txt">Recall that this approach begins with an unambiguous statement of learning goals: What, specifically, is it that students are to learn? It continues with the development or selection of assessments that clearly and unequivocally determine the extent to which learning goals have been met. Finally, it culminates with the planning of instructional approaches that will provide each learner with the highest probability of reaching learning goals.</p>

     <a id="p51"></a>   
	<p class="txt">As Biggs and Tang (2011) describe it, alignment involves three key components:</p>

	<ol class="nl">
	<li class="li_text">a conscious attempt to provide learners with clearly specified goals</li>
    <li class="li_text">the development and use of assessments that provide feedback to improve learning and to gauge the degree of alignment between goals, instruction, and assessment</li>
    <li class="li_text">the deliberate use of instructional strategies and learning activities designed to foster achievement of learning goals</li>
	</ol>

	<p class="key-point">In summary, good alignment happens when teachers think through their whole unit before beginning instruction.</p>

	<p class="txt">To do so, they specify learning objectives, identify the evidence they will collect to document student learning, and decide on instructional strategies and learner experiences that will maximize the learner&#39;s probability of achieving the goals.</p>
    
	<h3 class="h3">Alignment in Practice</h3>
	<p class="txt">Elementary school teacher Mrs. Murray provides a good example of educational alignment in her design of a unit on watersheds, areas of land where water predictably flows into certain rivers, basins, or oceans.</p>

	<p class="txt">Her goal was for students to meet the Virginia science standard: Science 4.8, section (a)&mdash;<em>The student will investigate and understand important VA natural resources (a) watershed and water resources</em> (Mrs. Murray&#39;s Fourth Grade Science, 2018). To organize her unit, she used a focusing question: &quot;What happens to the water flowing down your street after a big rainstorm?&quot; The learning goal (objective) was for students to be able to demonstrate, in measurable terms, their understanding of the overarching concept that every action has a consequence&mdash;in this case that the flow of water affects areas downstream. The instructional strategies she selected had children actively engaged in a variety of tasks: They discussed and debated issues of pollution and their personal ability and responsibility to prevent pollution, created vocabulary tools to learn relevant terminology, and built models of watersheds.</p>

	<p class="txt">As students performed these tasks, Mrs. Murray informally assessed the performance of each child, documenting individual knowledge and skill, providing ongoing feedback to each student (formative assessment), and gauging the learner&#39;s ability to demonstrate comprehension by describing important details and the implications of water flows. In addition to taking multiple quizzes throughout the unit, the students demonstrated their understanding of important concepts by completing a performance-based task. Everything was arranged to maintain alignment between learning objectives, assessments, and instruction.</p>
	
                 <div class="video-center">
	<p class="video-head">Purpose of Assessment</p>
	
	<p class="video-caption">A principal explains the difference between teaching and learning in regards to assessment.</p>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1501441/sp/150144100/embedIframeJs/uiconf_id/24035961/partner_id/1501441?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=0_3rexmhs2&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp&amp;wid=0_hgwvi6fj" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player"></iframe>

	<div class="clear-both"></div>
	
<p class="media">
Media not working on mobile? Try opening this eBook on your desktop or laptop computer.</p>
<div class="clear-both"></div>

<p class="video-caption">Critical-Thinking Questions</p>
<ul class="bl">
<li class="li_text">How is this new use of assessment an approach to teaching, rather than an approach to testing? </li>
<li class="li_text">What kinds of student records do teachers need to keep in an assessment-for-learning paradigm? </li>
<li class="li_text">How might technology help teachers track student learning?</li>
	</ul>
<div class="clear-both"></div>				
</div>  
       
       </div>
	<div class="inner-section">
	<h2 class="h2">Using Assessment to Improve Instruction</h2>
	<div class="single-column">
    <p class="key-point">The most effective teachers are those who use assessment as an integral part of the teaching&#8211;learning process&mdash;that is, those who make frequent, deliberate use of formative assessment.</p>

	<a id="p52"></a>
	<p class="txt">As we will see in Chapter 5, good formative assessment provides learners with frequent and timely feedback that is of immediate assistance in helping them learn.</p>

	<p class="txt">This does not mean that teachers need to develop specially designed or carefully constructed tests and quizzes to monitor their learners&#39; progress. Formative assessment often consists of making simple observations of student behavior and noting their responses to questions and instructions&mdash;observations that allow the teacher to provide learners with immediate feedback designed to assist their learning.</p>
	</div>
	</div>
	<h2 class="h2">Using Different Approaches to Assessment</h2>
	<div class="single-column">
    <p class="key-point">Effective, fair, and accurate assessments require a variety of different approaches, especially when important decisions depend on their outcomes.</p>

	<p class="txt">As we will see later in this chapter, test results are not always entirely valid (meaning their results are not appropriate for the ways in which they are interpreted and used) or reliable (that is, they don&#39;t always measure very accurately). The results of a single test might reflect temporary influences such as those related to fatigue, test anxiety, illness, distractions, or other factors. Grades and decisions based on a variety of assessments are more likely to be fair and valid.</p>

	<p class="txt">For example, when individuals are ready to demonstrate their driving skills and knowledge, they are subject to multiple assessments by the Department of Motor Vehicles (DMV). Drivers need to know the rules of the road and demonstrate that they know how to, say, parallel park. Therefore, DMV assessments include both a written and a driving field test&mdash;two different approaches to assessment.</p>
	</div>
	<div class="inner-section">
	<h2 class="h2">Constructing Tests According to Blueprints</h2>
	<div class="single-column">

	<p class="txt">A house construction blueprint describes in detail the components of a house&mdash;such as its dimensions, the number of rooms and their placement, the building materials to be used, the pitch of its roof, the depth of its basement, and its profile from different directions. A skilled contractor can read a blueprint and almost see the completed house.</p>

	<p class="txt">In much the same way, a <strong>test blueprint</strong> describes in detail the nature of the items to be used in building the test. It includes information about the number of items the test will contain, the content areas these items will tap, and the intellectual processes that will be assessed. A skilled educator can look at a test blueprint and almost see the completed test. Figure 2.1 further describes the guidelines for assessment.</p>
    </div>
	<a id="p53"></a>

   <figure class="figure-1">
	   <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 2.1</span><br />
          <span class="figure_title">Guidelines for assessment</span> </h4>
          <p class="caption">These guidelines are most useful when <em>planning</em> for assessment. Many other considerations must be kept in mind when devising, administering, grading, and interpreting teacher-made tests.</p>
		  <p class="src"></p>
	   </figcaption>

          <a class="trigger-readmore_for_Figure" href="#fig2-1" data-type="tipTrigger">
	        <img src="figures/Figure_2.1.jpg" alt="Five guidelines for assessment illustrated in a circular format: know and communicate learning targets; align instruction, goals, and assessment; use assessment to improve instruction; use a variety of assessments; and develop blueprints to construct tests." title="Five guidelines for assessment illustrated in a circular format: know and communicate learning targets; align instruction, goals, and assessment; use assessment to improve instruction; use a variety of assessments; and develop blueprints to construct tests." id="fig_2.1"/>
	     </a>
   </figure>

   <section class="tipBox" id="fig2-1">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_2.1.jpg" alt="Five guidelines for assessment illustrated in a circular format: know and communicate learning targets; align instruction, goals, and assessment; use assessment to improve instruction; use a variety of assessments; and develop blueprints to construct tests." title="Five guidelines for assessment illustrated in a circular format: know and communicate learning targets; align instruction, goals, and assessment; use assessment to improve instruction; use a variety of assessments; and develop blueprints to construct tests." id="fig_2.1"/>
 	  </div>
   </section> 

     

	  </div>
  </div> 
<script type="text/javascript" src="Scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// 2.3 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->

<section class="page" id="sec2.3">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.3</span> <span class="sec_title">Test Blueprints</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Questions</span> <span class="focus_q">Why are test blueprints helpful? What are the characteristics of a useful test blueprint?</span></h2>
  </div>
  <div class="section-lead">
   <p class="intro">It is important to keep in mind that tests are only one form of educational assessment.</p>
   </div>
  <div class="section-body">
   <div class="single-column">
   

    <p class="txt"><em>Assessment</em> is a broad term referring to all the various methods that might be used to obtain information about different aspects of teaching and learning. The word <em>test</em> is more precise: It refers to specific instruments or procedures designed to measure student achievement, progress, or characteristics.</p>

    <p class="txt">Educational tests are quite different from many of the other measuring instruments we use&mdash;instruments like rulers, tape measures, thermometers, and speedometers. These instruments measure directly and relatively exactly: We rarely have reason to doubt them.</p>

	<a id="p54"></a>
    <p class="txt">Our psychological and educational tests aren&#39;t like that: They measure indirectly and with varying accuracy. In effect, they measure a sample of behaviors. From students&#39; behaviors (responses), we make inferences about qualities we can&#39;t really measure directly at all. Thus, from a patient&#39;s response to a question such as &quot;What is the first word you think of when I say <em>mother</em>?,&quot; the psychologist makes inferences about hidden motives and feelings&mdash;and perhaps eventually arrives at a diagnosis.</p>
   
	<p class="txt">In much the same way, teachers make inferences about what learners know&mdash;and perhaps about learners&#39; thought processes as well&mdash;from responses to a handful of questions like this one.</p>

    <p class="txt"><em>Which of the following is most likely to be correct?</em></p>

	<ol class="nl">
	<li class="li_text"><em>Mr. Dayzie will still be alive at the end of the story.</em></li>
    <li class="li_text"><em>Mr. Dayzie will be in jail at the end of the story.</em></li>
    <li class="li_text"><em>Mr. Dayzie will have died within the next 30 pages.</em></li>
	<li class="li_text"><em>Mr. Dayzie will not be mentioned again.</em></li>
	</ol>
	<p class="txt">Tests that best allow the teacher to make valid and useful inferences are those that actually tap the knowledge and skills that make up course objectives. And the best way to ensure that this is the case is to use test construction blueprints that take these objectives into consideration (see Table 2.2 for an examples of a test blueprint).</p>
     </div>
<div class="tbl_scroll_on_mobile">	 
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_number">Table 2.2</span><br />
      <span class="tbl_title">Partial test blueprint for a short-essay test based on parts of the first chapter of this text</span>
      </caption>
      <thead class="tbl_header">
        <tr>
          <th>Domain</th>
          <th>Example of relevant test items</th>
		  <th>&#37; weighting</th>
          <th>&#35; of items</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Specific knowledge</strong></td>
          <td>Describe the three main kinds of educational assessment.</td>
          <td>20&#37;</td>
		  <td>2</td>
        </tr>
        <tr>
          <td><strong>Comprehension</strong></td>
          <td>Explain the implications of FERPA for a classroom teacher.</td>
          <td>30&#37;</td>
		  <td>3</td>
        </tr>
        <tr>
          <td><strong>Application</strong></td>
          <td>Illustrate the use of the backward design approach in aligning goals, assessments, and instruction in your area of expertise.</td>
          <td>50&#37;</td>
		  <td>1</td>
        </tr>
        <tr>
          <td><strong>Totals</strong></td>
          <td></td>
          <td>100&#37;</td>
		  <td>6</td>
        </tr>
      </tbody>
      <tfoot class="no-border">
        <tr>
          <td colspan="4"><em>Note</em>. This is a partial, rather than a complete, test blueprint. A complete blueprint would contain most of the features listed in the following section. Also, there are many different ways to construct test blueprints. For example, in Chapter 4, Tables 4.3 and 4.4 show examples of test blueprints based on a revision of Bloom&#39;s taxonomy of educational objectives.</td>
        </tr>
      </tfoot>
    </table>
</div>
	<a id="p55"></a>
	<div class="inner-section">
    <h2 class="h2">Guidelines for Constructing Test Blueprints</h2>
	<div class="single-column">
    <p class="txt">A good test blueprint will contain most of the following elements:</p>

	<ul class="bl">
          <li class="li_text">a clear statement of the test content related directly to instructional objectives</li>
          <li class="li_text">the performance, affective, or cognitive skills to be tapped</li>
          <li class="li_text">an indication of the test format, describing the kinds of test items to be used or the nature of the performances required</li>
          <li class="li_text">a summary of how marks are to be allocated in relation to different aspects of the content</li>
		  <li class="li_text">some notion of the achievement levels expected of learners</li>
          <li class="li_text">an indication of how achievement levels will be graded</li>
          <li class="li_text">a review of the implications of different grades</li>
    </ul>
	
	<p class="txt">Regrettably, not all teachers use test blueprints. Instead, when a test is required, many find it less trouble to sit down and simply write a number of test items that seem to them a reasonable examination of what they have taught. However, in too many cases what they have taught was not sufficiently guided by specific instructional objectives. That would have made test construction far easier.</p>
	</div>
	</div>

	<h2 class="h2">Advantages of Using Test Blueprints</h2>
	<div class="single-column">
	<p class="txt">Using test blueprints has a number of important advantages and benefits. Among them is that they encourage the teacher to clarify learning objectives and to make decisions about the importance of different aspects of content. They also encourage teachers to become more aware of the learner&#39;s cognitive processes and, by the same token, to pay more attention to the development of higher cognitive skills.</p>

	<p class="key-point">Good test blueprints can go a long way toward ensuring alignment between learning targets, assessments, and instruction.</p>
	
	<p class="txt">At a more practical level, using test blueprints makes it easier for teachers to produce similar tests at different times, thus maintaining uniform standards and allowing for comparisons among different classes and different students. Also, good test blueprints serve as a useful guide for constructing test items and perhaps make the teacher&#39;s work easier in the long run. Figure 2.2 summarizes some of the many benefits of using test blueprints.</p>
	</div>

	<a id="p56"></a>
	
      <figure class="figure-1">
	          <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 2.2</span><br />
          <span class="figure_title">Advantages of using test blueprints</span> </h4>
        <p class="caption">Making and using test blueprints presents a number of distinct benefits. Although developing blueprints can be time-consuming, it can ultimately make the teacher&#39;s task easier.</p>
		  </figcaption>

            <a class="trigger-readmore_for_Figure" href="#fig2-2" data-type="tipTrigger">
	        <img src="figures/Figure_2.2.jpg" alt="Advantages of using test blueprints are listed as follows in circular format: forces teacher to clarify learning targets; promotes the development of thinking rather than mainly remembering skills; encourages teachers to become more aware of learners&#39; cognitive activity; promotes decisions about the relative importance of different aspects of content; leads to more consistency among different tests, allowing more meaningful comparisons; simplifies test construction; increases test validity and reliability; and promotes alignment between objectives, assessment, and instruction." title="Advantages of using test blueprints are listed as follows in circular format: forces teacher to clarify learning targets; promotes the development of thinking rather than mainly remembering skills; encourages teachers to become more aware of learners&#39; cognitive activity; promotes decisions about the relative importance of different aspects of content; leads to more consistency among different tests, allowing more meaningful comparisons; simplifies test construction; increases test validity and reliability; and promotes alignment between objectives, assessment, and instruction." id="fig_2.2"/>
			</a>
	  </figure>

    <section class="tipBox" id="fig2-2">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_2.2.jpg" alt="Advantages of using test blueprints are listed as follows in circular format: forces teacher to clarify learning targets; promotes the development of thinking rather than mainly remembering skills; encourages teachers to become more aware of learners&#39; cognitive activity; promotes decisions about the relative importance of different aspects of content; leads to more consistency among different tests, allowing more meaningful comparisons; simplifies test construction; increases test validity and reliability; and promotes alignment between objectives, assessment, and instruction." title="Advantages of using test blueprints are listed as follows in circular format: forces teacher to clarify learning targets; promotes the development of thinking rather than mainly remembering skills; encourages teachers to become more aware of learners&#39; cognitive activity; promotes decisions about the relative importance of different aspects of content; leads to more consistency among different tests, allowing more meaningful comparisons; simplifies test construction; increases test validity and reliability; and promotes alignment between objectives, assessment, and instruction." id="fig_2.2"/>
 	</div>
   </section> 



  </div> 
<script type="text/javascript" src="Scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// 2.4 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->

<section class="page" id="sec2.4">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.4</span> <span class="sec_title">Why Some Tests Are Unfair</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What different factors might make a test unfair?</span></h2>
  </div>
  <div class="section-lead"> 
  <p class="intro">Determining the best assessment procedures and instruments is no simple matter and is thus not without controversy. Although educators and parents do not always agree on these matters, there is general consensus about the characteristics of good measuring instruments.</p>
  </div>
  <div class="section-body">
   <div class="single-column">
    
	<a id="p57"></a>
    <p class="key-point">Most important is that evaluative instruments be fair and that students see them as being fair.</p>

    <p class="txt">The most common student complaint about tests and testing practices has to do with their imagined or real lack of fairness (Bouville, 2008; Takayama, 2018). The importance of test fairness was highlighted during the Vietnam War in the 1960s. President John F. Kennedy&#39;s decision to send troops to Vietnam led large numbers of age-eligible men to be drafted as soldiers, some of whom died or were seriously injured in the war. Men who went to college were usually exempt from the draft&mdash;or their required military service was at least deferred. So, to avoid being drafted, it became crucial for many young men to be admitted to undergraduate or postgraduate studies. For some, passing college or graduate entrance exams was literally a matter of life and death. Therefore, it was vital that the admission exams be as fair as possible.</p>

    <p class="key-point">Just how fair are our educational assessments? We don&#39;t always know. Yet science provides ways to define and sometimes actually measure the characteristics of tests.</p>
   
	<p class="txt">For example, the best assessment instruments have three important qualities:</p>
     
	<ol class="nl">
	<li class="li_text">fairness</li>
    <li class="li_text">validity</li>
    <li class="li_text">reliability</li>
	</ol>

    <p class="txt">There are two ways of looking at <strong>test fairness</strong>, explains Bouville (2008). On the one hand, there is fairness of treatment; on the other, there is fairness of opportunity (Table 2.3). Fairness of treatment issues include problems relating to not making accommodations for children with special needs, cultural biases and stereotypes, the use of misleading &quot;trick&quot; questions, and inconsistent grading. Fairness of opportunity problems include testing students on material not covered, not providing an opportunity to learn, not allowing sufficient time for the assessment, and not guarding against cheating. We will look at each of these issues of test fairness in the next few sections, followed by a discussion of test validity and reliability.</p>
    </div>

	 <div class="tbl_scroll_on_mobile">
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_number">Table 2.3</span><br />
      <span class="tbl_title">Issues affecting test fairness</span>
      </caption>
      <thead class="tbl_header">
        <tr>
          <th>Issues of fairness of opportunity</th>
          <th>Issues of fairness of treatment</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><ul class="bl">
		  <li class="li_text">Not covering tested material</li>
		  <li class="li_text">Not providing an opportunity to learn</li>
		  <li class="li_text">Not allowing sufficient time to complete test</li>
		  <li class="li_text">Not guarding against cheating</li>
		  </ul></td>

          <td><ul class="bl">
		  <li class="li_text">Not accommodating special needs</li>
		  <li class="li_text">Not providing for reducing anxiety</li>
		  <li class="li_text">Not making provisions for ELL students</li>
		  <li class="li_text">Being influenced by biases and stereotypes</li>
		  <li class="li_text">Using misleading, trick questions</li>
		  <li class="li_text">Grading inconsistently</li>
		  </ul></td>
        </tr>
      </tbody>
    </table>
	</div>
	<div class="inner-section">
	<h2 class="h2">Content Problems</h2>
	<div class="single-column">
    <p class="txt">Tests are&mdash;or at the very least, seem&mdash;highly unfair when they ask questions or pose problems about matters that have not been covered or assigned.</p>

	<p class="key-point">Sometimes this issue stems from bad teaching; other times, it simply relates to bad test construction.</p>

	<a id="p58"></a>
	<p class="txt">For example, in Luke&#39;s second year in high school, he had a teacher who invariably peppered her quizzes and exams with questions about matters she had not discussed in class. &quot;We didn&#39;t have time,&quot; she would say when someone complained or pointed out that she had never mentioned <em>rhombuses</em> or <em>trapezoids</em> or <em>quadrilaterals</em>. &quot;But it&#39;s important and it&#39;s in the book and it might be on the final exam,&quot; she would add.</p>

    <p class="txt">Had she simply told her students that they were responsible for the content of Chapter 6, they would not have felt so unfairly treated. This example illustrates bad teaching as much as bad test construction.</p>

	<p class="txt">In connection with content problems that affect test fairness, it is interesting to note that when test scores are higher, students tend to perceive the test as being more fair. As Oller (2012) points out, higher scores are evidence that test makers and the better students agree that the content being tested on is most important. This agreement illustrates <em>educational alignment</em>: close correspondence among goals, instructional approaches, and assessments</p>

	<p class="txt">Conversely, exams that yield low scores for all students may reflect poor educational alignment: They indicate that what the teacher chose to test is not what even the better learners have learned. Hence, there is good reason to believe that tests that yield higher average scores are, in fact, more fair than those on which most students do very poorly. Moreover, teachers who raise learner marks, perhaps by scaling them so that they approximate a normal distribution with an acceptably high average, do little to alter students&#39; perceived fairness of the test.</p>
	</div>
	</div>
	
	<h2 class="h2">Trick Questions</h2>
	<div class="single-column">

	<p class="txt"><strong>Trick questions</strong> illustrate a problem that has less to do with test content than with test construction. In fact, some test makers are not even aware that they have written a trick questions.</p>

	<p class="key-point">Trick questions mislead and deceive, regardless of whether the deception is intentional or simply due to poor item construction. They do not test the intended learning targets but rather a student&#39;s ability to navigate a deceptive test.</p>

	<p class="txt">Items that students are most likely to consider trick questions include the following.</p>

	<ol class="nl">
	<li class="li_text">Questions that are ambiguous (even when the ambiguity is accidental rather than deliberate). They are ambiguous when they have more than one possible interpretation. For example, the question &quot;Did you see the man in your car?&quot; could be interpreted two ways: &quot;Did you see the man <em>who is in</em> your car?&quot; or &quot;Did you see the man when <em>you</em> were in your car?&quot; It is hard to answer a question correctly if the meaning of the question itself is unclear.</li>

    <li class="li_text">Multiple-choice items where two nearly identical alternatives seem correct. Or, as in the following example, where <em>all</em> alternatives are potentially correct:
	<br/><em>The Spanish word</em> fastidiar <em>means:</em><br/>
	<u>&#10003;</u> <em>annoy</em><br/>
	<u>&#10003;</u> <em>damage</em><br/>
	<u>&#10003;</u> <em>disgust</em><br/>
	<u>&#10003;</u> <em>harm</em></li>

    <li class="li_text"><a id="p59"></a>Items deliberately designed to catch students off their guard. For example, consider this item from a science test:
	<br/><em>During a very strong north wind, a rooster lays an egg on a flat roof: On what side of the roof is the egg most likely to roll off?</em><br/>
	<u>&nbsp;&ensp;</u> <em>north</em><br/>
	<u>&nbsp;&ensp;</u> <em>south</em><br/>
	<u>&nbsp;&ensp;</u> <em>east</em><br/>
	<u>&nbsp;&ensp;</u> <em>west</em><br/>
	<u>&#10003;</u> <em>No egg will roll off the roof.</em><br/>
	Students who aren&#39;t paying sufficient attention on this fast-paced, timed test might well say <em>south</em>. Seems reasonable. (But no; apparently, <em>roosters</em> rarely lay eggs.)</li>

	<li class="li_text">Questions that use double negatives. For example: <em>Is it true that people should never not eat everything they don&#39;t like?</em> The use of <em>never</em> and <em>not</em> together makes it difficult to determine what the question is really asking.</li>

	<li class="li_text">Items in which a seemingly trivial word turns out to be crucial. This is often the case for words such as <em>always, never, all,</em> and <em>most,</em> as in this item: <em>True or False? Organic products are</em> always <em>better for you than those that are nonorganic.</em> In this case the word <em>always</em> is critical. The learner would have to be certain the statement is true or false in every single imaginable situation.</li>

	<li class="li_text">Items that make a finer discrimination than expected. For example, say a teacher has described the speed of sound in dry air at 20 degrees centigrade as being right around 340 meters per second. Now she presents her students with this item:
	<br/><em>What is the speed of sound in dry air at 20 degrees centigrade?</em><br/>
	<u>&nbsp;&ensp;</u> <em>A. 339.2 meters per second</em><br/>
	<u>&nbsp;&ensp;</u> <em>B. around 340 meters per second</em><br/>
	<u>&#10003;</u> <em>C. 343.2 meters per second</em><br/>
	<u>&nbsp;&ensp;</u> <em>D. 343.8 meters per second</em><br/>
	Because the alternatives contain both the correct answer (C) and the less precise information given by the teacher (B), the item is deceiving.</li>
	</ol>
        
                     <div class="video-center">
	<p class="video-head">Fair and Reliable Assessments</p>
	
	<p class="video-caption">An assistant superintendent explains fairness and reliability.</p>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1501441/sp/150144100/embedIframeJs/uiconf_id/24035961/partner_id/1501441?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=0_mi7c6chp&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=0_m5zgqoty" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player"></iframe>

	<div class="clear-both"></div>
	
<p class="media">
Media not working on mobile? Try opening this eBook on your desktop or laptop computer.</p>
<div class="clear-both"></div>

<p class="video-caption">Critical-Thinking Questions</p>
<ul class="bl">
<li class="li_text">How can a teacher tell if the scores from the classroom assessment are the result of a lack of learning, a gap in teaching, a poorly written assessment, a lack of alignment between the test and the material, or something else? </li>
<li class="li_text">How does standardizing testing procedures help fairness? </li>
<li class="li_text">What kinds of individualized accommodations for classroom testing are teachers asked to make for students with special needs?</li>
	</ul>
<div class="clear-both"></div>				
</div>      
        
	</div>
	
	<div class="inner-section">
	<h2 class="h2">Lack of Opportunity to Learn</h2>
	<div class="single-column">

	<p class="txt">Tests are patently unfair when they sample concepts, skills, or cognitive processes that students have not had an opportunity to acquire.</p>

	<p class="key-point">Lack of opportunity to learn might reflect an instructional problem.</p>

	<p class="txt">For example, it might result from not being exposed to the material either in class or through instructional resources.</p>

	<p class="key-point">It might also result from not having sufficient time to learn.</p>

	<p class="txt">Bloom (1976), for example, believed that there are faster and slower learners (not gifted learners and those less gifted), and that all learners, given sufficient time, can master what schools offer.</p>
	</div>
	
	<a id="p60"></a>
	<div class="text_container">
      <h4><button class="ec_expand" aria-expanded="false" aria-controls="ecbox2-2">LEARN MORE</button></h4>
      <div id="ecbox2-2">

	  <figure class="photo-right"> <img src="images/2.1.jpg" alt="Middle school students taking an exam. " title="Middle school students taking an exam." id="img2.1"/>
        <figcaption>
          <p class="cr">lisafx/iStock/Getty Images Plus</p>
          <p class="caption">Ambiguous questions, misleading items, items about material not covered or assigned, overly long tests&mdash;all of these contribute to the perceived unfairness of tests.</p>
        </figcaption>
      </figure>

	  <div class="single-column">
        <p class="txt">If Bloom is correct, the results of many of our tests indicate that we simply don&#39;t allow some of our learners sufficient time for what we ask of them. Bloom&#39;s <em>mastery learning</em> system offers one possible solution. <strong>Mastery learning</strong> describes an instructional approach in which course content is broken into small, sequential units and steps are taken to ensure that all learners eventually master instructional objectives. Khan Academy (2018), a free website that boldly promises &quot;You can learn anything. For free. For everyone. Forever,&quot; is based on a mastery learning approach. (See Chapter 6 for a discussion of mastery learning).</p>
		
        <p class="txt">Another solution, suggests Beem (2010), is to expanded the use of technology and of <strong>virtual reality instructional programs</strong>. These are instructional computer-based simulations designed to provide a sensation of realism. She argues that these, along with other digital technologies such as computers and handheld devices, offer students an opportunity to learn at their own rate.</p>
		
        <p class="txt">As Ferlazzo and Sypnieski (2018) argue, great teaching is about giving students the opportunity to learn. Poor and unfair testing is about assessing the extent to which they have reached instructional objectives they have never had an opportunity to reach. See <em>Applications: Addressing Unfair Tests</em>.</p>
		

	 <div class="single-column">
      <p class="txt"><a class="trigger-readmore" href="#tip2-1" data-type="tipTrigger">Applications: Addressing Unfair Tests</a></p>
    </div>
    <section class="tipBox" id="tip2-1">
      <div class="box-6">
        <h3 class="h3">Applications: Addressing Unfair Tests</h3>
        <p class="txt">After adopting the CCSS in 2010, New York State hired a test publishing company to design standardized tests that would reflect the knowledge and skills described by the CCSS. In the spring of 2013, New York State students in grades 3 through 8 had their first experience with English Language Arts tests designed to be in alignment with the CCSS.</p>
		
        <p class="txt">According to school administrators, it was a harrowing ordeal for their students. After witnessing their students&#39; anguish during and following the tests, 21 principals were so outraged that they felt compelled to issue a formal protest by writing a letter to the state&#39;s commissioner of education. In it, they argued that the English Language Arts tests were unfair. One of their major concerns was the lack of alignment between the types of questions asked and the critical-thinking skills valued in the CCSS. The CCSS emphasize deep and rich analysis of fiction and nonfiction. But the English Language Arts tests focused mostly on specific lines and words rather than on the wider standards. What was taught in the classrooms was not assessed on the tests: The tests failed to meet the criterion of fairness of opportunity.</p>

		<p class="txt">While alignment is important, this was not the administrators&#39; only complaint. Administrators also argued that the tests&#39; structure was not developmentally appropriate. For example, testing required three 90-minute sessions on three consecutive days&mdash;a difficult undertaking for a mature student, let alone for a 10-year-old. Clearly, these tests violated the criterion of fairness of opportunity.</p>
		
        <p class="txt">Finally, the principals expressed concern that too much was riding on a flawed test. After all, students&#39; promotion to the next grade and admission to special programs are often based on these tests. In addition, teachers and schools are evaluated in terms of how well their students perform, even though that is not how the tests are supposed to be used. As a result, scores on these tests can affect the extent to which schools receive special funds or are put on improvement plans. These complications raise questions of the tests&#39; validity for these purposes.</p>

	    <p class="txt">Clearly, as the principals reflected on the new English Language Arts tests, they saw problems with both fairness of opportunity and validity.</p>
		</div>
	</section>
		  
		  </div>
      <button class="ec_collapse" type="button" name="ecbox2-2"><span>X</span> close</button>
	  </div>
      
	</div>
	</div>

	<a id="p61"></a>
	<h2 class="h2">Insufficient Time</h2>
	<div class="single-column">
	<p class="txt">Closely related to not having an opportunity to learn is the injustice of a test that does not give students a chance to demonstrate what they have learned. For some learners, this can happen simply because they tend to test more slowly than others and, as a result, often run out of time to complete a test.</p>

	<p class="txt">Suppose that a 100-item test is designed to sample all the target skills and information that define a course of study. If a student has time to respond to only 80 of these 100 items, then only 80&#37; of the instructional objectives have been tested. That test is probably unfair for that student.</p>

	<p class="txt">Some students may spend an inordinate amount of time taking a test simply because they do not know how to correctly respond to questions. In this case giving them more time would neither improve their score nor make the test more fair. However, some students might require more time for testing because of issues such as physical problems (e.g., arthritis or paralysis), learning disabilities (which can often affect a student&#39;s ability to process reading- or math-related problems), or language barriers (e.g., when the language of the test is not the learner&#39;s primary language). In these cases accommodations need to be made if the testing process is to be fair.</p>
	
	<p class="txt">There is a place for what is known as &quot;speeded testing,&quot; which is aimed at testing basic cognitive abilities in a very limited time. For example, processing speed can be a factor when assessing certain abilities that contribute to intelligence. (We will look at some of these tests in Chapter 10.)</p>

	<p class="key-point">But as a rule of thumb, teacher-made tests should always be of such a length and difficulty level that most, if not all, students can easily complete them within the allotted time.</p>
    </div>
</div>
<script type="text/javascript" src="scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// 2.5 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->
<a id="p62"></a>
<section class="page" id="sec2.5">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.5</span> <span class="sec_title">Accommodations for Special Needs</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Questions</span> <span class="focus_q">What accommodations might be appropriate for learners with special needs? How do these accommodations make assessment fair?</span></h2>
  </div>
  <div class="section-lead">
  <p class="intro">Timed tests can be especially unfair to some learners with special needs.</p>
  </div>
  <div class="section-body">
   <div class="single-column">
    

    <p class="txt">For example, Gregg and Nelson (2012) reviewed many studies that looked at performance on timed graduation tests&mdash;a form of high-stakes testing, the results of which have important consequences for transitioning from high school, school funding, and even the careers of teachers and administrators. These researchers found that, although students with learning disabilities would normally be expected to achieve lower-than-average test scores, if given the extra time they require, they would often perform at a level comparable to students without disabilities.</p>

    <p class="txt">Giving special needs students extra time for a test is a common accommodation and one widely used in North American schools and in postsecondary institutions. Sokal and Vermette (2017) report that although these accommodations are highly useful and fair, it is often the case that not enough extra time is provided. In addition, they note that these learners often require additional extra time throughout their postsecondary years.</p>
	</div>
	<div class="inner-section">
	<h2 class="h2">Accommodations for Test Anxiety</h2>
	<div class="single-column">
    <p class="txt">There is considerable evidence that test anxiety can significantly interfere with a student&#39;s performance on a test (e.g., Putwain &#38; Aveyard, 2018). To improve the performance of learners with test anxiety, instructors can discuss with their class how to reduce anxiety as well as teach specific relaxation techniques (Putwain, Daly, Chamberlain, &#38; Sadreddini, 2015), use game-based testing situations (Mavridis &#38; Tsiatsos, 2016), and use humor, both during tests and in general (Wyman, 2017). For severe cases of test anxiety, certain cognitive and behavioral therapies, undertaken by a skilled therapist, are sometimes highly effective (e.g., Ulusoy, Yavuz, Esen, Umut, &#38; Karatepe, 2016).</p>
   
	<p class="txt">Another way to alleviate test anxiety is to reduce negative attitudes toward certain subjects such as mathematics. As Putwain and Best (2011) showed, elementary school students are sometimes led to fear a subject if they are told that it will be difficult and that their performance will affect important decisions about their lives. Such information can cause their performance to suffer. The lesson is clear: Teachers should not try to motivate their students by appealing to their fears.</p>
	</div>
	</div>
	
    <h2 class="h2">Accommodations for English Language Learners</h2>
	<div class="single-column">
    <p class="txt">Considerable research indicates that children whose first language is not the dominant school language are often at a measurable disadvantage in school. These children are often referred to <strong>English language learner (ELL)</strong> students. Other widely used, related acronyms include <em>ESL</em> (English as a second language); <em>TESOL</em> (teachers of English as a second language); <em>TEFL</em> (teachers of English as a foreign language); and <em>ESOL</em> (English to speakers of other languages).</p>

	<a id="p63"></a>
	<p class="txt">The disadvantages of ELL students can become especially apparent if no accommodations are made in assessment instruments and procedures&mdash;as is sometimes the case for standardized tests given to children whose dominant language is not the test language (Haman et al., 2017). As Lakin and Lai (2012) note, without accommodations for ELL students, there are some serious issues with measuring their abilities fairly and reliably. As we saw in Chapter 1, the law requires accommodations to be made in these cases. Teachers have two main types of classroom accommodations for testing ELLs at their disposal: changing the tests themselves (for example, using simpler language, adding visual aids, or even providing tests that are translated into the student&#39;s language) and changing the testing procedure (for example, giving the student more time to complete the test, allowing the use of a dictionary, reading questions and instructions out loud, and providing verbal guidance during the test).</p>
	</div>
	<div class="inner-section">
	<h2 class="h2">Accommodations for Other Special Needs</h2>
	<div class="single-column">
	<p class="txt">Teachers must be sensitive to, and make accommodations for, many other special needs. These might include medical problems, sensory disabilities such as vision and hearing problems, emotional exceptionalities, learning disabilities, and intellectual disabilities. They might also include cultural and ethnic differences among learners. Table 2.4 describes some of the accommodations that might be required to fairly assess students with special needs.</p>
	</div>
<div class="tbl_scroll_on_mobile">
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_number">Table 2.4</span><br />
      <span class="tbl_title">Possible accommodations for the fair assessment of students with special needs</span>
      </caption>
      <thead class="tbl_header">
        <tr>
          <th>Instructional accommodations</th>
          <th>Testing accommodations</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><ul class="bl">
		  <li class="li_text">Teacher aides and other professional assistance</li>
		  <li class="li_text">Special classes and programs</li>
		  <li class="li_text">Individual education plans</li>
		  <li class="li_text">Special materials such as large print or audio devices</li>
		  <li class="li_text">Provisions for reducing test anxiety</li>
		  <li class="li_text">Increased time for learning</li>
		  </ul></td>

          <td><ul class="bl">
		  <li class="li_text">Increased time for test completion</li>
		  <li class="li_text">Special equipment for test taking</li>
		  <li class="li_text">Different form of test (for example, verbal rather than written)</li>
		  <li class="li_text">Testing in a different setting</li>
		  <li class="li_text">Testing in a different language</li>
		  </ul></td>
        </tr>
      </tbody>
    </table>
	</div>
	</div>
	 </div>

	 <script type="text/javascript" src="Scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// 2.6 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->

<section class="page" id="sec2.6">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.6</span> <span class="sec_title">Biases and Stereotypes</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="q">What are some biases and stereotypes that can affect a test&#39;s fairness?</span></h2>
  </div>
  <div class="section-lead">
   <p class="intro">Making accommodations for language differences are not especially difficult. But overcoming the many biases and stereotypes that can affect the fairness of assessments often is.</p>
   </div>
  <div class="section-body">
   <div class="single-column">
 
	
	<p class="txt"><strong>Biases</strong> are preconceived, personal judgments usually in favor of or against a person, thing, or idea. <strong>Stereotypes</strong> are widely shared ideas that attribute certain characteristics to all members of a group.</p>

    <p class="txt">For example, Natalie might think that a regular old cup of coffee is far superior to an expensive cappuccino. That is a harmless bias. And like most biases, it is a personal tendency. But if North Americans tend to believe that all Laplanders are such and such, and most Roma are this and that (<em>such and such</em> and <em>this and that</em>, of course, being negative), then they hold stereotypes that may be highly detrimental.</p>
	</div>

	<a id="p64"></a>
	<div class="inner-section">
	<h2 class="h2">Gender Biases</h2>
	<div class="single-column">
	<p class="txt">Historically, gender stereotypes exist about male&#8211;female differences, the consequences of which can be unfair to both genders. Some of these stereotypes are based on long-held beliefs rooted in culture and tradition and propagated through centuries of recorded &quot;expert&quot; opinion. Others are based on controversial and often contested scientific findings.</p>
        
	<p class="txt">While males and females have some biologically linked sex differences&mdash;mainly in terms of physical skills that require strength, speed, and stamina&mdash;it is not so clear whether there are gender-linked psychological differences. Early research on male&#8211;female differences (Maccoby &#38; Jacklin, 1974) reported significant differences in four areas: verbal ability, favoring females; mathematical ability, favoring males; spatial&#8211;visual ability (evident, for example, in navigation and orientation skills) favoring males; and aggression (higher in males). However, there is increasing evidence that when early experiences are similar, differences are minimal or nonexistent (Schmitt, 2015; Yarbrough, Cannon, Bergman, Kidder-Ashley, &#38; McCane-Bowling, 2017).</p>

	<p class="txt">However, experiences are not always similar; nor are opportunities and expectations. Given this, the results of many assessments still reveal gender differences. These often favor males in mathematics and females in language arts, but differences tend to be very small (e.g., Ganley &#38; Lubienski, 2016). There is also evidence that the stereotypes many people still hold&mdash;regarding, say, girls&#39; inferiority in mathematics&mdash;might unfairly affect girls&#39; opportunities and their outcomes.</p>
	</div>
	</div>
	<h2 class="h2">Cultural Biases</h2>
	<div class="single-column">
	<p class="txt">As discussed in Chapter 1, considerable research has indicated that some tests can be markedly unfair to cultural groups other than the dominant majority. This is especially true of standardized tests, even when they are normed with very large national samples. As Shuttleworth-Edwards (2016) points out, the fact that a test is representative of a large multicultural population does not mean that it is a fair and valid test for the cultural minorities within that population.</p>

	<p class="txt">In the United States numerous studies illustrate instances of test bias against African Americans and other persons of color. For example, Petchauer, Bowe, and Wilson (2018) summarize studies that show racial bias in teacher-competency exams. They suggest that the use of standardized teacher competency exams&mdash;specifically the edTPA&mdash;might unfairly discriminate against African American teachers.</p>

	<p class="txt">For example, biased questions might feature obscure topics that are clearly of greater concern to certain social and ethnic groups than to others. Even more blatant historical examples of test bias are the oral exams that were sometimes used to make hiring decisions and were designed to weed out people who spoke with certain accents (Quillian, Pager, Hexel, &#38; Arnfinn, 2017).</p>

	<a id="p65"></a>
	<p class="txt">Other research points to a significant achievement gap between African American and White students in American schools. Scott, Gage, Hirn, and Han (2018) suggest that some of this disparity in achievement may be attributed to an implicit and unconscious bias that affects how teachers interact with learners from different cultural backgrounds. For example, there is a tendency for White math teachers to underestimate the abilities of their African American and Hispanic or Latino students even when their math scores are not significantly different from those of their White students (Cherng, 2017).</p>

    <p class="txt">In addition, when compared to White students, African American and Hispanic or Latino students are more likely to be referred for special-needs assessment and less likely to be referred for gifted and talented programs (Warikoo, Sinclair, Fei, &#38; Jacoby-Senghor, 2016). What&#39;s more, African American students are more than twice as likely as White students to be held back in a grade at least once before 12th grade (see Figure 11.11, Chapter 11). Thus, at least in some cases, standardized tests display apparent racial biases, as do some teachers in their informal and perhaps unconscious day-to-day assessments of their students. (See <em>In the Classroom: Culturally Unfair Assessments</em>.)</p>

	<p class="txt"><a class="trigger-readmore" href="#tip2-2" data-type="tipTrigger">In the Classroom: Culturally Unfair Assessments</a></p>
    </div>
    <section class="tipBox" id="tip2-2">
      <div class="box-6">
        <h3 class="h3">In the Classroom: Culturally Unfair Assessments</h3>
        <p class="txt">Joseph Chaganache knew all the legends his grandfather and the other elders told&mdash;even those he had heard only once. His favorites were the legend of the Warriors of the Rainbow and the legend of Kuikuhâchâu, the man who took the form of the wolverine. These legends are long, complicated stories, but Joseph never forgot a single detail, never confused one with the other. The elders named him &#244;h&#244;, which is the word for owl, the wise one. They knew that Joseph was extraordinarily gifted.</p>
		
        <p class="txt">But in school, it seemed that Joseph was unremarkable. He read and wrote well, and he performed better than many. Yet no one even bothered to give him the tests that singled out students who are gifted and talented. Those individuals are often identified through a combination of methods, beginning with teacher nominations that then lead to further testing and perhaps interviews and auditions (Pfeiffer, 2015). Those who don&#39;t do as well in school, sometimes because of cultural or language differences, tend to be overlooked.</p>

		<p class="txt">Joseph is not alone. Aboriginal and other culturally different children are vastly underrepresented among the gifted and the talented (Stargardter, 2016). By the same token, they tend to be overrepresented among programs for those with learning disabilities and emotional disorders (Gordon, 2017). How different do you suppose Joseph Chaganache&#39;s life might have been had his gifts been recognized when he was still a child?</p>
		</div>
	</section>

	<a id="p66"></a>
	<div class="inner-section">
	<h2 class="h2">Inconsistent Grading</h2>
	<div class="single-column">
	<p class="txt">As we will see in Chapter 11, approaches to grading can vary enormously in different schools and even in different classrooms within the same school. They might involve an enormous range of practices, including</p>

	<ul class="bl">
          <li class="li_text">giving or deducting marks for behavior;</li>
          <li class="li_text">giving or deducting marks for class participation;</li>
          <li class="li_text">giving or deducting marks for punctuality;</li>
          <li class="li_text">using well-defined rubrics for grading;</li>
          <li class="li_text">basing grades solely on test results;</li>
          <li class="li_text">giving zeros for missed assignments;</li>
          <li class="li_text">ignoring missed assignments;</li>
          <li class="li_text">using grades as a form of reward or punishment;</li>
          <li class="li_text">grading on any of a variety of letter, number, percentage, verbal descriptor, or other systems;</li>
          <li class="li_text">allowing students to disregard their lowest grade;</li>
          <li class="li_text">and on and on...</li>
    </ul>

	<p class="txt">No matter what practices are used in a given school, for assessments to be fair, grades need to be arrived at in a predictable and transparent manner. Moreover, the rules and practices that underlie their calculation need to be consistent. This approach is also critical for describing what students know and are able to do. If a math grade is confounded with behavioral objectives such as participation, how will the student and parents know what the student&#39;s math skills truly are?</p>

    <p class="txt">Inconsistent grading practices are sometimes evident in disparities within schools, where different teachers grade their students using very different rules. In one class, for example, students might be assured of receiving relatively high grades if they dutifully complete all their assignments as required. But in another class, grades might depend entirely on test results. And in yet another, grades might be strongly influenced by class participation or by spelling and grammar.</p>

	<p class="txt">Inconsistent grading within a class can also present serious problems of fairness for students. A social studies teacher should not ignore grammatical and spelling errors on a short-answer test one week and deduct handfuls of marks for the same sorts of errors the following week. Simply put, the criteria that govern grading should be clearly understood by both the teacher and students, and those criteria should be followed consistently.</p>
  </div> 
  </div>
  </div>
 <script type="text/javascript" src="Scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// 2.7 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->

<section class="page" id="sec2.7">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.7</span> <span class="sec_title">Cheating and Test Fairness</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">How has high-stakes testing led to cheating among both students and educators?</span></h2>
  </div>
  <div class="section-lead">
  <p class="intro">Cheating is not uncommon in schools, especially in higher grades and in postsecondary programs where the stakes are so much higher. Today there are far more opportunities for cheating than there were for previous generations.</p>
  </div>
  <div class="section-body">
    <div class="single-column">
      
	  <a id="p67"></a>		
      <p class="txt">Wireless electronic communication; instant transmission of images, videos, and messages; and wide-scale access to Internet resources have made it easier than ever to cheat. There are now websites that make it very simple and inexpensive to buy term papers online or copy solutions for a huge variety of homework problems. In turn, there are websites for educators that make it easier to detect plagiarism (for example, the Turnitin site, at <a class="link" target="_blank" href="https://www.turnitin.com/">https://www.turnitin.com</a>).</p>

	  <p class="txt">Most people, even those who engage in it, believe that <strong>cheating</strong> is immoral. Sometimes it is even illegal&mdash;such as when people cheat on their income tax returns. And clearly, cheating is unfair.</p>

	  <p class="key-point">If cheating results in a higher grade than is warranted, the grade does not represent the student&#39;s progress or accomplishments. What&#39;s more, those who cheat, by that very act, often cheat other students.</p>

      <p class="txt">When Elvira succeeded in sneaking through the window into Professor Clark&#39;s office and copying the midterm exam the class was about to take, she assured herself of an enviable grade. That she also wrote out the correct answers for most of the questions and then sold them to a bunch of her classmates also ensured that they would get high grades. And because Professor Clark scaled the scores so that the class average would be neither distressingly low nor alarmingly high, many of her classmates were cheated out of the grades they would otherwise have received. Carrying this example to its logical extreme, it is possible that some students might be deprived of awards and scholarships, or even of a passing grade, because others have cheated.</p>
	  </div>
	<div class="inner-section">
	<h2 class="h2">Campbell&#39;s Law and the Cobra Effect</h2>
	<div class="single-column">
	<p class="txt">Increased cheating as a result of high-stakes testing is an illustration of <strong>Campbell&#39;s law</strong>. Developed by psychologist Donald Campbell (1979), this law states that &quot;the more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption and the more apt it will be to distort and corrupt the social processes it is intended to monitor&quot; (p. 85).</p>

	<p class="key-point">In other words, when the results of educational tests are overwhelmingly important, they can distort the goals of the teaching process, directing efforts toward doing well on high-stakes tests rather than toward reaching educational standards.</p>

	<p class="txt">As a result, both learners and educators may be encouraged to cheat.</p>
	
	<a id="p68"></a>
	<div class="text_container">
      <h4><button class="ec_expand" aria-expanded="false" aria-controls="ecbox2-3">LEARN MORE</button></h4>
      <div id="ecbox2-3">

        <p class="txt">As Jones and Ennes (2018) point out, some school systems cheat on high-stakes tests by excluding certain students who are not expected to do well; others cheat by not adhering to guidelines for administering the tests, perhaps by giving students more time or even by giving them hints and answers.</p>
		
        <p class="txt">A more subtle form of administrative and teacher cheating on high-stakes tests takes the form of &quot;narrowing&quot; the curriculum. In effect, instructional objectives are narrowed to topics covered by the tests, and instruction is focused specifically on those targets to the exclusion of all others. Once again illustrating Campbell&#39;s law, in &quot;narrowed&quot; curricula, the social indicator defined by results on high-stakes tests loses its original meaning and value.</p>
		
        <p class="txt">Campbell&#39;s law is closely related to what is known as the <strong>cobra effect</strong>&mdash;an unintended consequence, such as when the solution for a problem simply makes it worse. The use of the term <em>cobra effect</em> to mean <em>unintended consequence</em> is said to have originated when the British, who were then rulers of India, began to pay a bounty for cobras in an attempt to control the large number of snakes in the country. The program was highly successful, and the snake population plummeted.</p>

		<p class="txt">However, when wild cobras eventually became scarce, a large number of entrepreneurs began to raise their own in captivity so that they could then turn them in for the bounty. When the British became aware of this, they immediately discontinued the bounty program. So the entrepreneurs turned their now worthless cobras loose, and the population of wild cobras skyrocketed. In the end, a highly effective solution had only made the problem worse.</p>
		
        <p class="key-point">Continuing the cobra effect analogy, testing programs intended to increase student motivation and achievement, improve teacher performance, and make schools accountable for student performance may sometimes have the opposite effect.</p>
		
        <p class="txt">In some cases these testing programs have become so important (or <em>high stakes</em>) that they have led not to improvements in educational offerings but rather to various forms of cheating&mdash;not only by students but at the highest levels of educational administration. For example, state investigators in Georgia found that 178 administrators and teachers in 44 Atlanta schools who had early access to standardized test results systematically cheated, changing answers to improve student scores (Schachter, 2011). In the end, the attempted solution might in some cases have made the problem worse.</p>
      <button class="ec_collapse" type="button" name="ecbox2-3"><span>X</span> close</button>
	  </div>
	</div>
	</div>
	 </div>
    <h2 class="h2">Preventing Cheating Among Students</h2>
	<div class="single-column">
	<p class="txt">Among the various suggestions for preventing or reducing cheating on exams are the following.</p>

	<ul class="bl">
          <li class="li_text">Encourage students to value honesty.</li>
          <li class="li_text">Be aware of school policy regarding the consequences of cheating, and communicate them to students.</li>
          <li class="li_text">Clarify for students exactly what cheating is.</li>
          <li class="li_text">When possible, use more than one form of an exam so that no two adjacent students have the same form.</li>
          <li class="li_text">Stagger seats so that seeing other students&#39; work is unlikely.</li>
          <li class="li_text">Randomize and assign seating for exams.</li>
          <li class="li_text">Guard the security of exams and answer sheets.</li>
          <li class="li_text">Monitor exams carefully.</li>
          <li class="li_text">Prohibit talking or other forms of communication during exams.</li>
          <li class="li_text">Use technology, such as plagiarism detection websites, to uncover cheating.</li>
    </ul>

	<p class="txt">Of course, these tactics cannot guarantee that students won&#39;t cheat. In fact, one large-scale study found that 21&#37; of 40,000 undergraduate students surveyed had cheated on tests, and an astonishing 51&#37; had cheated at least once on their written work (McCabe, 2005; Figure 2.3).</p>
    
	<a id="p69"></a>
	<p class="txt">Other studies have found even higher rates of cheating depending on how it is defined. For example, Barnhardt (2016) reports that studies examining the occurrence of &quot;cheating in any form&quot; often report that over 90&#37; of American high school students admit to having cheated at least once. He clarifies, however, that many of the &quot;cheating&quot; behaviors are trivial (for example, helping another student complete an assignment) and hardly immoral.</p>

	<p class="txt">Regardless, the prevalence of cheating does not justify it. Nor does it do anything to increase the fairness of testing practices.</p>
	</div>

      <figure class="figure-1">
	          <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 2.3</span><br />
          <span class="figure_title">Cheating among college undergraduates</span> </h4>
        <p class="caption">Estimates of the frequency of cheating in schools vary enormously, depending on how cheating is defined.</p>
		<p class="src">Based on &quot;It Takes a Village: Academic Dishonesty,&quot; by D. McCabe, 2005, <span class="no-italics">Liberal Education</span> Summer/Fall (http://www.middlebury.edu/media/view/257515/original/It_takes_a_village.pdf).</p>
		  </figcaption>

            <a class="trigger-readmore_for_Figure" href="#fig2-3" data-type="tipTrigger">
	        <img src="figures/Figure_2.3.jpg" alt="Bar graph showing the percentage of undergraduate students who have admitted to cheating at least once. Of 40,000 students who admitted to cheating, around 21&#37; admitted to cheating on tests and around 51&#37; admitted to cheating on written work" title="Bar graph showing the percentage of undergraduate students who have admitted to cheating at least once. Of 40,000 students who admitted to cheating, around 21&#37; admitted to cheating on tests and around 51&#37; admitted to cheating on written work" id="fig_2.3"/>
			</a>
	  </figure>

    <section class="tipBox" id="fig2-3">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_2.3.jpg" alt="Bar graph showing the percentage of undergraduate students who have admitted to cheating at least once. Of 40,000 students who admitted to cheating, around 21&#37; admitted to cheating on tests and around 51&#37; admitted to cheating on written work" title="Bar graph showing the percentage of undergraduate students who have admitted to cheating at least once. Of 40,000 students who admitted to cheating, around 21&#37; admitted to cheating on tests and around 51&#37; admitted to cheating on written work" id="fig_2.3"/>
 	</div>
   </section> 


	<div class="single-column">
	<p class="txt">The main characteristics of fair assessment practices are as follows</p>
	<ul class="bl">
          <li class="li_text">Cover material that every student has had an opportunity to learn.</li>
          <li class="li_text">Reflect learning targets for that course.</li>
          <li class="li_text">Allow sufficient time for students to finish the test.</li>
          <li class="li_text">Discourage cheating.</li>
          <li class="li_text">Provide accommodations for learners with special needs.</li>
          <li class="li_text">Guard against influence of biases and stereotypes.</li>
          <li class="li_text">Avoid misleading questions.</li>
          <li class="li_text">Follow consistent and clearly understood grading practices.</li>
          <li class="li_text">Base important decisions on a variety of different assessments.</li>
          <li class="li_text">Take steps to ensure the validity and reliability of assessments.</li>
    </ul>

	<p class="txt">Related to this, Table 2.10 presents the APA&#39;s Code of Fair Testing Practices in Education. Because of its importance, the code is reprinted in its entirety at the end of this chapter.</p>
    </div>
	</div>
    <script type="text/javascript" src="Scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// 1.8 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->
<a id="p70"></a>
<section class="page" id="sec2.8">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.8</span> <span class="sec_title">Validity</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Questions</span> <span class="focus_q">What are some different types of validity? Why are they important?</span></h2>
  </div>
  <div class="section-lead">
   <p class="intro">In addition to the fair assessment practices listed previously and in Table 2.4, the fairness of a test or assessment system depends on the <em>reliability</em> of the test instruments (discussed later in this chapter) and on the <em>validity</em> of inferences and uses based on test results.</p>
   </div>
  <div class="section-body">
   <div class="single-column">
     

      <p class="key-point">In order to have <strong>validity</strong>, a test has to be demonstrably appropriate for the interpretations of and uses for its results. This means it must measure what it claims to measure, <em>and</em> it must be used for its intended purposes.</p>

      <p class="txt">Thus, if a high schooler&#39;s ACT scores were used to decide whether the student should have a driver&#39;s license, then that college readiness test could not easily be shown to be valid. However, when the same scores are used as a basis for college admission decisions, their interpretation and use is valid. The test is designed to predict college performance rather than readiness to drive.</p>

      <p class="txt">&quot;Interpretations and uses that make sense and are supported by appropriate evidence are considered to be valid,&quot; explains Kane (2016), &quot;and interpretations or uses that are not adequately supported by evidence are not considered to be valid&quot; (p. 202). In the example cited above, &quot;appropriate evidence&quot; of the validity of a driver&#39;s test relates to whether the person who passes it can safely operate a motor vehicle. Similarly, appropriate evidence of the validity of the ACT is found in the subsequent college performance of those who pass (or fail) the test.</p>

	  <p class="key-point">Validity is less a property of tests themselves than of the uses to which tests results are put. It relates to the inferences we make based on test results and the consequences that follow.</p>

      <p class="txt">In effect, interpreting test scores amounts to making an inference about some quality or characteristic of the test taker or making a prediction about how he or she will perform in the future.</p>

      <p class="txt">For example, based on Nora&#39;s brilliant performance on a mathematics test, her teacher infers that Nora has commendable math skills and understanding. One consequence of this inference might be that Nora is invited to join the lunchtime mathematics enrichment group. But note that the inference and the consequence are appropriate and defensible only if the test on which Nora performed so admirably actually measures relevant mathematical skills and understanding.</p>

	  <p class="txt">From a measurement point of view, validity is the most important characteristic of a measuring instrument.</p>

      <p class="key-point">If a test does not measure what it is intended to measure, and if it is not interpreted and used as intended, its scores will be invalid no matter how consistent and predictable they are.</p>

      <p class="txt">The important point is that in educational assessment, validity is best defined in terms of the way test results are interpreted and used. Accordingly, a test may be valid for some purposes but totally invalid for others.</p>
	  </div>

	  <a id="p71"></a>
	<div class="inner-section">
	  <h2 class="h2">Face Validity</h2>
	  <div class="single-column">
      <p class="txt">How can you, as a teacher, determine whether a test is valid? In addition, how can you make sure that your interpretations and uses of test results are valid? There are a number of ways to do so. One of the most obvious is to look at the items that make up the test. Does the mathematics test <em>look</em> like it measures mathematics? Does the grammar test <em>appear to be</em> a grammar test? Answers to these sorts of questions determine the <em>face validity</em> of the test. Basically, <strong>face validity</strong> is the extent to which the test appears to measure what it is supposed to measure. If the mathematics test consists of appropriate mathematical problems, it has face validity.</p>

	  <p class="key-point">Face validity is especially important for teacher-made tests.</p>

      <p class="txt">Just by looking at a test, students should immediately know that they are being tested on the right things. A mathematics test that has face validity will not ask a series of questions about characters and events in Shakespeare&#39;s <em>Julius Caesar</em>.</p>

      <p class="txt">Occasionally, however, test makers are careful to avoid any hint of face validity. For example, a psychologist constructing a test designed to measure a personality characteristic such as honesty would not want the test participants to know what is being measured. If the instrument had face validity&mdash;that is, if it looked like it was measuring honesty&mdash;the scoundrels who take the test might actually lie and act as if they are honest when they really aren&#39;t. Better to deceive them, lie to them, and pretend to be testing motivational qualities or character strength, to determine what liars and rogues they really are.</p>
	  </div>
	</div>
	  <h2 class="h2">Content Validity</h2>
	  <div class="single-column">
	  <p class="txt">Of course, a test must not only appear to measure what it is intended to measure; it should actually do so. That is, its content should reflect the instructional objectives it is designed to assess. This indicator of validity, termed <strong>content validity</strong>, is assessed by analyzing the content of test items in relation to the objectives of the course, unit, or lesson.</p>

	  <h3 class="h3">Determining Content Validity</h3>
	  <p class="key-point">Content validity is important for measuring school achievement.</p>

      <p class="txt">A test with high content validity includes items that sample all important course objectives in proportion to their importance. Thus, if some of a unit&#39;s instructional objectives have to do with the development of cognitive processes, a relevant test will have content validity to the extent that it samples these processes. And if 40&#37; of the course content, and consequently the course objectives, deal with knowledge (rather than with comprehension or analysis, for example), 40&#37; of the test items should assess knowledge.</p>

      <p class="txt">Determining a test&#39;s content validity is largely a matter of carefully and logically analyzing its items. Basically, the test needs to include items that tap knowledge and skills that define course objectives.</p>
	  
	  <a id="p72"></a>
	  <h3 class="h3">Increasing Content Validity</h3>
	  <p class="txt">As Wilson, Pan, and Schumsky (2012) explain, test makers should follow the following steps to ensure content validity.</p>

	  <ol class="nl">
	  <li class="li_text">Define the content (the instructional objectives).</li>
      <li class="li_text">Define the level of difficulty or abstraction for the items.</li>
      <li class="li_text">Develop a pool of representative items.</li>
      <li class="li_text">Determine what ratio of different items best represents the instructional objectives.</li>
      <li class="li_text">Develop a test blueprint.</li>
	  </ol>
	  </div>

	  <figure class="photo-right"> <img src="images/2.2.jpg" alt="Young boy writes with a pencil on a piece of paper." title="Young boy writes with a pencil on a piece of paper." id="img2.2"/>
        <figcaption>
          <p class="cr">Milatas/iStock/Getty Images Plus</p>
          <p class="caption">One measure of validity is reflected in the extent to which the predictions we base on test results are borne out. If this boy does exceptionally well on this standardized battery of tests, will he also do well next year in fifth grade? In high school? In college?</p>
        </figcaption>
      </figure>
	  
	  <div class="single-column">
      <p class="txt">One of the main advantages of preparing a test blueprint (also referred to as a table of specifications) is that it ensures a relatively high degree of content validity (providing, of course, that the test maker follows the blueprint).</p>

      <p class="key-point">It is important to realize that tests and test items do not possess validity as a sort of intrinsic quality; a test is not valid or invalid in and of itself.</p>

	  <p class="txt">Rather, it is valid for certain purposes and with certain individuals, and it is invalid for others. For example, if the following item, which measures recall, is intended to measure comprehension, it does not have content validity.</p>

	  <p class="ext"><em>How many different kinds of validity are discussed in this chapter?</em><br/>
		<u>&nbsp;&ensp;</u> A. 1<br/>
		<u>&nbsp;&ensp;</u> B. 2<br/>
		<u>&nbsp;&ensp;</u> C. 3<br/>
		<u>&#10003;</u> D. 5<br/>
		<u>&nbsp;&ensp;</u> E. 10</p>

      <p class="txt">If, on the other hand, the item is intended to measure knowledge recall, it would have content validity. And an item such as the following might have content validity with respect to measuring comprehension.</p>

      <p class="ext"><em>Explain why face validity is important for teacher-constructed tests.</em></p>

	  <p class="txt">Note, however, that this last item measures comprehension only if students are required to demonstrate that they truly understand the information. If the teacher has given them the exact answer to this question in class, then this test item requires nothing more than knowledge recall. What an item measures is not inherent in the item itself but in the relationship between the material as the students have learned it and what the item requires.</p>
	  </div>

	  <a id="p73"></a>
	<div class="inner-section">
	  <h2 class="h2">Construct Validity</h2>
	  <div class="single-column">
      <p class="txt">A third approach to determining validity involves what is termed <strong>construct validity</strong>. It is somewhat less relevant for teacher-constructed tests but is highly relevant for many other psychological measures (e.g., personality and intelligence tests).</p>

	  <p class="txt">In essence, a construct is a <strong>hypothetical variable</strong>&mdash;an unobservable characteristic or quality, often inferred from theory. For example, a theory might argue that individuals who are highly intelligent should be reflective rather than impulsive. Reflectivity would be evident in the care and caution they use to solve problems or make decisions. Impulsivity would be apparent in a person&#39;s hastiness and failure to consider all aspects of a situation. One way to determine the construct validity of a test designed to measure intelligence, then, would be to look at how well it correlates with measures of reflection and impulsivity (see Chapter 9 for a discussion of correlation&mdash;a mathematical index of relationships).</p>
	  </div>
	</div>
	  <h2 class="h2">Criterion-Related Validity</h2>
	  <div class="single-column">
      <p class="txt">If Matthew does exceptionally well on all his 12th-grade year-end tests, his teachers might be justified in using his results to predict that he will do well in college. Colleges that subsequently admit Matthew into one of their programs because of his grade-12 marks are also making the same prediction. The validity of Matthew&#39;s 12th-grade tests is reflected in the extent to which they meet certain criteria&mdash;hence the expression <strong>criterion-related validity</strong>, of which there are two aspects illustrated by Matthew&#39;s case. One is <em>predictive validity</em>, the validity of predictions based on test scores; the other is <em>concurrent validity</em>, a measure of how well test scores agree with other similar measures that could be used for the same purpose. Both these aspects of criterion-related validity are explained further below.</p>

	  <h3 class="h3">Predictive Validity</h3>
      <p class="txt">At all levels, prediction is one of the main uses of summative (rather than formative) assessments. We assume that all students who do well on year-end fifth-grade achievement tests will do reasonably well in sixth grade. We also predict that those who perform poorly on these tests will not do as well in sixth grade, and we might use this prediction to justify requiring them to undertake remedial work.</p>

	  <p class="txt">The extent to which our predictions are accurate reflects <strong>predictive validity</strong>. Predictive validity is easily measured by looking at the relationship between actual performance on a test and subsequent performance. Thus, a college entrance examination designed to identify students whose chances of college success are high has predictive validity to the extent that its predictions are borne out. Thus, our interpretation and uses of test scores are supported by evidence&mdash;a situation that defines validity.</p>
	  
	  <a id="p74"></a>
	  <h3 class="h3">Concurrent Validity</h3>
      <p class="txt"><strong>Concurrent validity</strong>, the second aspect of criterion-related validity, is the relationship between a given test and other measures of the same behaviors or characteristics.</p>

      <p class="txt">For example, as we will see in Chapter 10, the most accurate way to measure intelligence is to administer a time-consuming and expensive <strong>individual test</strong>. A second option is to administer a quick, inexpensive <strong>group test</strong>; a third, far less consistent approach is to have teachers informally assess intelligence based on what they know of their students&#39; achievements and effort. Teachers&#39; assessments are said to have concurrent validity to the extent that they yield assessments comparable to the more formal measures. In the same way, a group or an individual test is said to have concurrent validity if it produces results similar to those obtained using a different and presumably valid test.</p>

	  <p class="txt">Figure 2.4 summarizes the various approaches to determining test validity.</p>
	  </div>

      <figure class="figure-1">
	          <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 2.4</span><br />
          <span class="figure_title">Types of test validity</span> </h4>
        <p class="caption">The validity of a test reflects how defensible its uses and interpretations are. Validity is evident in the test&#39;s appearance, content, theoretical underpinning, and usefulness for making predictions.</p>
		<p class="src"></p>
		  </figcaption>

            <a class="trigger-readmore_for_Figure" href="#fig2-4" data-type="tipTrigger">
	        <img src="figures/Figure_2.4.jpg" alt="Diagram displaying the four main types of test validity: face, content, construct, and criterion-related. There are then two types of criterion-related validity: predictive and concurrent. " title="Diagram displaying the four main types of test validity: face, content, construct, and criterion-related. There are then two types of criterion-related validity: predictive and concurrent. " id="fig_2.4"/>
			</a>
	  </figure>

    <section class="tipBox" id="fig2-4">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_2.4.jpg" alt="Diagram displaying the four main types of test validity: face, content, construct, and criterion-related. There are then two types of criterion-related validity: predictive and concurrent. " title="Diagram displaying the four main types of test validity: face, content, construct, and criterion-related. There are then two types of criterion-related validity: predictive and concurrent. " id="fig_2.4"/>
 	</div>
   </section> 



  </div> 
<script type="text/javascript" src="Scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// 2.9 ///////////////////////////////
/////////////////////////////////////////////////////////////
-->

<section class="page" id="sec2.9">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.9</span> <span class="sec_title">Types of Reliability</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Questions</span> <span class="focus_q">What are some types of reliability? How can you determine the reliability of a test?</span></h2>
  </div>
  <div class="section-lead">
  <p class="intro">Reliability is what we want in our cars, computers, and airplanes, not to mention our bathroom scales. We want our cars, computers, and airplanes to start when we perform the appropriate actions, and we want them to function as they were designed to function.</p>
  </div>
  <div class="section-body">
    <div class="single-column">
    
    <a id="p75"></a>
	<p class="txt">Reliability is predicated on predictability and consistency. If Jay steps on his bathroom scale five times in a row, he expects it to display the same weight each time. Jay&#39;s scale is predictable and consistent. It is reliable.</p>
   
	<p class="txt"><strong>Reliability</strong> in educational measurement is no different. Basically, it relates to consistency.</p>
     
    <p class="key-point">Good measuring instruments must not only measure what they are intended to measure (they must have validity), they must also provide consistent, dependable, and reliable measures.</p>

	<p class="txt">In testing, reliability has to do with the accuracy of our measurements. The more errors in our measurements, the less reliable our test results. A reliable intelligence test, for example, should yield similar results from one week to the next, or even from one year to the next.</p>

	<p class="txt">However, the reliability of most of our educational and psychological measures is never perfect. If you give Chiara an intelligence test this week and another in 2 weeks, it is highly unlikely that her scores will be identical. This is acceptable as long as the difference between the two scores is not too great. After all, many factors can account for this <strong>error of measurement</strong>. Say Chiara scored 123 the first week but only 102 the second. The difference between the two scores might be because Chiara had a headache at the time of the second test. Or perhaps she was distracted by personal problems, tired from a long trip, anxious about the test, or confused by some new instructions.</p>

	<p class="txt">In psychology and education, we tend to assume that the things we measure are relatively stable. Yet much of what we measure is variable. So at least some of the error in our measurements is likely due to change in what we measure. But if two similar measures of achievement in chemistry yield a score of 82&#37; one week but only 53&#37; the next week for the same student, then the test we are using may well have a reliability problem. How can we assess the reliability of our tests? (See Table 2.5.)</p>
	</div>
<div class="tbl_scroll_on_mobile">
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_number">Table 2.5</span><br />
      <span class="tbl_title">Validity and reliability</span>
      </caption>
      <thead class="tbl_header">
        <tr>
          <th>Validity</th>
          <th>Reliability</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><ul class="bl">
		  <li class="li_text">Validity is not an intrinsic, unchanging characteristic of a test but is defined by the extent to which interpretation and use of test results are supported by evidence.</li>
		  <li class="li_text">A test can be highly valid when its results are interpreted and used for one purpose; the same test might be totally invalid when interpreted and used for a different purpose.</li>
		  </ul></td>

          <td><ul class="bl">
		  <li class="li_text">Reliability is defined by the extent to which test results are consistent and stable.</li>
		  <li class="li_text">Highly reliable tests tend to produce similar results for the same individuals when measuring stable characteristics at different points in time.</li>
		  </ul></td>
        </tr>
      </tbody>
    </table>
	</div>
	<div class="inner-section">
	<h2 class="h2">Test&#8211;Retest Reliability</h2>
	<div class="single-column">

	<p class="txt">If a test measures what it purports to measure (that is, if it is valid), and if what it measures does not fluctuate unpredictably, no matter how often it is given, the test should yield similar scores. If it does not, it is not only unreliable but probably invalid as well. In fact, a test cannot be valid without being reliable. If it yields inconsistent scores for a stable characteristic, we can hardly insist that it is measuring what it is supposed to measure and that our uses of test results will be defensible.</p>
	
	<a id="p76"></a>
	<p class="key-point">A test should yield similar scores from one testing to the next&mdash;unless, of course, the test is simple enough that the student learns and remembers appropriate responses.</p>

	<p class="txt">That is the basis for one of the most common measures of reliability. Giving the same test two or more times and comparing the results obtained at each testing yields a measure of what is known as <strong>test&#8211;retest reliability</strong> (sometimes also called <em>repeated-measures reliability</em> or <em>stability reliability</em>).</p>

	<p class="txt">Say, for example, that Mr. Ranjan gives a group of first-grade students a standardized language proficiency test at the end of October and then gives them the same test again at the end of November. If the results are those shown in columns 2 and 3 of Table 2.6 (&quot;October test results&quot; and &quot;Hypothetical November test results&quot;), we can immediately see that the test yields consistent, stable scores and is therefore highly reliable. Students who scored high in October continue to score high in November&mdash;as we would expect, given our assumption that language proficiency should not change dramatically in the span of a month.</p>
	</div>
   <div class="tbl_scroll_on_mobile">
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_number">Table 2.6</span><br />
      <span class="tbl_title">Test&#8211;retest reliability</span>
      </caption>
      <thead class="tbl_header">
        <tr>
          <th>Student</th>
          <th>October test results</th>
		  <th>Hypothetical November test results</th>
          <th>Alternate November test results</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>A</td> 
		  <td>72</td>
		  <td>75</td> 
		  <td>55</td>
        </tr>

		<tr>
          <td>B</td> 
		  <td>84</td>
		  <td>83</td> 
		  <td>64</td>
        </tr>

		<tr>
          <td>C</td> 
		  <td>56</td>
		  <td>57</td> 
		  <td>22</td>
        </tr>

		<tr>
          <td>D</td> 
		  <td>79</td>
		  <td>82</td> 
		  <td>69</td>
        </tr>

		<tr>
          <td>E</td> 
		  <td>55</td>
		  <td>57</td> 
		  <td>93</td>
        </tr>

		<tr>
          <td>F</td> 
		  <td>84</td>
		  <td>79</td> 
		  <td>49</td>
        </tr>

		<tr>
          <td>G</td> 
		  <td>91</td>
		  <td>88</td> 
		  <td>57</td>
        </tr>
      </tbody>
    </table>
	</div>

	<div class="single-column">

	<p class="txt">If, however, the results are those shown in columns 2 and 4 (&quot;October test results&quot; and &quot;Alternate November test results&quot;), we would have legitimate questions about the reliability of this language proficiency test (unless there is some other logical explanation). Now some of the students who scored high in October do very poorly in November, and others who did poorly in October do exceptionally well in November.</p>

	<p class="txt">Statistically, the reliability of this test would be obtained by looking at the <strong>correlation</strong> between scores obtained on the test and those obtained on the retest (see Chapter 9 for an explanation of correlation). The first chart in Figure 2.5 shows how the hypothetical November results closely parallel the October results. In fact, there is a high positive correlation (+.98) between these results. If a test is reliable, we would expect not only that the correlations between repeated measurements would be high but also that the spreads around the average would be similar on the two tests. (Variation from the average is usually measured as the <em>standard deviation</em>, explained in Chapter 9.)</p>

	<p class="txt">The second chart in Figure 2.5 shows how the alternate November results do not parallel the October results. In fact, the correlation between the two is zero (.002), indicating that the relationship between them is essentially random.</p>
	</div>
	
      <a id="p77"></a>
		<figure class="figure-1">
	          <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 2.5</span><br />
          <span class="figure_title">Test&#8211;retest reliability</span> </h4>
        <p class="caption">If a test is reliable, it should yield similar scores when given to the same students at different times. Chart 1 (based on Table 2.6) shows high reliability (correlation +.98); Chart 2 illustrates low reliability (correlation .002: basically a random relationship).</p>
		<p class="src"></p>
		  </figcaption>

            <a class="trigger-readmore_for_Figure" href="#fig2-5" data-type="tipTrigger">
	        <img src="figures/Figure_2.5.jpg" alt="Two line graphs showing the differing appearances of high test&#8211;retest reliability and low test&#8211;retest reliability. Both graphs contain the same axes: the x-axis for individual students and the y-axis for test scores. Both graphs contain the same October test results plotted as a line on the graph. Chart 1 contains a line plotted for hypothetical November test results, which almost perfectly overlaps with the October results line, showing high test reliability. Chart 2 contains a line plotted for alternate November test results, which not only does not overlap with the October results line but is a completely different shape from the October line, showing a random relationship between the two tests." title="Two line graphs showing the differing appearances of high test&#8211;retest reliability and low test&#8211;retest reliability. Both graphs contain the same axes: the x-axis for individual students and the y-axis for test scores. Both graphs contain the same October test results plotted as a line on the graph. Chart 1 contains a line plotted for hypothetical November test results, which almost perfectly overlaps with the October results line, showing high test reliability. Chart 2 contains a line plotted for alternate November test results, which not only does not overlap with the October results line but is a completely different shape from the October line, showing a random relationship between the two tests." id="fig_2.5"/>
			</a>
	  </figure>

    <section class="tipBox" id="fig2-5">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_2.5.jpg" alt="Two line graphs showing the differing appearances of high test&#8211;retest reliability and low test&#8211;retest reliability. Both graphs contain the same axes: the x-axis for individual students and the y-axis for test scores. Both graphs contain the same October test results plotted as a line on the graph. Chart 1 contains a line plotted for hypothetical November test results, which almost perfectly overlaps with the October results line, showing high test reliability. Chart 2 contains a line plotted for alternate November test results, which not only does not overlap with the October results line but is a completely different shape from the October line, showing a random relationship between the two tests." title="Two line graphs showing the differing appearances of high test&#8211;retest reliability and low test&#8211;retest reliability. Both graphs contain the same axes: the x-axis for individual students and the y-axis for test scores. Both graphs contain the same October test results plotted as a line on the graph. Chart 1 contains a line plotted for hypothetical November test results, which almost perfectly overlaps with the October results line, showing high test reliability. Chart 2 contains a line plotted for alternate November test results, which not only does not overlap with the October results line but is a completely different shape from the October line, showing a random relationship between the two tests." id="fig_2.5"/>
 	  </div>
     </section> 


	</div>
	<h2 class="h2">Parallel-Forms Reliability</h2>
	<div class="single-column">

	<p class="txt">Test&#8211;retest measures of reliability look at the correlation between results obtained by giving the same test twice to the same individuals. But in some cases it is not possible or convenient to administer the same test twice. If the test is very simple or contains striking and highly memorable questions (or answers), some learners may improve dramatically from one testing to the next. Or if the teacher goes over the test and discusses possible responses, some students might learn enough to improve, and others might not.</p>

    <p class="key-point">A second approach to estimating test reliability gets around this problem by administering a different form of the test the second time.</p>

    <p class="txt">The different form of the test is designed to be highly similar to the first and is expected to yield similar scores. It is therefore labeled a <em>parallel</em> form. The correlation between these parallel forms of the same test yields a measure of <strong>parallel-forms reliability</strong> (also termed <em>alternate-form reliability</em>). Figure 2.6 plots the scores obtained by seven students on parallel forms of a test. Note how the results follow each other. That is, a student who scores high on form A of the test is also likely to score high on form B. In this case the correlation between the two forms is .86.</p>
	</div>

 <a id="p78"></a>
        <figure class="figure-1">
	          <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 2.6</span><br />
          <span class="figure_title">Parallel-forms reliability</span> </h4>
        <p class="caption">The relationship between scores on two parallel forms of the same test given to the same group is an indication of how dependably and consistently (reliably) the test measures.</p>
		<p class="src"></p>
		  </figcaption>

            <a class="trigger-readmore_for_Figure" href="#fig2-6" data-type="tipTrigger">
	        <img src="figures/Figure_2.6.jpg" alt="Table and graph showing parallel-forms reliability. The table contains test A and test B results for individual students. The results for test A and test B, respectively, are as follows: 62 and 60 (student A), 74 and 83 (student B), 66 and 57 (student C), 34 and 55 (student D), 79 and 86 (student E), 23 and 44 (student F), 91 and 79 (student G). These numbers are plotted on a line graph with an x-axis for individual students and a y-axis for test scores. There are two lines plotted, one for test A results and another for test B results. " title="Table and graph showing parallel-forms reliability. The table contains test A and test B results for individual students. The results for test A and test B, respectively, are as follows: 62 and 60 (student A), 74 and 83 (student B), 66 and 57 (student C), 34 and 55 (student D), 79 and 86 (student E), 23 and 44 (student F), 91 and 79 (student G). These numbers are plotted on a line graph with an x-axis for individual students and a y-axis for test scores. There are two lines plotted, one for test A results and another for test B results. " id="fig_2.6"/>
			</a>
	  </figure>

    <section class="tipBox" id="fig2-6">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_2.6.jpg" alt="Table and graph showing parallel-forms reliability. The table contains test A and test B results for individual students. The results for test A and test B, respectively, are as follows: 62 and 60 (student A), 74 and 83 (student B), 66 and 57 (student C), 34 and 55 (student D), 79 and 86 (student E), 23 and 44 (student F), 91 and 79 (student G). These numbers are plotted on a line graph with an x-axis for individual students and a y-axis for test scores. There are two lines plotted, one for test A results and another for test B results. " title="Table and graph showing parallel-forms reliability. The table contains test A and test B results for individual students. The results for test A and test B, respectively, are as follows: 62 and 60 (student A), 74 and 83 (student B), 66 and 57 (student C), 34 and 55 (student D), 79 and 86 (student E), 23 and 44 (student F), 91 and 79 (student G). These numbers are plotted on a line graph with an x-axis for individual students and a y-axis for test scores. There are two lines plotted, one for test A results and another for test B results. " id="fig_2.6"/>
 	  </div>
     </section> 


	<div class="inner-section">
	<h2 class="h2">Split-Half Reliability</h2>
	<div class="single-column">

	<p class="txt">Teachers seldom go to the trouble of making up two forms of the same test and establishing that they are equivalent. Fortunately, there is a clever way of calculating test reliability. If a teacher prepares a comprehensive test made up of a large number of items, many of them will overlap in what they assess.</p>

    <p class="key-point">It is therefore reasonable to assume that if the teacher were to split the test in two and administer each half to the students, their scores on the two halves would be highly similar.</p>

    <p class="txt">However, it isn&#39;t actually necessary to split the test: The teacher simply needs to give the entire test to all students and then score the test as though it had been split.</p>

	<p class="txt">Suppose, for example, that the original test consisted of 100 multiple-choice items, carefully constructed to reflect all of the instructional objectives. When scoring the test, the teacher might consider the 50 even-numbered items as one test and the other 50 as a separate test. Calculating the correlation between these two halves of the test reveals the <strong>split-half reliability</strong>. Figure 2.7 illustrates split-half reliability based on a 90-item test split into two 45-item halves.</p>

    <p class="key-point">Note that the longer the test, the more accurate the measure of reliability.</p>

    <p class="txt">Table 2.7 summarizes the various ways to assess test reliability.</p>
	</div>
	
	<a id="p79"></a>
        <figure class="figure-1">
	          <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 2.7</span><br />
          <span class="figure_title">Split-half reliability</span> </h4>
        <p class="caption">A single test scored as though it were two separate tests provides information for judging its internal consistency (reliability). In this case, the correlation between the two test halves is .8.</p>
		<p class="src"></p>
		  </figcaption>

            <a class="trigger-readmore_for_Figure" href="#fig2-7" data-type="tipTrigger">
	        <img src="figures/Figure_2.7.jpg" alt="Table and line graph showing split-half reliability. The table lists odd items and even items of each student. The number of odd and even items, respectively, are as follows: 42 and 45 (student A), 33 and 35 (student B), 23 and 40 (student C), 34 and 34 (student D), 36 and 42 (student E), 47 and 48 (student F), 34 and 28 (student G), 44 and 41 (student H), 22 and 25 (student I), 18 and 16 (student J), 33 and 37 (student K). These numbers are plotted on a line graph with an x-axis for individual students and a y-axis for scores on each half. There are two lines plotted, one for odd items and another for even items." title="Table and line graph showing split-half reliability. The table lists odd items and even items of each student. The number of odd and even items, respectively, are as follows: 42 and 45 (student A), 33 and 35 (student B), 23 and 40 (student C), 34 and 34 (student D), 36 and 42 (student E), 47 and 48 (student F), 34 and 28 (student G), 44 and 41 (student H), 22 and 25 (student I), 18 and 16 (student J), 33 and 37 (student K). These numbers are plotted on a line graph with an x-axis for individual students and a y-axis for scores on each half. There are two lines plotted, one for odd items and another for even items." id="fig_2.7"/>
			</a>
	  </figure>

    <section class="tipBox" id="fig2-7">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_2.7.jpg" alt="Table and line graph showing split-half reliability. The table lists odd items and even items of each student. The number of odd and even items, respectively, are as follows: 42 and 45 (student A), 33 and 35 (student B), 23 and 40 (student C), 34 and 34 (student D), 36 and 42 (student E), 47 and 48 (student F), 34 and 28 (student G), 44 and 41 (student H), 22 and 25 (student I), 18 and 16 (student J), 33 and 37 (student K). These numbers are plotted on a line graph with an x-axis for individual students and a y-axis for scores on each half. There are two lines plotted, one for odd items and another for even items." title="Table and line graph showing split-half reliability. The table lists odd items and even items of each student. The number of odd and even items, respectively, are as follows: 42 and 45 (student A), 33 and 35 (student B), 23 and 40 (student C), 34 and 34 (student D), 36 and 42 (student E), 47 and 48 (student F), 34 and 28 (student G), 44 and 41 (student H), 22 and 25 (student I), 18 and 16 (student J), 33 and 37 (student K). These numbers are plotted on a line graph with an x-axis for individual students and a y-axis for scores on each half. There are two lines plotted, one for odd items and another for even items." id="fig_2.7"/>
 	  </div>
     </section> 

    <div class="tbl_scroll_on_mobile">
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_number">Table 2.7</span><br/>
      <span class="tbl_title">Measures of test reliability</span>
      </caption>
      <tbody>
        <tr>
          <td class="heading">Test&#8211;retest reliability</td> 
		  <td>Correlation is between scores obtained on the same test given to the same students on two different occasions.</td> 
        </tr>

		<tr>
          <th class="heading">Parallel-forms reliability</th> 
		  <td>Correlation is between two forms of a test given to the same examinees.</td> 
        </tr>

		<tr>
          <td class="heading">Split-half reliability</td> 
		  <td>Correlation is between halves of a single test.</td> 
        </tr>
      </tbody>
    </table>
	</div>
	</div>
	</div>

<script type="text/javascript" src="Scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// 2.10 //////////////////////////////
/////////////////////////////////////////////////////////////
-->

<section class="page" id="sec2.10">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.10</span> <span class="sec_title">Factors That Affect Reliability</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Question</span> <span class="focus_q">What factors affect test reliability?</span></h2>
  </div>
  <div class="section-lead">
  <p class="intro">Ms. Isachsen requires a unit-end assessment and ranking of her students in a 12th-grade physics course. Unfortunately, she has put off building her final exam until the night before it is to be administered.</p>
  </div>
  <div class="section-body">
  <div class="single-column">
    

    <p class="txt">So she writes out a single question. Then, having just completed a measurement course, she cleverly devises a list of detailed scoring criteria.</p>

	<p class="txt">The question and scoring criteria are shown in Table 2.8.</p>
	</div>
	
	<a id="p80"></a>
	<div class="tbl_scroll_on_mobile">
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_number">Table 2.8</span><br />
      <span class="tbl_title">Illustrative single-question 12th-grade physics exam</span>
      </caption>
      <tbody>
        <tr>
          <td class="heading">Question</td> 
		  <td class="heading"></td> 
        </tr>

		<tr>
          <th class="white">Explain, in your own words, the details of vertical projectile motion.</th> 
		  <th class="white"></th> 
        </tr>

		<tr>
          <td class="heading">Scoring Criteria</td> 
		  <td class="heading">Points</td> 
        </tr>

		<tr>
          <th class="white">Describes what is meant by motion in a gravitational field<br/>
		  Explains acceleration<br/>
		  Mentions zero velocity at zenith<br/>
		  Includes free-fall equation<br/>
		  Applies free-fall equation to hypothetical situation<br/>
		  Includes graph of vertical projectile motion</th> 
		  <th class="white">10<br/>
		   10<br/> 
		   5<br/>
		   10<br/>
		   20<br/>
		   10</th> 
        </tr>

		<tr>
          <td class="heading">Maximum Points</td> 
		  <td class="heading">65</td> 
        </tr>

      </tbody>
    </table>
	</div>

	<div class="inner-section">
	<h2 class="h2">Length of Test</h2>
	<div class="single-column">
      <p class="txt">If Mrs. Isachsen&#39;s physics unit covered only vertical projectile motion, and if her instructional objectives are well represented in her scoring criteria, her one-item exam might be quite good. Under these circumstances, it might actually measure what she intends to measure, allowing her defensible use and interpretation of test results (it would have high validity). And, given careful application of her scoring criteria, the results might be consistent and stable (it would have reasonable reliability).</p>

	  <p class="txt">However, if her unit also covered topics such as elastic and inelastic collisions, relative velocity, notions of frames of reference, and other related topics, her single-item test would be about as useful as a snowmobile in Los Angeles.</p>

      <p class="txt">Although it might occasionally be possible to achieve an acceptable level of validity and reliability with a single item, that is not often the case. Poor reliability in a single-item test is especially likely if the test consists of objective test items such as multiple-choice questions, matching problems, or true&#8211;false exercises. It is difficult to imagine that a single multiple-choice item could measure all instructional objectives. In most cases the more items in the test, the more valid and reliable it is likely to be.</p>
	  </div>
	</div>
	
	  <h2 class="h2">Stability of Characteristics</h2>
	  <div class="single-column">

	  <p class="txt">The stability of what is being measured also affects a test&#39;s reliability. If what we are measuring is unstable and unpredictable, our measures are also likely to be inconsistent and unpredictable. However, we assume that most of what we measure in education will not fluctuate unpredictably.</p>

      <p class="txt">For example, we know that cognitive strategies develop over time and as knowledge increases. Tests that are both valid and reliable are expected to reflect these changes. These are predictable changes that do not reduce the reliability of our measuring instruments.</p>
	  </div>
	  
	<a id="p81"></a>
	<div class="inner-section">
	  <h2 class="h2">The Effects of Chance</h2>

	  <figure class="photo-right"> <img src="images/2.3.jpg" alt="High school student holding an exam graded F, looking disappointed." title="High school student holding an exam graded F, looking disappointed." id="img2.3"/>
        <figcaption>
          <p class="cr">Digital Vision/Digital Vision/Getty Images Plus</p>
          <p class="caption">This young man has just received the results of an educational assessment. If the validity, reliability, and fairness of the assessment are suspect, an injustice may have occurred.</p>
        </figcaption>
      </figure>
	  <div class="single-column">

      <p class="txt">Another factor that can affect the reliability of a test is <strong>chance</strong>, especially with respect to objective, teacher-made tests. We know, for example, that if students answer a true&#8211;false question without any knowledge of the subject matter, their chance of being correct is 50&#8211;50. On a 60-item test, those with Lady Luck on their side may do strikingly well, while unfortunate others may do quite poorly. But a later administration of this test might lead to startlingly different scores, resulting in a very low measure of test&#8211;retest reliability.</p>

	  <p class="txt">One way to reduce the effects of chance is to make tests longer or to use a larger number of short tests. The important point is that teachers should not base any important decision on only one or two measures.</p>
	  </div>
	</div>
	  <h2 class="h2">Item Difficulty</h2>
	  <div class="single-column">
      <p class="txt">Test reliability is also affected by the difficulty of items. Tests that are made up of excessively easy or impossibly difficult items will almost invariably have lower measured reliability scores than tests composed of items of moderate difficulty. Very easy and very difficult items tend to result in less consistent patterns of responding.</p>
    </div>
</div>

<script type="text/javascript" src="Scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// 2.11 //////////////////////////////
/////////////////////////////////////////////////////////////
-->

<section class="page" id="sec2.11">
  <div class="section-header">
    <h1 class="h1"><span class="sec_number">2.11</span><span class="sec_title">How to Improve Test Reliability</span></h1>
    <h2 class="focus"><span class="focus_lbl">Focus Questions</span> <span class="focus_q">Can a test be reliable but invalid? How can you improve test validity and reliability?</span></h2>
  </div>
  <div class="section-lead">
   <p class="intro">It&#39;s important to realize that a test cannot be valid without also being reliable.</p>
   </div>
  <div class="section-body">
  <div class="single-column">
   
      
    <p class="txt">If what we want to measure is a stable characteristic, and if the measures we obtain are inconsistent and unpredictable (hence, unreliable), then we are not measuring what we intend to measure, and the uses and interpretations of our test results will remain unsupported. Figure 2.8 summarizes the meanings of validity, reliability, and fairness and the relationship among them.</p>
	</div>
    
	<a id="p82"></a>
        <figure class="figure-1">
	          <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 2.8</span><br />
          <span class="figure_title">Three essential qualities of educational assessment</span> </h4>
        <p class="caption">The most subjective of these qualities of educational assessment, <em>fairness</em>, is often the one students think is most important.</p>
		<p class="src"></p>
		  </figcaption>

            <a class="trigger-readmore_for_Figure" href="#fig2-8" data-type="tipTrigger">
	        <img src="figures/Figure_2.8.jpg" alt="Three columns labeled reliability, validity, and fairness. An arrow points from reliability to validity, indicating that reliability is necessary for validity. Reliability means consistency and accuracy of measurement, and is estimated by testing and retesting, parallel-forms tests, and split-half tests. Validity is the extent to which interpretations and uses of test results are supported by relevant evidence, and is estimated by face (appearance), content, construct, and criterion-related (both predictive and concurrent). Fairness is the extent to which a test treats examinees in a just and equitable manner, and is a subjective estimate influenced by the extent to which material tested has been covered, all students have had an equal opportunity to learn, sufficient time is allowed for testing, there are safeguards against cheating, assessments are free of biases and stereotypes, misleading and trick questions have been avoided, accommodations are made for special needs, and grading is consistent." title="Three columns labeled reliability, validity, and fairness. An arrow points from reliability to validity, indicating that reliability is necessary for validity. Reliability means consistency and accuracy of measurement, and is estimated by testing and retesting, parallel-forms tests, and split-half tests. Validity is the extent to which interpretations and uses of test results are supported by relevant evidence, and is estimated by face (appearance), content, construct, and criterion-related (both predictive and concurrent). Fairness is the extent to which a test treats examinees in a just and equitable manner, and is a subjective estimate influenced by the extent to which material tested has been covered, all students have had an equal opportunity to learn, sufficient time is allowed for testing, there are safeguards against cheating, assessments are free of biases and stereotypes, misleading and trick questions have been avoided, accommodations are made for special needs, and grading is consistent. " id="fig_2.8"/>
			</a>
	  </figure>

    <section class="tipBox" id="fig2-8">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_2.8.jpg" alt="Three columns labeled reliability, validity, and fairness. An arrow points from reliability to validity, indicating that reliability is necessary for validity. Reliability means consistency and accuracy of measurement, and is estimated by testing and retesting, parallel-forms tests, and split-half tests. Validity is the extent to which interpretations and uses of test results are supported by relevant evidence, and is estimated by face (appearance), content, construct, and criterion-related (both predictive and concurrent). Fairness is the extent to which a test treats examinees in a just and equitable manner, and is a subjective estimate influenced by the extent to which material tested has been covered, all students have had an equal opportunity to learn, sufficient time is allowed for testing, there are safeguards against cheating, assessments are free of biases and stereotypes, misleading and trick questions have been avoided, accommodations are made for special needs, and grading is consistent." title="Three columns labeled reliability, validity, and fairness. An arrow points from reliability to validity, indicating that reliability is necessary for validity. Reliability means consistency and accuracy of measurement, and is estimated by testing and retesting, parallel-forms tests, and split-half tests. Validity is the extent to which interpretations and uses of test results are supported by relevant evidence, and is estimated by face (appearance), content, construct, and criterion-related (both predictive and concurrent). Fairness is the extent to which a test treats examinees in a just and equitable manner, and is a subjective estimate influenced by the extent to which material tested has been covered, all students have had an equal opportunity to learn, sufficient time is allowed for testing, there are safeguards against cheating, assessments are free of biases and stereotypes, misleading and trick questions have been avoided, accommodations are made for special needs, and grading is consistent. " id="fig_2.8"/>
 	  </div>
     </section> 

  
  <div class="single-column">
   <p class="key-point">On the other hand, a test can be highly reliable without being valid.</p>
   
   <p class="txt">Consider the test and scoring guide shown in Figure 2.9. This is an extremely reliable test: Examinees invariably answer all questions correctly and always obtain the same score. But as a measure of intelligence, it clearly lacks face, content, construct, and criterion-related validity. It measures reliably, but it does not measure what it is intended to measure.</p>
   </div>
   
   <a id="p83"></a>
        <figure class="figure-1">
	          <figcaption>
	   <h4 class="figure_name"> <span class="figure_number">Figure 2.9</span><br />
          <span class="figure_title">Human intelligence scale</span> </h4>
        <p class="caption">This example is not a true intelligence test. It simply illustrates that highly reliable (consistent) measures can be desperately invalid.</p>
		<p class="src"></p>
		  </figcaption>

            <a class="trigger-readmore_for_Figure" href="#fig2-9" data-type="tipTrigger">
	        <img src="figures/Figure_2.9.jpg" alt="Fictitious example of a human intelligence scale containing areas for name, age, and address. Questions are difficult to answer incorrectly. For example, What is your name?; What is your address?; and How old were you on your last birthday? " title="Fictitious example of a human intelligence scale containing areas for name, age, and address. Questions are difficult to answer incorrectly. For example, What is your name?; What is your address?; and How old were you on your last birthday? " id="fig_2.9"/>
			</a>
	  </figure>

    <section class="tipBox" id="fig2-9">
      <div class="box-6">
        <img class="trigger-readmore_zoom" src="figures/Figure_2.9.jpg" alt="Fictitious example of a human intelligence scale containing areas for name, age, and address. Questions are difficult to answer incorrectly. For example, What is your name?; What is your address?; and How old were you on your last birthday? " title="Fictitious example of a human intelligence scale containing areas for name, age, and address. Questions are difficult to answer incorrectly. For example, What is your name?; What is your address?; and How old were you on your last birthday? " id="fig_2.9"/>
 	  </div>
     </section> 


	  
	<div class="single-column">
  
   <p class="txt">Test reliability is not something that most teachers are likely to calculate for their tests. However, they should understand what reliability is, how important it is in educational assessment, and how it can be improved. For example, when selecting from among a number of different standardized tests, it is important to have, and understand, information about their reliability. Similarly, when making decisions about students, teachers need to have some knowledge of the reliability of the assessments on which they base their decisions.</p>

   <p class="txt">It might be useful to know that the reliability of teacher-made tests is around .5. As we will see in Chapter 9, which deals with statistical measures, this is a modest index of reliability. The fact is that most teacher-made tests have a relatively high degree of measurement error.</p>

   <p class="txt">Standardized tests, on the other hand, tend to have higher reliabilities (around .9; Salvia, Ysseldyke, &#38; Witmer, 2017). As a result, the most important decisions that affect the lives of students should be based on carefully selected standardized tests&mdash;and on professional opinion where necessary&mdash;rather than on teacher-constructed tests, and certainly never on hunches or intuitive impressions.</p>
   </div>
	
	<a id="p84"></a>
	<div class="tbl_scroll_on_mobile">
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_number">Table 2.9</span><br />
      <span class="tbl_title">Improving reliability and validity</span>
      </caption>
      <thead class="tbl_header">
        <tr>
          <th>Suggestions for improving test validity</th>
          <th>Suggestions for improving test reliability</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><ul class="bl">
		  <li class="li_text">Use clear and easily understood tasks.</li>
		  <li class="li_text">Sample from all skill and content areas.</li>
		  <li class="li_text">Select items to reflect importance of specific objectives.</li>
		  <li class="li_text">Allow sufficient time for all students to complete the test.</li>
		  <li class="li_text">Use blueprints to guide instruction and test construction.</li>
		  <li class="li_text">Analyze items to determine how well they match learning targets.</li>
		  <li class="li_text">Check to see if students who do well on your tests also do well in other comparable classes.</li>
		  <li class="li_text">Use a variety of approaches to assessment.</li>
		  </ul></td>

          <td><ul class="bl">
		  <li class="li_text">Make tests longer.</li>
		  <li class="li_text">Enlist the assistance of other raters when using performance assessments.</li>
		  <li class="li_text">Develop moderately difficult rather than excessively easy or very difficult items.</li>
		  <li class="li_text">Try to eliminate subjective influences in scoring.</li>
		  <li class="li_text">Develop and use clear rubrics and checklists for scoring performance assessments.</li>
		  <li class="li_text">Restrict distracting influences whenever possible.</li>
		  <li class="li_text">Use a variety of different assessments.</li>
		  <li class="li_text">Eliminate or reduce the possibility that chance might affect test outcomes.</li>
		  </ul></td>
        </tr>
      </tbody>
    </table>
	</div>
	
	<div class="single-column">
   <p class="txt">Table 2.9 summarizes a number of ways in which the reliability and validity of educational assessments can be increased. Table 2.10 is the APA&#39;s Code of Fair Testing Practices in Education. Especially important are suggested guidelines for test users with respect to selecting tests, administering them, and interpreting and reporting their results.</p>
   </div>
	
	<div class="tbl_scroll_on_mobile">
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_number">Table 2.10</span><br />
      <span class="tbl_title">The APA Code of Fair Testing Practices in Education</span><br/><br/>
	  <span class="tbl_subtitle">A. Developing and selecting appropriate tests</span>
      </caption>
      <thead class="tbl_header">
        <tr>
          <th>Test developers</th>
          <th>Test users</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Test developers should provide the information and supporting evidence that test users need to select appropriate tests.</td> 
		  <td>Test users should select tests that meet the intended purpose and that are appropriate for the intended test takers.</td>
        </tr>

		<tr>
          <td>A-1. Provide evidence of what the test measures, the recommended uses, the intended test takers, and the strengths and limitations of the test, including the level of precision of the test scores.</td> 
		  <td>A-1. Define the purpose for testing, the content and skills to be tested, and the intended test takers. Select and use the most appropriate test based on a thorough review of available information.</td>
        </tr>

		<tr>
          <td>A-2. Describe how the content and skills to be tested were selected and how the tests were developed.</td> 
		  <td>A-2. Review and select tests based on the appropriateness of test content, skills tested, and content coverage for the intended purpose of testing.</td>
        </tr>

		<tr>
          <td>A-3. Communicate information about a test&#39;s characteristics at a level of detail appropriate to the intended test users.</td> 
		  <td>A-3. Review materials provided by test developers and select tests for which clear, accurate, and complete information is provided.</td>
        </tr>

		<tr>
          <td>A-4. Provide guidance on the levels of skills, knowledge, and training necessary for appropriate review, selection, and administration of tests.</td> 
		  <td>A-4. Select tests through a process that includes persons with appropriate knowledge, skills, and training.</td>
        </tr>

		<tr>
          <td>A-5. Provide evidence that the technical quality, including reliability and validity, of the test meets its intended purposes.</td> 
		  <td>A-5. Evaluate evidence of the technical quality of the test provided by the test developer and any independent reviewers.</td>
        </tr>

		<tr>
          <td>A-6. Provide to qualified test users representative samples of test questions or practice tests, directions, answer sheets, manuals, and score reports.</td> 
		  <td>A-6. Evaluate representative samples of test questions or practice tests, directions, answer sheets, manuals, and score reports before selecting a test.</td>
        </tr>

		<tr>
		  <td>A-7. Avoid potentially offensive content or language when developing test questions and related materials.</td> 
		  <td>A-7. Evaluate procedures and materials used by test developers, as well as the resulting test, to ensure that potentially offensive content or language is avoided.</td>
		</tr>

		<tr>
		  <td>A-8. Make appropriately modified forms of tests or administration procedures available for test takers with disabilities who need special accommodations.</td> 
		  <td>A-8. Select tests with appropriately modified forms or administration procedures for test takers with disabilities who need special accommodations.</td>
		</tr>

		<tr>
		  <td>A-9. Obtain and provide evidence on the performance of test takers of diverse subgroups, making significant efforts to obtain sample sizes that are adequate for subgroup analyses. Evaluate the evidence to ensure that differences in performance are related to the skills being assessed.</td> 
		  <td>A-9. Evaluate the available evidence on the performance of test takers of diverse subgroups. Determine to the extent feasible which performance differences may have been caused by factors unrelated to the skills being assessed.</td>
		</tr>
      </tbody>
    </table>
	</div>
   
   <a id="p85"></a>
   <div class="tbl_scroll_on_mobile">
   <table class="tbl-xl">
      <caption class="tbl_name">
	  <span class="tbl_subtitle">B. Administering and scoring tests</span>
      </caption>
      <thead class="tbl_header">
        <tr>
          <th>Test developers</th>
          <th>Test users</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Test developers should explain how to administer and score tests correctly and fairly.</td> 
		  <td>Test users should administer and score tests correctly and fairly.</td>
        </tr>

		<tr>
          <td>B-1. Provide clear descriptions of detailed procedures for administering tests in a standardized manner.</td> 
		  <td>B-1. Follow established procedures for administering tests in a standardized manner.</td>
        </tr>

		<tr>
          <td>B-2. Provide guidelines on reasonable procedures for assessing persons with disabilities who need special accommodations or those with diverse linguistic backgrounds.</td> 
		  <td>B-2. Provide and document appropriate procedures for test takers with disabilities who need special accommodations or those with diverse linguistic backgrounds. Some accommodations may be required by law or regulation.</td>
        </tr>

		<tr>
          <td>B-3. Provide information to test takers or test users on test question formats and procedures for answering test questions, including information on the use of any needed materials and equipment.</td> 
		  <td>B-3. Provide test takers with an opportunity to become familiar with test question formats and any materials or equipment that may be used during testing.</td>
        </tr>

		<tr>
          <td>B-4. Establish and implement procedures to ensure the security of testing materials during all phases of test development, administration, scoring, and reporting.</td> 
		  <td>B-4. Protect the security of test materials, including respecting copyrights and eliminating opportunities for test takers to obtain scores by fraudulent means.</td>
        </tr>

		<tr>
          <td>B-5. Provide procedures, materials and guidelines for scoring the tests, and for monitoring the accuracy of the scoring process. If scoring the test is the responsibility of the test developer, provide adequate training for scorers.</td> 
		  <td>B-5. If test scoring is the responsibility of the test user, provide adequate training to scorers and ensure and monitor the accuracy of the scoring process.</td>
        </tr>

		<tr>
          <td>B-6. Correct errors that affect the interpretation of the scores and communicate the corrected results promptly.</td> 
		  <td>B-6. Correct errors that affect the interpretation of the scores and communicate the corrected results promptly.</td>
        </tr>

		<tr>
          <td>B-7. Develop and implement procedures for ensuring the confidentiality of scores.</td> 
		  <td>B-7. Develop and implement procedures for ensuring the confidentiality of scores.</td>
        </tr>
      </tbody>
    </table>
    </div>

	<a id="p86"></a>
	<div class="tbl_scroll_on_mobile">
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_subtitle">C. Reporting and interpreting test results</span>
      </caption>
      <thead class="tbl_header">
        <tr>
          <th>Test developers</th>
          <th>Test users</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Test developers should report test results accurately and provide information to help test users interpret test results correctly.</td> 
		  <td>Test users should report and interpret test results accurately and clearly.</td>
        </tr>

		<tr>
          <td>C-1. Provide information to support recommended interpretations of the results, including the nature of the content, norms or comparison groups, and other technical evidence. Advise test users of the benefits and limitations of test results and their interpretation. Warn against assigning greater precision than is warranted.</td> 
		  <td>C-1. Interpret the meaning of the test results, taking into account the nature of the content, norms or comparison groups, other technical evidence, and benefits and limitations of test results.</td>
        </tr>

	  <tr>           
		<td>C-2. Provide guidance regarding the interpretations of results for tests administered with modifications. Inform test users of potential problems in interpreting test results when tests or test administration procedures are modified.</td> 
		  
		 <td>C-2. Interpret test results from modified test or test administration procedures in view of the impact those modifications may have had on test results.</td>
        </tr>

		<tr>
          <td>C-3. Specify appropriate uses of test results and warn test users of potential misuses.</td> 
		  <td>C-3. Avoid using tests for purposes other than those recommended by the test developer unless there is evidence to support the intended use or interpretation.</td>
        </tr>

		<tr>
          <td>C-4. When test developers set standards, provide the rationale, procedures, and evidence for setting performance standards or passing scores. Avoid using stigmatizing labels.</td> 
		  <td>C-4. Review the procedures for setting performance standards or passing scores. Avoid using stigmatizing labels.</td>
        </tr>

		<tr>
          <td>C-5. Encourage test users to base decisions about test takers on multiple sources of appropriate information, not on a single test score.</td> 
		  <td>C-5. Avoid using a single test score as the sole determinant of decisions about test takers. Interpret test scores in conjunction with other information about individuals.</td>
        </tr>

		<tr>
          <td>C-6. Provide information to enable test users to accurately interpret and report test results for groups of test takers, including information about who were and who were not included in the different groups being compared, and information about factors that might influence the interpretation of results.</td> 
		  <td>C-6. State the intended interpretation and use of test results for groups of test takers. Avoid grouping test results for purposes not specifically recommended by the test developer unless evidence is obtained to support the intended use. Report procedures that were followed in determining who were and who were not included in the groups being compared and describe factors that might influence the interpretation of results.</td>
        </tr>

		<tr>
          <td>C-7. Provide test results in a timely fashion and in a manner that is understood by the test taker.</td> 
		  <td>C-7. Communicate test results in a timely fashion and in a manner that is understood by the test taker.</td>
        </tr>

		<tr>
          <td>C-8. Provide guidance to test users about how to monitor the extent to which the test is fulfilling its intended purposes.</td> 
		  <td>C-8. Develop and implement procedures for monitoring test use, including consistency with the intended purposes of the test.</td>
        </tr>
      </tbody>
    </table>
    </div>

	<a id="p87"></a>
	<div class="tbl_scroll_on_mobile">
	<table class="tbl-xl">
      <caption class="tbl_name">
      <span class="tbl_subtitle">D. Informing test takers</span>
      </caption>
	  <thead>
        <tr>
          <th class="no_border">Under some circumstances, test developers have direct communication with the test takers and/or control of the tests, testing process, and test results. In other circumstances the test users have these responsibilities.</th> 
        </tr>
      </thead>
      <tbody>
		<tr>
          <td><strong>Test developers or test users should inform test takers about the naformative rather than summativeture of the test, test taker rights and responsibilities, the appropriate use of scores, and procedures for resolving challenges to scores.</strong></td> 
        </tr>

		<tr>
          <td>D-1. Inform test takers in advance of the test administration about the coverage of the test, the types of question formats, the directions, and appropriate test-taking strategies. Make such information available to all test takers.</td> 
        </tr>

		<tr>
          <td>D-2. When a test is optional, provide test takers or their parents/guardians with information to help them judge whether a test should be taken&mdash;including indications of any consequences that may result from not taking the test (e.g., not being eligible to compete for a particular scholarship) &mdash;and whether there is an available alternative to the test.</td> 
        </tr>

		<tr>
          <td>D-3. Provide test takers or their parents/guardians with information about rights test takers may have to obtain copies of tests and completed answer sheets, to retake tests, to have tests rescored, or to have scores declared invalid.</td> 
        </tr>

		<tr>
          <td>D-4. Provide test takers or their parents/guardians with information about responsibilities test takers have, such as being aware of the intended purpose and uses of the test, performing at capacity, following directions, and not disclosing test items or interfering with other test takers.</td> 
        </tr>

		<tr>
          <td>D-5. Inform test takers or their parents/guardians how long scores will be kept on file and indicate to whom, under what circumstances, and in what manner test scores and related information will or will not be released. Protect test scores from unauthorized release and access.</td> 
        </tr>

		<tr>
          <td>D-6. Describe procedures for investigating and resolving circumstances that might result in canceling or withholding scores, such as failure to adhere to specified testing procedures.</td> 
        </tr>

		<tr>
          <td>D-7. Describe procedures that test takers, parents/guardians, and other interested parties may use to obtain more information about the test, register complaints, and have problems resolved.</td> 
        </tr>
      </tbody>
	  <tfoot class="no-border">
          <tr>
            <td><em>Source</em>: Code of Fair Testing Practices in Education, <em>by Joint Committee on Testing Practices, 2004, Washington, DC: Author. Copyright &#169; 2004 by the Joint Committee on Testing Practices.</em></td>
          </tr>
        </tfoot>
    </table>
	</div>

  </div> 
<script type="text/javascript" src="Scripts/app-min.js"></script>
</section>

<!--
/////////////////////////////////////////////////////////////
///////////////////////// EOC //////////////////////////////
/////////////////////////////////////////////////////////////
-->

<section class="page" id="ch2_summary">
  <div class="chapter-review-header">
    <h1 class="eoc_h1"><strong>Chapter 2<br />
      </strong> Themes and Questions</h1>
    <div class="seperator">&nbsp;</div>
  </div>
  <div class="chapter-review">
    <h2 class="h2">Themes</h2>
    <div class="single-column">
      <h3 class="h3"> Purposes of Educational Assessment</h3>
      <p class="txt">Assessments can be used to summarize student achievement (<em>summative</em>); for selection and placement purposes (<em>placement</em>); or as a means of providing feedback to improve teaching and learning (<em>formative</em>). <em>Diagnostic</em> assessment is a form of preassessment (placement assessment) useful for identifying learner strengths and weaknesses. The overriding purpose of all forms of educational assessment is to help the student achieve learning objectives. Planning for effective assessment requires clarifying and communicating instructional objectives and aligning assessment and instruction with these goals (<em>backward design</em>). As much as possible, assessment should be an integral part of the instructional process, designed to provide ongoing feedback for both teachers and learners to assist and improve minute-to-minute decisions. Teachers should use a variety of approaches to assessment.</p>

	  <a id="p88"></a>
      <h3 class="h3">Fairness</h3>
      <p class="txt">Tests are fair when they treat all learners in an evenhanded manner and when they provide all learners with an equal opportunity to learn. Tests are unfair when they examine content that has neither been covered nor assigned; if they deliberately or unintentionally use misleading &quot;trick&quot; questions; when some learners have not been given sufficient opportunity to learn the material; if they don&#39;t allow sufficient time for all learners to finish; when they fail to accommodate to the special needs of individual learners; when they reflect gender and cultural biases and stereotypes; if scoring is influenced by stereotypes and teacher expectations; when steps are not taken to guard against cheating; and if they are graded inconsistently.</p>

      <h3 class="h3">Validity</h3>
      <p class="txt">Tests are seldom fair if they are not also valid. Validity relates to the sensible and defensible interpretation of test results rather than to any intrinsic characteristic of the test itself. A test is valid to the extent that the interpretation and uses of its results are supported by evidence of their usefulness when applied in real-life situations. <em>Face validity</em> is a measure of the extent to which a test appears to measure what it is meant to measure. <em>Content validity</em> is determined by the extent to which test items reflect course objectives, and it may be determined through a careful analysis and selection of items. <em>Construct validity</em> relates to how consistent the test is with the thinking that gave rise to it; this type of validity is more relevant for psychological assessments (e.g., personality or intelligence tests) than for teacher-made tests. <em>Criterion-related validity</em> is reflected in agreement between test results and related predictions (<em>predictive validity</em>) and in the extent to which the results of a test agree with the results of other tests that measure the same characteristics.</p>

	  <h3 class="h3">Reliability</h3>
      <p class="txt">Reliability is reflected in consistency, stability, and predictability of measurements. It has to do with error of measurement: The greater the errors in our assessments, the lower the reliability. For a test to be valid, it must also be reliable. High reliability is reflected in similar scores for the same individual given the same test on two occasions (<em>test&#8211;retest reliability</em>); similar scores on equivalent forms of a test (<em>parallel-forms reliability</em>); and comparable scores on similar halves of a single test (<em>split-half reliability</em>). Reliability is affected by the length of a test (generally higher with longer tests); by the stability of what is being measured (unstable characteristics yield lower reliabilities); chance factors; and item difficulty (moderately difficult items usually lead to higher test reliability than very easy or very difficult items).</p>

    </div>
    <h2 class="h2">Applied Questions</h2>
    <div class="single-column">
      <ol class="applied-questions">
        <li class="li_text"><em>What are the principal uses of educational assessment? </em>Research and defend the proposition that the most important function of educational assessment should be formative.</li>
        <li class="li_text"><em>What are the main characteristics of good educational assessment? </em>Explain in your own words why you think one of these characteristics is more important than the others in educational measurement.</li>
        <li class="li_text"><em>What different factors can make a test unfair?</em> Evaluate the fairness of the last important test you took. Explain why it was fair or suggest how it might have been made fairer.</li>
        <li class="li_text"><em>Can a test be valid but unreliable? </em>Generate a handful of related test items to illustrate and support your answer for this question.</li>
        <li class="li_text"><em>How can you determine the reliability of a test? </em>Outline the steps a teacher might take to increase test reliability without having to go to the trouble of actually calculating it.</li>
        <li class="li_text"><em>How can you improve test validity and reliability? </em>Make a list of what you consider to be guidelines for sound testing practices. Identify those that would improve test validity and reliability.</li>
      </ol>
    </div>

	<a id="p89"></a>
    <h2 class="h2">Key Terms</h2>
	  
	  	<script type="text/javascript"> (function(a){a.fn.answerToggle=function(b){var b=a.extend({revealClass:".reveal"},b),c=a(b.revealClass);this.length!=c.length?console.log(this.length,c.length):(a(b.revealClass).hide(),this.each(function(b){var d=a(c[b]);a(this).click(function(a){a.preventDefault();d.slideToggle("fast")})}))}})(jQuery);
$('document').ready(function(){
$('.answerToggle').answerToggle('.reveal');
}); </script>
	  
    <div class="single-column">
      <p class="tx"><a href="#" class="answerToggle"><strong>bias</strong></a>&ensp;</p>
<p class="reveal"> A personal prejudice (prejudgment) in favor of or against a person, thing, or idea, when such prejudgment is typically unfair.</p>
			
      <p class="tx"><a href="#" class="answerToggle"><strong>Campbell&#39;s law</strong></a>&ensp;</p>
<p class="reveal"> The observation that as a social indicator becomes progressively more important, it is increasingly likely to become corrupted and lose its value.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>chance</strong></a>&ensp;</p>
<p class="reveal"> An unpredictable, unforeseen cause of events.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>cheating</strong></a>&ensp;</p>
<p class="reveal"> The act of defrauding, swindling, deceiving, or tricking. Using hidden and often immoral and illegal means to reach a goal.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>cobra effect</strong></a>&ensp;</p>
<p class="reveal"> A situation in which a seemingly effective solution for a problem actually makes it worse.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>concurrent validity</strong></a>&ensp;</p>
<p class="reveal"> A measure of validity determined by comparing a test to other previously established measures. An aspect of criterion-related validity. See also <em>criterion- related validity</em>.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>construct validity</strong></a>&ensp;</p>
<p class="reveal"> An estimate of test validity based on the extent to which test results agree with and reflect the theories that underlie the test.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>content validity</strong></a>&ensp;</p>
<p class="reveal"> Test validity determined by a careful analysis of the content of test items and a comparison of the content with course objectives. See also <em>face validity, predictive validity, reliability</em>.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>correlation</strong></a>&ensp;</p>
<p class="reveal"> A statistical measure of the degree of relationship between variables. A positive correlation is necessary for, but does not imply, causation.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>criterion-related validity</strong></a>&ensp;</p>
<p class="reveal"> A measure of the extent to which predictions based on test results are accurate (predictive validity) and how well the test agrees with other related measures (concurrent validity).</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>educational alignment</strong></a>&ensp;</p>
<p class="reveal"> A deliberate attempt to align instructional objectives with learning activities and assessment approaches. Also used to denote the need to align local curriculum content, instructional approaches, and assessment practices with mandated educational standards.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>English language learner (ELL) </strong></a>&ensp;</p>
<p class="reveal">A learner whose dominant language is not English and for whom educational approaches include instruction in learning the English language.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>error of measurement</strong></a>&ensp;</p>
<p class="reveal"> The difference between a measurement and the actual value of what is being measured.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>face validity </strong></a>&ensp;</p>
<p class="reveal">The extent to which a test appears to be measuring what it is intended to measure. See also <em>content validity, predictive validity, reliability, validity</em>.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>group test</strong></a>&ensp;</p>
<p class="reveal"> A test that may be given to large groups of subjects at one time.</p>

	  <a id="p90"></a>
      <p class="tx"><a href="#" class="answerToggle"><strong>hypothetical variable</strong></a>&ensp;</p>
<p class="reveal"> A quality or characteristic that can vary (hence a <em>variable</em>) but that is inferred rather than directly observable.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>individual test</strong></a>&ensp;</p>
<p class="reveal"> A test that can be given to only one individual at a time.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>mastery learning</strong></a>&ensp;</p>
<p class="reveal"> An instructional approach described by Bloom in which a learning sequence is broken down into specific objectives, and progress is gauged through criterion-referenced testing designed to help each learner master sequential competencies.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>parallel-forms reliability</strong></a>&ensp;</p>
<p class="reveal"> A measure of test consistency (reliability) obtained by looking at the correlation between scores obtained by the same individual on two different but equivalent (parallel) forms of one test. See also <em>split-half reliability</em>.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>predictive validity</strong></a>&ensp;</p>
<p class="reveal"> A measure of the extent to which predictions based on test results are accurate.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>reliability</strong></a>&ensp;</p>
<p class="reveal"> The consistency with which a test measures whatever it measures.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>split-half reliability</strong></a>&ensp;</p>
<p class="reveal"> An index of test reliability (consistency) derived by arbitrarily dividing a test into parallel halves (odd- and even-numbered items, for example) and looking at the agreement between scores obtained by each individual on the two halves. See also <em>parallel-forms reliability</em>.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>stereotype</strong></a>&ensp;</p>
<p class="reveal"> An overly simplified and usually negative view attributed to all individuals who share similar characteristics. Stereotypes are usually shared by many people and are often directed toward identifiable ethnic, religious, cultural, or political groups.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>test blueprint</strong></a>&ensp;</p>
<p class="reveal"> A table of specifications for a teacher-made test. A good test blueprint provides information about the topics to be tested, the nature of the questions to be used, and the objectives (outcomes) to be assessed.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>test fairness</strong></a>&ensp;</p>
<p class="reveal"> A measure of the extent to which a test treats examinees in a just and equitable manner both in terms of its content and of the opportunity each learner has had to prepare for the test.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>test&#8211;retest reliability</strong></a>&ensp;</p>
<p class="reveal"> An estimate of the consistency (reliability) of a test based on the degree of agreement among scores obtained from different presentations of the same test. See also <em>parallel-forms reliability</em>.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>trick questions</strong></a>&ensp;</p>
<p class="reveal"> Exam or quiz questions that intentionally or accidentally mislead examinees into thinking they should answer a certain way, or questions that are simply confusing.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>validity</strong></a>&ensp;</p>
<p class="reveal"> The extent to which the interpretations and uses to which test results are put can be supported by evidence. See also <em>construct validity, content validity, criterionrelated validity, face validity, reliability</em>.</p>

      <p class="tx"><a href="#" class="answerToggle"><strong>virtual reality instructional programs</strong></a>&ensp;</p>
<p class="reveal"> Computer-based simulations used for instructional purposes. To produce a sensation of realism, they typically involve a number of sensory systems (such as bodily sensations, visual images, and auditory signals).</p>
    </div>
  </div>
</section>
<script type="text/javascript" src="Scripts/app-min.js"></script>

</body>
</html>